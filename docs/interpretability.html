<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Interpretability | ML Case Studies</title>
  <meta name="description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Interpretability | ML Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Case studies for reproducibility, imputation, and interpretability" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Interpretability | ML Case Studies" />
  
  <meta name="twitter:description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-05-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="imputation.html"/>
<link rel="next" href="acknowledgements.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint/kePrint.js"></script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>ML Case Studies</h3></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>1</b> Reproducibility of scientific papers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="reproducibility.html"><a href="reproducibility.html#title-of-the-article"><i class="fa fa-check"></i><b>1.1</b> Title of the article</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract"><i class="fa fa-check"></i><b>1.1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.1.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.1.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work"><i class="fa fa-check"></i><b>1.1.3</b> Related Work</a></li>
<li class="chapter" data-level="1.1.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology"><i class="fa fa-check"></i><b>1.1.4</b> Methodology</a></li>
<li class="chapter" data-level="1.1.5" data-path="reproducibility.html"><a href="reproducibility.html#results"><i class="fa fa-check"></i><b>1.1.5</b> Results</a></li>
<li class="chapter" data-level="1.1.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="reproducibility.html"><a href="reproducibility.html#how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers"><i class="fa fa-check"></i><b>1.2</b> How to measure reproducibility? Classification of problems with reproducing scientific papers</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-1"><i class="fa fa-check"></i><b>1.2.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction"><i class="fa fa-check"></i><b>1.2.2</b> Introduction</a></li>
<li class="chapter" data-level="1.2.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-1"><i class="fa fa-check"></i><b>1.2.3</b> Related Work</a></li>
<li class="chapter" data-level="1.2.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-1"><i class="fa fa-check"></i><b>1.2.4</b> Methodology</a></li>
<li class="chapter" data-level="1.2.5" data-path="reproducibility.html"><a href="reproducibility.html#results-1"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-1"><i class="fa fa-check"></i><b>1.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="reproducibility.html"><a href="reproducibility.html#aging-articles.-how-time-affects-reproducibility-of-scientific-papers"><i class="fa fa-check"></i><b>1.3</b> Aging articles. How time affects reproducibility of scientific papers?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-2"><i class="fa fa-check"></i><b>1.3.1</b> Abstract</a></li>
<li class="chapter" data-level="1.3.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-1"><i class="fa fa-check"></i><b>1.3.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-2"><i class="fa fa-check"></i><b>1.3.3</b> Methodology</a></li>
<li class="chapter" data-level="1.3.4" data-path="reproducibility.html"><a href="reproducibility.html#results-2"><i class="fa fa-check"></i><b>1.3.4</b> Results</a></li>
<li class="chapter" data-level="1.3.5" data-path="reproducibility.html"><a href="reproducibility.html#conclusions"><i class="fa fa-check"></i><b>1.3.5</b> Conclusions</a></li>
<li class="chapter" data-level="1.3.6" data-path="reproducibility.html"><a href="reproducibility.html#summary"><i class="fa fa-check"></i><b>1.3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reproducibility.html"><a href="reproducibility.html#ways-to-reproduce-articles-in-terms-of-release-date-and-magazine"><i class="fa fa-check"></i><b>1.4</b> Ways to reproduce articles in terms of release date and magazine</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-3"><i class="fa fa-check"></i><b>1.4.1</b> Abstract</a></li>
<li class="chapter" data-level="1.4.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-1"><i class="fa fa-check"></i><b>1.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.4.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-2"><i class="fa fa-check"></i><b>1.4.3</b> Related Work</a></li>
<li class="chapter" data-level="1.4.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-3"><i class="fa fa-check"></i><b>1.4.4</b> Methodology</a></li>
<li class="chapter" data-level="1.4.5" data-path="reproducibility.html"><a href="reproducibility.html#results-3"><i class="fa fa-check"></i><b>1.4.5</b> Results</a></li>
<li class="chapter" data-level="1.4.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-2"><i class="fa fa-check"></i><b>1.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-of-outdated-articles-about-up-to-date-r-packages"><i class="fa fa-check"></i><b>1.5</b> Reproducibility of outdated articles about up-to-date R packages</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-4"><i class="fa fa-check"></i><b>1.5.1</b> Abstract</a></li>
<li class="chapter" data-level="1.5.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-2"><i class="fa fa-check"></i><b>1.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.5.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-3"><i class="fa fa-check"></i><b>1.5.3</b> Related Work</a></li>
<li class="chapter" data-level="1.5.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-4"><i class="fa fa-check"></i><b>1.5.4</b> Methodology</a></li>
<li class="chapter" data-level="1.5.5" data-path="reproducibility.html"><a href="reproducibility.html#results-4"><i class="fa fa-check"></i><b>1.5.5</b> Results</a></li>
<li class="chapter" data-level="1.5.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-3"><i class="fa fa-check"></i><b>1.5.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="reproducibility.html"><a href="reproducibility.html#correlation-between-reproducibility-of-components-of-research-papers-and-their-purpose"><i class="fa fa-check"></i><b>1.6</b> Correlation between reproducibility of components of research papers and their purpose</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-5"><i class="fa fa-check"></i><b>1.6.1</b> Abstract</a></li>
<li class="chapter" data-level="1.6.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-3"><i class="fa fa-check"></i><b>1.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.6.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-4"><i class="fa fa-check"></i><b>1.6.3</b> Related Work</a></li>
<li class="chapter" data-level="1.6.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-5"><i class="fa fa-check"></i><b>1.6.4</b> Methodology</a></li>
<li class="chapter" data-level="1.6.5" data-path="reproducibility.html"><a href="reproducibility.html#results-5"><i class="fa fa-check"></i><b>1.6.5</b> Results</a></li>
<li class="chapter" data-level="1.6.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-4"><i class="fa fa-check"></i><b>1.6.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="reproducibility.html"><a href="reproducibility.html#how-active-development-affects-reproducibility"><i class="fa fa-check"></i><b>1.7</b> How active development affects reproducibility</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-6"><i class="fa fa-check"></i><b>1.7.1</b> Abstract</a></li>
<li class="chapter" data-level="1.7.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-4"><i class="fa fa-check"></i><b>1.7.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.7.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-6"><i class="fa fa-check"></i><b>1.7.3</b> Methodology</a></li>
<li class="chapter" data-level="1.7.4" data-path="reproducibility.html"><a href="reproducibility.html#results-6"><i class="fa fa-check"></i><b>1.7.4</b> Results</a></li>
<li class="chapter" data-level="1.7.5" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-5"><i class="fa fa-check"></i><b>1.7.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language"><i class="fa fa-check"></i><b>1.8</b> Reproducibility differences of articles published in various journals and using R or Python language</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-7"><i class="fa fa-check"></i><b>1.8.1</b> Abstract</a></li>
<li class="chapter" data-level="1.8.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-5"><i class="fa fa-check"></i><b>1.8.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.8.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-7"><i class="fa fa-check"></i><b>1.8.3</b> Methodology</a></li>
<li class="chapter" data-level="1.8.4" data-path="reproducibility.html"><a href="reproducibility.html#results-7"><i class="fa fa-check"></i><b>1.8.4</b> Results</a></li>
<li class="chapter" data-level="1.8.5" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-6"><i class="fa fa-check"></i><b>1.8.5</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>2</b> Imputation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="imputation.html"><a href="imputation.html#default-imputation-efficiency-comparision"><i class="fa fa-check"></i><b>2.1</b> Default imputation efficiency comparision</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="imputation.html"><a href="imputation.html#abstract-8"><i class="fa fa-check"></i><b>2.1.1</b> Abstract</a></li>
<li class="chapter" data-level="2.1.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-6"><i class="fa fa-check"></i><b>2.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.1.3" data-path="imputation.html"><a href="imputation.html#related-work-5"><i class="fa fa-check"></i><b>2.1.3</b> Related Work</a></li>
<li class="chapter" data-level="2.1.4" data-path="imputation.html"><a href="imputation.html#methodology-8"><i class="fa fa-check"></i><b>2.1.4</b> Methodology</a></li>
<li class="chapter" data-level="2.1.5" data-path="imputation.html"><a href="imputation.html#results-8"><i class="fa fa-check"></i><b>2.1.5</b> Results</a></li>
<li class="chapter" data-level="2.1.6" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-7"><i class="fa fa-check"></i><b>2.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="imputation.html"><a href="imputation.html#various-data-imputation-techniques-in-r"><i class="fa fa-check"></i><b>2.2</b> Various data imputation techniques in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="imputation.html"><a href="imputation.html#abstract-9"><i class="fa fa-check"></i><b>2.2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-7"><i class="fa fa-check"></i><b>2.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.2.3" data-path="imputation.html"><a href="imputation.html#methodology-9"><i class="fa fa-check"></i><b>2.2.3</b> Methodology</a></li>
<li class="chapter" data-level="2.2.4" data-path="imputation.html"><a href="imputation.html#results-9"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
<li class="chapter" data-level="2.2.5" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-8"><i class="fa fa-check"></i><b>2.2.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="imputation.html"><a href="imputation.html#comparison-of-efficiency-of-various-data-imputation-techniques"><i class="fa fa-check"></i><b>2.3</b> Comparison of efficiency of various data imputation techniques</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="imputation.html"><a href="imputation.html#abstract-10"><i class="fa fa-check"></i><b>2.3.1</b> Abstract</a></li>
<li class="chapter" data-level="2.3.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-8"><i class="fa fa-check"></i><b>2.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.3.3" data-path="imputation.html"><a href="imputation.html#related-work-6"><i class="fa fa-check"></i><b>2.3.3</b> Related Work</a></li>
<li class="chapter" data-level="2.3.4" data-path="imputation.html"><a href="imputation.html#methodology-10"><i class="fa fa-check"></i><b>2.3.4</b> Methodology</a></li>
<li class="chapter" data-level="2.3.5" data-path="imputation.html"><a href="imputation.html#results-10"><i class="fa fa-check"></i><b>2.3.5</b> Results</a></li>
<li class="chapter" data-level="2.3.6" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-9"><i class="fa fa-check"></i><b>2.3.6</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="interpretability.html"><a href="interpretability.html#building-an-explainable-model-for-ordinal-classification.-meeting-black-box-model-performance-levels."><i class="fa fa-check"></i><b>3.1</b> Building an explainable model for ordinal classification. Meeting black box model performance levels.</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="interpretability.html"><a href="interpretability.html#abstract-11"><i class="fa fa-check"></i><b>3.1.1</b> Abstract</a></li>
<li class="chapter" data-level="3.1.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-9"><i class="fa fa-check"></i><b>3.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.1.3" data-path="interpretability.html"><a href="interpretability.html#related-work-7"><i class="fa fa-check"></i><b>3.1.3</b> Related Work</a></li>
<li class="chapter" data-level="3.1.4" data-path="interpretability.html"><a href="interpretability.html#methodology-11"><i class="fa fa-check"></i><b>3.1.4</b> Methodology</a></li>
<li class="chapter" data-level="3.1.5" data-path="interpretability.html"><a href="interpretability.html#results-11"><i class="fa fa-check"></i><b>3.1.5</b> Results</a></li>
<li class="chapter" data-level="3.1.6" data-path="interpretability.html"><a href="interpretability.html#model-explanantion"><i class="fa fa-check"></i><b>3.1.6</b> Model explanantion</a></li>
<li class="chapter" data-level="3.1.7" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-10"><i class="fa fa-check"></i><b>3.1.7</b> Summary and conclusions</a></li>
<li class="chapter" data-level="3.1.8" data-path="interpretability.html"><a href="interpretability.html#references"><i class="fa fa-check"></i><b>3.1.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="interpretability.html"><a href="interpretability.html#predicting-code-defects-using-interpretable-static-measures."><i class="fa fa-check"></i><b>3.2</b> Predicting code defects using interpretable static measures.</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="interpretability.html"><a href="interpretability.html#abstract-12"><i class="fa fa-check"></i><b>3.2.1</b> Abstract</a></li>
<li class="chapter" data-level="3.2.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-10"><i class="fa fa-check"></i><b>3.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.2.3" data-path="interpretability.html"><a href="interpretability.html#dataset"><i class="fa fa-check"></i><b>3.2.3</b> Dataset</a></li>
<li class="chapter" data-level="3.2.4" data-path="interpretability.html"><a href="interpretability.html#methodology-12"><i class="fa fa-check"></i><b>3.2.4</b> Methodology</a></li>
<li class="chapter" data-level="3.2.5" data-path="interpretability.html"><a href="interpretability.html#results-12"><i class="fa fa-check"></i><b>3.2.5</b> Results</a></li>
<li class="chapter" data-level="3.2.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-11"><i class="fa fa-check"></i><b>3.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="interpretability.html"><a href="interpretability.html#using-interpretable-machine-learning-models-in-the-higgs-boson-detection."><i class="fa fa-check"></i><b>3.3</b> Using interpretable Machine Learning models in the Higgs boson detection.</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="interpretability.html"><a href="interpretability.html#abstract-13"><i class="fa fa-check"></i><b>3.3.1</b> Abstract</a></li>
<li class="chapter" data-level="3.3.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-11"><i class="fa fa-check"></i><b>3.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.3.3" data-path="interpretability.html"><a href="interpretability.html#related-work-8"><i class="fa fa-check"></i><b>3.3.3</b> Related Work</a></li>
<li class="chapter" data-level="3.3.4" data-path="interpretability.html"><a href="interpretability.html#methodology-13"><i class="fa fa-check"></i><b>3.3.4</b> Methodology</a></li>
<li class="chapter" data-level="3.3.5" data-path="interpretability.html"><a href="interpretability.html#results-13"><i class="fa fa-check"></i><b>3.3.5</b> Results</a></li>
<li class="chapter" data-level="3.3.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-12"><i class="fa fa-check"></i><b>3.3.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="interpretability.html"><a href="interpretability.html#can-automated-regression-beat-linear-model"><i class="fa fa-check"></i><b>3.4</b> Can Automated Regression beat linear model?</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="interpretability.html"><a href="interpretability.html#abstract-14"><i class="fa fa-check"></i><b>3.4.1</b> Abstract</a></li>
<li class="chapter" data-level="3.4.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-12"><i class="fa fa-check"></i><b>3.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.4.3" data-path="interpretability.html"><a href="interpretability.html#data-1"><i class="fa fa-check"></i><b>3.4.3</b> Data</a></li>
<li class="chapter" data-level="3.4.4" data-path="interpretability.html"><a href="interpretability.html#methodology-14"><i class="fa fa-check"></i><b>3.4.4</b> Methodology</a></li>
<li class="chapter" data-level="3.4.5" data-path="interpretability.html"><a href="interpretability.html#results-14"><i class="fa fa-check"></i><b>3.4.5</b> Results</a></li>
<li class="chapter" data-level="3.4.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-13"><i class="fa fa-check"></i><b>3.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="interpretability.html"><a href="interpretability.html#interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models---exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric."><i class="fa fa-check"></i><b>3.5</b> Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="interpretability.html"><a href="interpretability.html#abstract-15"><i class="fa fa-check"></i><b>3.5.1</b> Abstract</a></li>
<li class="chapter" data-level="3.5.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-13"><i class="fa fa-check"></i><b>3.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.5.3" data-path="interpretability.html"><a href="interpretability.html#related-work-9"><i class="fa fa-check"></i><b>3.5.3</b> Related Work</a></li>
<li class="chapter" data-level="3.5.4" data-path="interpretability.html"><a href="interpretability.html#methodology-15"><i class="fa fa-check"></i><b>3.5.4</b> Methodology</a></li>
<li class="chapter" data-level="3.5.5" data-path="interpretability.html"><a href="interpretability.html#results-15"><i class="fa fa-check"></i><b>3.5.5</b> Results</a></li>
<li class="chapter" data-level="3.5.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-14"><i class="fa fa-check"></i><b>3.5.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="interpretability.html"><a href="interpretability.html#surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering"><i class="fa fa-check"></i><b>3.6</b> Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="interpretability.html"><a href="interpretability.html#abstract-16"><i class="fa fa-check"></i><b>3.6.1</b> Abstract</a></li>
<li class="chapter" data-level="3.6.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-14"><i class="fa fa-check"></i><b>3.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.6.3" data-path="interpretability.html"><a href="interpretability.html#data-2"><i class="fa fa-check"></i><b>3.6.3</b> Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="interpretability.html"><a href="interpretability.html#related-work-10"><i class="fa fa-check"></i><b>3.6.4</b> Related work</a></li>
<li class="chapter" data-level="3.6.5" data-path="interpretability.html"><a href="interpretability.html#methodology-16"><i class="fa fa-check"></i><b>3.6.5</b> Methodology</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="interpretability.html"><a href="interpretability.html#which-neighbours-affected-house-prices-in-the-90s"><i class="fa fa-check"></i><b>3.7</b> Which Neighbours Affected House Prices in the ’90s?</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="interpretability.html"><a href="interpretability.html#introduction-2"><i class="fa fa-check"></i><b>3.7.1</b> Introduction</a></li>
<li class="chapter" data-level="3.7.2" data-path="interpretability.html"><a href="interpretability.html#related-work-11"><i class="fa fa-check"></i><b>3.7.2</b> Related Work</a></li>
<li class="chapter" data-level="3.7.3" data-path="interpretability.html"><a href="interpretability.html#data-3"><i class="fa fa-check"></i><b>3.7.3</b> Data</a></li>
<li class="chapter" data-level="3.7.4" data-path="interpretability.html"><a href="interpretability.html#methodology-17"><i class="fa fa-check"></i><b>3.7.4</b> Methodology</a></li>
<li class="chapter" data-level="3.7.5" data-path="interpretability.html"><a href="interpretability.html#results-16"><i class="fa fa-check"></i><b>3.7.5</b> Results</a></li>
<li class="chapter" data-level="3.7.6" data-path="interpretability.html"><a href="interpretability.html#conclusions-1"><i class="fa fa-check"></i><b>3.7.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="interpretability.html"><a href="interpretability.html#explainable-computer-vision-with-embeddings-and-knn-classifier"><i class="fa fa-check"></i><b>3.8</b> Explainable Computer Vision with embeddings and KNN classifier</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="interpretability.html"><a href="interpretability.html#abstract-17"><i class="fa fa-check"></i><b>3.8.1</b> Abstract</a></li>
<li class="chapter" data-level="3.8.2" data-path="interpretability.html"><a href="interpretability.html#introduction-3"><i class="fa fa-check"></i><b>3.8.2</b> 3.8.1 Introduction</a></li>
<li class="chapter" data-level="3.8.3" data-path="interpretability.html"><a href="interpretability.html#data-4"><i class="fa fa-check"></i><b>3.8.3</b> 3.8.2 Data</a></li>
<li class="chapter" data-level="3.8.4" data-path="interpretability.html"><a href="interpretability.html#methodology-18"><i class="fa fa-check"></i><b>3.8.4</b> 3.8.3 Methodology</a></li>
<li class="chapter" data-level="3.8.5" data-path="interpretability.html"><a href="interpretability.html#standard-intepretable-models"><i class="fa fa-check"></i><b>3.8.5</b> 3.8.4 Standard Intepretable Models</a></li>
<li class="chapter" data-level="3.8.6" data-path="interpretability.html"><a href="interpretability.html#our-approach"><i class="fa fa-check"></i><b>3.8.6</b> 3.8.5 Our Approach</a></li>
<li class="chapter" data-level="3.8.7" data-path="interpretability.html"><a href="interpretability.html#black-box-convolutional-neural-networks"><i class="fa fa-check"></i><b>3.8.7</b> 3.8.6 Black-Box Convolutional Neural Networks</a></li>
<li class="chapter" data-level="3.8.8" data-path="interpretability.html"><a href="interpretability.html#results-17"><i class="fa fa-check"></i><b>3.8.8</b> Results</a></li>
<li class="chapter" data-level="3.8.9" data-path="interpretability.html"><a href="interpretability.html#conclusions-2"><i class="fa fa-check"></i><b>3.8.9</b> Conclusions</a></li>
<li class="chapter" data-level="3.8.10" data-path="interpretability.html"><a href="interpretability.html#bibliography"><i class="fa fa-check"></i><b>3.8.10</b> Bibliography</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>4</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpretability" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Interpretability</h1>
<p>Interpretability</p>

<div id="building-an-explainable-model-for-ordinal-classification.-meeting-black-box-model-performance-levels." class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Building an explainable model for ordinal classification. Meeting black box model performance levels.</h2>
<p><em>Authors: Karol Saputa, Małgorzata Wachulec, Aleksandra Wichrowska (Warsaw University of Technology)</em></p>
<div id="abstract-11" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-9" class="section level3" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Introduction and Motivation</h3>
<p>In the classification problems, the main goal is to map inputs to a categorical target variable. Most machine learning algorithms assume that the class attribute is unordered. However, there are many problems where target variable is ranked, for example while predicting movie ratings. When applying standard methods to such problems, we lose some informaton, which could improve our model performance.</p>
<p>This paper presents various methods to make an explainable machine learning model for ordinal classification problem. The aim is to achieve better results than ‘black box’ model does. We will test some existing approaches to ordinal classification and take advantage of analysis and imputation of missing data, feature transformation and selection, as well as knowledge from exploratory data analysis.</p>
<p>Our experiments are based on ‘eucalyptus’ dataset from OpenML. The dataset’s objective is to find the best seedlot for soil conservation in seasonally dry hill country. Predictions are made depending on features such as height, diameter and survival of the plants. Target variable is ordered - it is represented by values ‘low’, ‘average’, ‘good’ and ‘best’.</p>
</div>
<div id="related-work-7" class="section level3" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Related Work</h3>
<div id="an-approach-to-ordinal-classification" class="section level4" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> An approach to ordinal classification</h4>
<div id="ordinal-classification-as-a-regression-task" class="section level5" number="3.1.3.1.1">
<h5><span class="header-section-number">3.1.3.1.1</span> Ordinal classification as a regression task</h5>
<p>As described in [2], one of the most fundamental techniques is to cast target labels to a sequence of numbers. Then, a standard regression can be applied. There is an additional information about the classes in comparison to usual nominal classification. Also a metric used is different than in a classification task - mean square error used in a regression task takes into account similarities between two labels when conversion is applied.</p>
</div>
<div id="transformation-to-multiple-binary-classification-problems" class="section level5" number="3.1.3.1.2">
<h5><span class="header-section-number">3.1.3.1.2</span> Transformation to multiple binary classification problems</h5>
</div>
</div>
<div id="comparing-black-box-to-linear-model" class="section level4" number="3.1.3.2">
<h4><span class="header-section-number">3.1.3.2</span> Comparing black box to linear model</h4>
</div>
</div>
<div id="methodology-11" class="section level3" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Methodology</h3>
<div id="initial-preprocessing" class="section level4" number="3.1.4.1">
<h4><span class="header-section-number">3.1.4.1</span> Initial preprocessing</h4>
<p>The aim of this article was to build the best interpretable model for an ordinal classification problem and comparing it to a black box model. First undertaken step was dividing the data into training and test sets, consisting of 70% and 30% of all data, respectively. Since the considered dataset has a categorical target variable, the division was done using random sampling within each class of the target variable in an attempt to balance the class distributions within the splits. A seed was used to assure the same split in each tested model.</p>
<p>In order to get a legitimate comparison, the data was initially preprocessed in such a way as to assure that both models’ performances are compared on the same test set. This initial preprocessing included:</p>
<ol style="list-style-type: decimal">
<li><p>deleting the observations with ‘none’ value in the target variable from both the training set and the test set;</p></li>
<li><p>deleting observations with missing values from test set, resulting in a 6% decease of the test set observations.</p></li>
</ol>
<p>It is important to note that missing values are still present in the training set.</p>
<p>The reason why the missing data is deleted from the test set is that many of the explainable models cannot be run with missing data present. This means that the missing values will have to either be deleted or imputed later on. This leads to a possibility that the explainable model will impute missing data differently than the black box model, resulting in two different tests sets. And, if - instead of imputing missing data - we decide to delete it in order to make running explainable model possible, then the obtained test sets will differ in number of rows, making it impossible to draw any meaningful conclusions. Hence the missing data were deleted from the test set.</p>
</div>
<div id="running-the-black-box-model" class="section level4" number="3.1.4.2">
<h4><span class="header-section-number">3.1.4.2</span> Running the black box model</h4>
<p>The black box model chosen for comparison is an extreme gradient boosting model. After the initial preprocessing the xgboost model was trained on the training set and used to predict results on the test set. As this model can only deal with numerical data, categorical (factor) variables were transformed using one hot encoding. The training proces and prediction were done using the mlr package in R, and the exact model specifications were the following:</p>
<ul>
<li>Learner classif.xgboost from package xgboost</li>
<li>Type: classif</li>
<li>Name: eXtreme Gradient Boosting; Short name: xgboost</li>
<li>Class: classif.xgboost</li>
<li>Properties: twoclass,multiclass,numerics,prob,weights,missings,featimp</li>
<li>Predict-Type: response</li>
<li>Hyperparameters: nrounds=200,verbose=0,objective=multi:softmax</li>
</ul>
<p>The quality of prediction was measured using the AUC (area under ROC curve) measure. This provides the base for this research, to which other models’ results will be compared to.</p>
</div>
<div id="running-the-basic-version-the-explainable-model" class="section level4" number="3.1.4.3">
<h4><span class="header-section-number">3.1.4.3</span> Running the basic version the explainable model</h4>
<p>We have chosen a tree model to be the considered explainable model, its exact specifications were the following:</p>
<ul>
<li>Learner classif.rpart from package rpart</li>
<li>Type: classif</li>
<li>Name: Decision Tree; Short name: rpart</li>
<li>Class: classif.rpart</li>
<li>Properties: twoclass,multiclass,missings,numerics,factors,ordered,prob,weights,featimp</li>
<li>Predict-Type: response</li>
<li>Hyperparameters: xval=0</li>
</ul>
<p>As this model cannot be run with missing data, they were deleted from the training set before training the model. Another step was deleting one from each of the one-hot-encoded variables (the default function transforms variable with n factor levels into n columns, but n-1 columns are sufficient as the n-th column is a linear combination of the remaining n-1 columns). This model performed worse than the black box model - the outcomes are presented in the Results section of this article.</p>
</div>
<div id="improving-the-explainable-model" class="section level4" number="3.1.4.4">
<h4><span class="header-section-number">3.1.4.4</span> Improving the explainable model</h4>
<p>As mentioned before, the explainable model was enhanced by applying existing approaches to ordinal classification, feature transformation and selection and missing data imputation. The refinement process consisted of, but was not limited to, the following:</p>
<ol style="list-style-type: decimal">
<li>Splitting a multiclass classification problem into 3 binary classification problems, like explained in the An approach to ordinal classification section of this article, and using the rpart model on each of the binary problems.</li>
<li>Changing the levels of the target variable:“low”, “average”, “good”, “best” into numeric values: 1, 2, 3, 4, respectively and running a regression rpart model.</li>
<li>Imputing missing data in the training set.</li>
<li>Selecting variables: deleting the site names and specific location tags.</li>
<li>Transforming Latitude variable from factor to numeric.</li>
</ol>
<p>The fourth step has a scientific justification. The experiment for which the data was collected was focused on finding the best seedlot for soil conservation in seasonally dry hill country. All the data in this dataset comes from New Zealand, but there is a chance that the results of such experiment would be used for other geographical regions. So far our model was making the prediction based also on specific flat map coordinates and site names, that are present both in the training and the test set. This means it would be impossible to use this model for judging seedlots of eucalypti planted outside of New Zealand. To make this possible, we have decided to take away all the variables that give away the exact position of the seedlots, leaving features such as latitude and the characteristics of plants and their habitat.</p>
<p>After each improvement the model was retrained and the results obtained on the test set were saved and compared with the previous version of the model. If the new change has improved the model’s performance on the test set then it became the base for further development. Instead, if it has not improved the model’s performance, the previous version of the model was being further developed.</p>
</div>
</div>
<div id="results-11" class="section level3" number="3.1.5">
<h3><span class="header-section-number">3.1.5</span> Results</h3>
<p><img src="3-1-graphs/CompareModels.png" /></p>
<p><strong>Explainable models</strong>:</p>
<table>
<colgroup>
<col width="29%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="12%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>AUC</th>
<th>MSE</th>
<th>ACC</th>
<th>ACC1</th>
<th>Percent Best AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Basic rpart</td>
<td>0.8259145</td>
<td>0.5284076</td>
<td>0.5835207</td>
<td>0.9797400</td>
<td>95,89%</td>
</tr>
<tr class="even">
<td>Three binary rparts</td>
<td>0.8430115</td>
<td>0.5392606</td>
<td>0.5816277</td>
<td>0.9826626</td>
<td>97,88%</td>
</tr>
<tr class="odd">
<td>Regression rpart</td>
<td>0.86115629</td>
<td>0.4994688</td>
<td>0.5814851</td>
<td>0.93207583</td>
<td>99,99%</td>
</tr>
<tr class="even">
<td>Regression rpart with imputation</td>
<td>0.8597930</td>
<td>0.5038168</td>
<td>0.5798347</td>
<td>0.9306883</td>
<td>99,83%</td>
</tr>
<tr class="odd">
<td>Regression rpart with no location</td>
<td>0.8612852</td>
<td>0.4995913</td>
<td>0.5815428</td>
<td>0.9323226</td>
<td>100,00%</td>
</tr>
<tr class="even">
<td>Regression rpart with no location and numeric lattitide</td>
<td>0.8612474</td>
<td>0.4993377</td>
<td>0.5816065</td>
<td>0.9322628</td>
<td>100,00%</td>
</tr>
</tbody>
</table>
<p><strong>Black box models</strong>:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>AUC</th>
<th>MSE</th>
<th>ACC</th>
<th>ACC1</th>
<th>Percent Best AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Xgboost</td>
<td>0.8589771</td>
<td>0.4466546</td>
<td>0.6247675</td>
<td>0.9873244</td>
<td>99.73%</td>
</tr>
<tr class="even">
<td>Xgboost with nrounds=5</td>
<td>0.8405102</td>
<td>0.4997935</td>
<td>0.6043936</td>
<td>0.9830193</td>
<td>97,59%</td>
</tr>
</tbody>
</table>
</div>
<div id="model-explanantion" class="section level3" number="3.1.6">
<h3><span class="header-section-number">3.1.6</span> Model explanantion</h3>
</div>
<div id="summary-and-conclusions-10" class="section level3" number="3.1.7">
<h3><span class="header-section-number">3.1.7</span> Summary and conclusions</h3>
</div>
<div id="references" class="section level3" number="3.1.8">
<h3><span class="header-section-number">3.1.8</span> References</h3>
<ol style="list-style-type: decimal">
<li>Frank, Eibe &amp; Hall, Mark. (2001). A Simple Approach to Ordinal Classification. Lecture Notes in Computer Science. 2167. 145-156. 10.1007/3-540-44795-4_13</li>
<li>P. A. Gutiérrez, M. Pérez-Ortiz, J. Sánchez-Monedero, F. Fernández-Navarro and C. Hervás-Martínez, “Ordinal Regression Methods: Survey and Experimental Study,” in IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 1, pp. 127-146, 1 Jan. 2016, doi: 10.1109/TKDE.2015.2457911.</li>
</ol>

</div>
</div>
<div id="predicting-code-defects-using-interpretable-static-measures." class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Predicting code defects using interpretable static measures.</h2>
<p><em>Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology)</em></p>
<div id="abstract-12" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-10" class="section level3" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Introduction and Motivation</h3>
<p>Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, <span class="citation">(<a href="#ref-mccabe76" role="doc-biblioref">1976</a>)</span> <span class="citation">(<a href="#ref-halstead77" role="doc-biblioref">1977</a>)</span>). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program <span class="citation">(Fenton and Pfleeger <a href="#ref-fenton97" role="doc-biblioref">1997</a>)</span>.</p>
<p>To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white-box models (e.g. trees, linear regression and k nearest neighbors algorithm). Our goal is to build, using simple data transformations and easily explainable methods, such model that will achieve results comparable to the black-box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated.</p>
</div>
<div id="dataset" class="section level3" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Dataset</h3>
<p>Our dataset comes from the original research of Halstead and McCabe. We obtain it by combining the sets from <code>OpenML</code> <span class="citation">(Vanschoren et al. <a href="#ref-OpenML2013" role="doc-biblioref">2013</a>)</span> and supplementing them with data from the <code>PROMISE</code> repository <span class="citation">(Sayyad Shirabad and Menzies <a href="#ref-Sayyad-Shirabad+Menzies:2005" role="doc-biblioref">2005</a>)</span>.</p>
<p>It contains data collected from NASA systems written in <code>C</code> and <code>C++</code> languages. The data is in the form of a data frame containing more than <span class="math inline">\(15000\)</span> records. Each record describes one “program module”. – with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups:</p>
<ul>
<li>Basic measures (such as number of lines of code, number of operands, etc.).</li>
<li>McCabe’s measures how complex the code is in terms of control flow and cross-references .</li>
<li>Halstead’s measures for general code readability.</li>
<li>Target column (1 if module contains defects, 0 if not).</li>
<li>Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems).</li>
</ul>
<p>The dataset is slightly imbalanced – about <span class="math inline">\(20\%\)</span> of records are classified as having defects.</p>
<p>In order to verify our hypotheses, we decide at the beginning to remove the Halstead’s measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black-box model without them. We also wanted to remove McCabe’s measurements, but the basic measurements that he used to calculate his measurements are not preserved in the dataset, so we decide to keep them.</p>
<p>There are not many records with openly missing data in the set (<span class="math inline">\(&lt; 1\%\)</span>), however, the values of some columns raise doubts – in the column containing information about the number of lines of code of a given module in many cases there is a value <span class="math inline">\(0\)</span>, which is not reliable. However, it turned out during the preliminary analysis that deleting those records significantly worsens the model score, so we decided to keep them.</p>
</div>
<div id="methodology-12" class="section level3" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Methodology</h3>
<p>Our research consist of the following stages:</p>
<ol style="list-style-type: decimal">
<li><p>Data exploration.</p></li>
<li><p>Initial data preparation.</p></li>
<li><p>Building of black-box and white-box models and comparing them against the relevant measurements.</p></li>
<li><p>Repeating the cycle:</p>
<ol style="list-style-type: lower-alpha">
<li>Improvement of white-box models by modifying their parameters or data.<br />
</li>
<li>Measuring the effectiveness of the models built.</li>
<li>Analysis of the resulting models.</li>
<li>Keeping or rejecting changes for further work.</li>
</ol></li>
<li><p>Selection of the best white-box model and final comparison with the black-box model.</p></li>
</ol>
<p>During the step 4. in some cases we decide to take a step back and use other similar transformation or other order of transformations if we suspect that it can yield a better result.</p>
<p>We use <code>R</code> programming language and popular machine learning project management packages – <code>mlr</code> <span class="citation">(Bischl et al. <a href="#ref-mlr" role="doc-biblioref">2016</a><a href="#ref-mlr" role="doc-biblioref">b</a>)</span> and <code>drake</code> <span class="citation">(Landau <a href="#ref-drake" role="doc-biblioref">2018</a>)</span>.</p>
<div id="data-exploration" class="section level4" number="3.2.4.1">
<h4><span class="header-section-number">3.2.4.1</span> Data exploration</h4>
<p>At this stage, we take a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships.</p>
</div>
<div id="initial-data-preparation" class="section level4" number="3.2.4.2">
<h4><span class="header-section-number">3.2.4.2</span> Initial data preparation</h4>
<p>This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column (in fact, we add five indicator columns, which contain one-hot-encoded value, as models generally do not cope well with character columns). Since there is not much missing data, we impute them with the median, because this method is effective and fast.</p>
<p>Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that would still work. We do not carry out further transformations at this stage because we do not want to disturb the results of the next stage.</p>
</div>
<div id="starting-models" class="section level4" number="3.2.4.3">
<h4><span class="header-section-number">3.2.4.3</span> Starting models</h4>
<p>We build models on this almost unaltered data. We use one poorly interpretable model (black-box) – random forest, specifically <code>ranger</code> package <span class="citation">(Wright and Ziegler <a href="#ref-ranger" role="doc-biblioref">2017</a>)</span>, because it is fast and low-effort. Among well interpretable models (white-boxes) used in our work there are:</p>
<ul>
<li>logistic regression (<code>lm</code>),</li>
<li>decision tree (<code>rpart</code>),</li>
<li>k-nearest neighbors algorithm (<code>kknn</code>).</li>
</ul>
<p>We train the models into data that we have divided into five folds with a similar distribution of the decision variable, on which we will perform cross-validation. Then we compare the results using commonly used measure – AUC (Area Under Curve) <span class="citation">(Flach, Hernandez-Orallo, and Ferri <a href="#ref-auc" role="doc-biblioref">2011</a>)</span>, which not only assesses whether the observations are well classified, but also takes into account the likelihood of belonging to a class. AUC is not the best measure to be used on imbalanced dataset. However, the unbalance here is not big enough to make this choice unreliable. We use AUC as the main comparative criterion of the models also in the further part of our work.</p>
</div>
<div id="improving-white-boxes" class="section level4" number="3.2.4.4">
<h4><span class="header-section-number">3.2.4.4</span> Improving white-boxes</h4>
<p>This is a key part of our work. In the iterative cycle we use different methods to improve the quality of the white-box models. After applying each of these methods, we check whether it has improved our score and possibly analyze the model, using statistical methods (residuals analysis) and explanatory machine learning (<code>DALEX</code> package <span class="citation">(Biecek <a href="#ref-DALEX" role="doc-biblioref">2018</a>)</span>), to draw indications of what should be done next.</p>
<p>We are trying the following methods:</p>
<ul>
<li><strong>Tuning hyperparameters of models</strong> – Default hyperparameters for models are generally good, but in specific cases using specific hyperparameters may yield in better results, so we use model-based optimization for tuning those parameters <span class="citation">(Bischl et al. <a href="#ref-mlrmbo" role="doc-biblioref">2017</a>)</span>.</li>
<li><strong>Reducing outliers</strong> – For each variable a two-value vector that indicates the thresholds for which the values are considered as outliers is generated. Then all outliers are changed to the nearest value of obtained earlier vector.</li>
<li><strong>Logarithmic and exponential transformations of individual variables</strong> – So that linear relationships can be better captured and to reduce the influence of outliers, we transform variables using exponential and polynomial functions.</li>
<li><strong>Discretization of continuous features</strong> – Some variables do not have a linear effect on the response variable, even if they are transformed by simple functions like exponential function, sometimes there are clear thresholds – so we can replace the variable with indexes of individual segments. The <code>SAFE</code> algorithm helps with this <span class="citation">(Gosiewska et al. <a href="#ref-gosiewska2019safe" role="doc-biblioref">2019</a>)</span>.</li>
<li><strong>Generating new columns as functions of other columns</strong> – There may be interactions between variables that cannot be captured by linear models. In order to take them into account, we generate new columns, applying to the rest of them various transformations – we take their inverses, products, quotients, elevations to power, and so on. As a result of these operations, a lot of new measures, potentially simillar to those proposed by McCabe and Halstead, are created, which we later evaluate. We also analyze their interpretability, i.e. to what extent they are translatable into an intuitive understanding of such a measure. At this point we also consider Halstead and McCabe’s measurements.
A model with thousands of variables is not well interpretable, so we need to select meaningful measures. We do this by training rpart and ranger models with these additional features and we use <code>DALEX</code>to select the significant ones. We do it twice so at the and we had around 10 of the most important features.</li>
<li><strong>Oversampling</strong> – On the basis of the data set, we generate more observations from the minority class using the <code>SMOTE</code> algorithm <span class="citation">(Hu and Li <a href="#ref-smote" role="doc-biblioref">2013</a>)</span> so that the model more emphasizes the differences in characteristics of individual classes. In order to avoid model overfitting, we generate data within each fold separately and during crossvalidation we use 4 folds with synthetic data as a train set and the 5th fold without synthetic data as a test set.</li>
</ul>
<p>Our goal is to beat black-box model. In our case we chose random forest model from package ranger. As a white-box model we used logistic regression. Results were tested on dataset with different transformations.</p>
</div>
<div id="selecting-the-best-model" class="section level4" number="3.2.4.5">
<h4><span class="header-section-number">3.2.4.5</span> Selecting the best model</h4>
<p>At the end of the process, we select the model that has the highest AUC score for crossvalidation on our dataset.</p>
</div>
</div>
<div id="results-12" class="section level3" number="3.2.5">
<h3><span class="header-section-number">3.2.5</span> Results</h3>
<p>Our base black-box model result is <span class="math inline">\(0.792\)</span>. The results of individual models after applying transformations are shown in the Table 1.</p>
<table>
<thead>
<tr class="header">
<th>Order</th>
<th>Applied operation</th>
<th>logreg</th>
<th>kknn</th>
<th>rpart</th>
<th>Kept?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>-</td>
<td>Base</td>
<td><span class="math inline">\(0.735\)</span></td>
<td><span class="math inline">\(0.728\)</span></td>
<td><span class="math inline">\(0.500\)</span></td>
<td>-</td>
</tr>
<tr class="even">
<td>0</td>
<td><code>rpart</code> tuning</td>
<td><span class="math inline">\(0.735\)</span></td>
<td><span class="math inline">\(0.728\)</span></td>
<td><span class="math inline">\(0.737\)</span></td>
<td><strong>yes</strong></td>
</tr>
<tr class="odd">
<td>1a</td>
<td>Normalization</td>
<td><span class="math inline">\(0.735\)</span></td>
<td><span class="math inline">\(0.727\)</span></td>
<td><span class="math inline">\(0.737\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>1b</td>
<td>Outlier reduction</td>
<td><span class="math inline">\(0.743\)</span></td>
<td><span class="math inline">\(0.732\)</span></td>
<td><span class="math inline">\(0.739\)</span></td>
<td>no</td>
</tr>
<tr class="odd">
<td>1c</td>
<td>Logarithm</td>
<td><span class="math inline">\(0.744\)</span></td>
<td><span class="math inline">\(0.718\)</span></td>
<td><span class="math inline">\(0.725\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>2a</td>
<td>Outlier reduction and normalization</td>
<td><span class="math inline">\(0.743\)</span></td>
<td><span class="math inline">\(0.732\)</span></td>
<td><span class="math inline">\(0.739\)</span></td>
<td><strong>yes</strong></td>
</tr>
<tr class="odd">
<td>2b</td>
<td>Logarithm and outlier reduction</td>
<td><span class="math inline">\(0.744\)</span></td>
<td><span class="math inline">\(0.717\)</span></td>
<td><span class="math inline">\(0.725\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>3a</td>
<td>Gain-ratio discretization</td>
<td><span class="math inline">\(0.743\)</span></td>
<td><span class="math inline">\(0.732\)</span></td>
<td><span class="math inline">\(0.739\)</span></td>
<td>no</td>
</tr>
<tr class="odd">
<td>3b</td>
<td><code>rSAFE</code></td>
<td><span class="math inline">\(0.744\)</span></td>
<td><span class="math inline">\(0.718\)</span></td>
<td><span class="math inline">\(0.734\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>4a</td>
<td>New features selected by <code>ranger</code></td>
<td><span class="math inline">\(0.747\)</span></td>
<td><span class="math inline">\(0.729\)</span></td>
<td><span class="math inline">\(0.733\)</span></td>
<td>no</td>
</tr>
<tr class="odd">
<td>4b</td>
<td>New features selected by <code>rpart</code></td>
<td><span class="math inline">\(0.745\)</span></td>
<td><span class="math inline">\(0.731\)</span></td>
<td><span class="math inline">\(0.739\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>4c</td>
<td>Halstead’s measures</td>
<td><span class="math inline">\(0.745\)</span></td>
<td><span class="math inline">\(0.731\)</span></td>
<td><span class="math inline">\(0.738\)</span></td>
<td>no</td>
</tr>
<tr class="odd">
<td>5a</td>
<td><code>SMOTE</code> with new features by <code>ranger</code></td>
<td><span class="math inline">\(0.749\)</span></td>
<td><span class="math inline">\(0.737\)</span></td>
<td><span class="math inline">\(0.800\)</span></td>
<td>no</td>
</tr>
<tr class="even">
<td>5b</td>
<td><code>SMOTE</code> with new features by <code>rpart</code></td>
<td><span class="math inline">\(0.747\)</span></td>
<td><span class="math inline">\(0.736\)</span></td>
<td><span class="math inline">\(0.793\)</span></td>
<td>no</td>
</tr>
<tr class="odd">
<td>5c</td>
<td><code>SMOTE</code> without new features</td>
<td><span class="math inline">\(0.745\)</span></td>
<td><span class="math inline">\(0.736\)</span></td>
<td><span class="math inline">\(0.804\)</span></td>
<td><strong>yes</strong></td>
</tr>
</tbody>
</table>
<p>Table 1: AUC in white-box models. Each row describes one of the operation that we apply in order to obtain improvement and results of individual models. The first column indicates the order in which we used these transformations. The letters “a, b, c” indicate that we used different transformations with “backtracking”, i.e. we applied them parallel to the same model as they are similar. The last column informs if we decided to keep the change or reject it.</p>
<p>Since at the beginning <code>rpart</code> had the AUC value of <span class="math inline">\(0.5\)</span>, firstly we tuned its hyperparameters and there was significant improvement in result.</p>
<p>Then as step 1a-1c we tried 3 basic transformations of data as normalization of all columns, outlier reduction and logarithm of nearly all columns (without one hot encoded source). As we can see the best improvement was achieved by using outliers reduction so in step 2a we tried to add normalization to that and in step 2b we tried to reduce outliers after applying logarithm. Only outliers reduction and normalization were good enough to be used in the following experiments.</p>
<p>Next we used discretization of some columns. Unfortunately, any of the gain-ratio method from <code>funModeling</code> (3a) and <code>rSAFE</code> (3b) made statistically important better result. Using partial dependency plot on the black-box model do not show that there is variable which can be discretized.
<!-- plot --></p>
<p>Then we tried to create new measures based on basic Halstead measures. We selected best measures by <code>DALEX</code> variable importance on one of two learners: ranger or <code>rpart</code>. Then on we test the models after adding the best 10 measures to the dataset, but the results were not promising except from the logistic regression in 4a.</p>
<p>Use of the <code>SMOTE</code> algorithm gave a huge improvement especially in decision tree and the best result is in dataset with only outliers reduction and normalization without any new features.</p>
</div>
<div id="summary-and-conclusions-11" class="section level3" number="3.2.6">
<h3><span class="header-section-number">3.2.6</span> Summary and conclusions</h3>
<p><img src="3-2-graphs/rpart_result.png" /> <!-- add some summary --></p>
<p>The graph shows the improvement of the AUC compared to the difference between the original black-box model and the white-box model. <span class="math inline">\(0\)</span> represents the result of the <code>rpart</code>, <span class="math inline">\(100\)</span> represents the result of the <code>ranger</code>.</p>

</div>
</div>
<div id="using-interpretable-machine-learning-models-in-the-higgs-boson-detection." class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Using interpretable Machine Learning models in the Higgs boson detection.</h2>
<p><em>Authors: Mateusz Bakala, Michal Pastuszka, Karol Pysiak (Warsaw University of Technology)</em></p>
<div id="abstract-13" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Abstract</h3>
<p>In this section we compare efficiency of explainable and black-box AI models in detection of the Higgs boson. Afterwards, we explore possible improvements to explainable models which might boost their accuracy to be on par with black-boxes, of which the most notable is R package called <code>rSAFE</code>. Lastly, we conclude that (((conclusion depends on results))).</p>
</div>
<div id="introduction-and-motivation-11" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Introduction and Motivation</h3>
<p>‘We fear that, which we do not understand’. This principle has often been used to portray artificial intelligence in popular culture. The notion that it is something beyond human comprehension became so common, that it started to affect the way we perceive computer algorithms. Because of that, it comes as no surprise that people become wary, hearing that the decisions which affect them directly were made by an AI.</p>
<p>Machine Learning models, regardless of the algorithm used, are often treated as a black box that takes some data as an input and returns a prediction based on said data. This approach is most often seen from people who are unfamiliar with the methods used. In many fields, where the only concern is achieving results that are as accurate as possible, this is not a concern. There are however situations, where the risk of unpredictable or unjustified results is unacceptable. This includes uses in medical and judicial systems, where an incorrect decision may have severe consequences. Artificial intelligence could find plenty of uses in those areas, but it would require creating models, which make decisions based on a transparent set of rules. Those include algorithms such as decision trees and linear regression.</p>
<p>That raises another problem. Such models often underperform in comparison to more advanced algorithms, notably artificial neural networks and ensemble based solutions, which are more capable at detecting nonlinearities and interactions in the data. Being able to achieve results comparable with said models, while retaining explainability would allow to increase the role of machine learning in fields where transparency is required. Furthermore, it would provide tools suitable for scientific analysis of relations between complex phenomena.</p>
<p>The problem we study tackles methods of improving the predictions of simple models while retaining their transparency. In the rest of the paper we will describe methods of transforming the data to increase the quality of predictions. Most notably we will focus on a tool named rSAFE, which allows us to extract nonlinearities in the data, based on predictions of a so called ‘surrogate’ model.</p>
</div>
<div id="related-work-8" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Related Work</h3>
<p><a href="https://arxiv.org/pdf/1402.4735.pdf" class="uri">https://arxiv.org/pdf/1402.4735.pdf</a> – Searching for Exotic Particles in High-Energy Physics with Deep Learning</p>
</div>
<div id="methodology-13" class="section level3" number="3.3.4">
<h3><span class="header-section-number">3.3.4</span> Methodology</h3>
<div id="data" class="section level4" number="3.3.4.1">
<h4><span class="header-section-number">3.3.4.1</span> Data</h4>
<p>Modern high energy physics is being done in colliders, of which the best known is Large Hadron Collider near Geneva. Here, basic particles are accelerated to collide at almost speed of light, resulting in creation of other, searched for particles. However, not every collision leads to desired processes and even then target particles may be highly unstable.</p>
<p>One of such tasks is producing Higgs bosons by colliding two gluons. If a collision is successful, two gluons fuse into electrically-neutral Higgs boson, which then decays into a W boson and electrically-charged Higgs boson, which in turn decays into another W boson and the light Higgs boson. The last one decays mostly into two bottom quarks. However, if a collisions isn’t successful, two gluons create two top quarks, decaying then into a W boson and a bottom quark each, resulting in the same end-products, but without a Higgs boson in an intermediate state. The first process is called ‘signal’, while the second – ‘background’.</p>
<p>Our dataset is a dataset called ‘higgs’, retrieved from OpenML database. It contains about 100 thousand rows of data generated with an event generator using Monte Carlo method and is a subset of a <a href="archive.ics.uci.edu/ml/datasets/HIGGS">HIGGS dataset from UCI Machine Learning Repository</a>. The purpose of this data is to be used as a basis for approximation of a likelihood function, which in turn is used to extract a subspace of high-dimensional experiment data where null hypothesis can be rejected, effectively leading to a discovery of a new particle.</p>
<p>Each row describes event, which must satisfy a set of requirements. Namely, exactly one electron or muon must be detected, as well as no less than four jets, every of these with the momentum transverse to the beam direction of value <span class="math inline">\(&gt; 20 GeV\)</span> and an absolute value of pseudorapidity less than <span class="math inline">\(2.5\)</span>. Also, at least two of the jets must have b-tag, meaning than they come from bottom quarks.</p>
<p>Each event is described by its classification, 1 if a Higgs boson took part in the process or 0 if not. This column is our target, so it’s a two-class classification problem. There are 21 columns describing various low-level features, amongst which there are 16 describing momentum transverse to the beam direction (<code>jetXpt</code>), pseudorapidity (<code>jetXeta</code>), azimuthal angle (<code>jetXphi</code>) and presence of b-tag (<code>jetXb-tag</code>) for each of the four jets, as well as three first of these for the lepton (that is, electron or muon) and missing energy magnitude and its azimuthal angle.</p>
<p>There are also 7 columns describing reconstructed invariant mass of particles appearing as non-final products of the collision event. Names like <code>m_jlv</code> or <code>m_wbb</code> mean that the mass reconstructed is of a particle which products consist of jet, lepton and neutrino or a W boson and two bottom quarks respectively. In this case, these particles are namely top quark and electrically-charged Higgs boson. These columns are so-called top-level features.</p>
</div>
<div id="measures" class="section level4" number="3.3.4.2">
<h4><span class="header-section-number">3.3.4.2</span> Measures</h4>
<p>The first measure that we will use is a simple accuracy. It can be misleading, especially when the target variable is unbalanced, but it is very intuitive and in our dataset the target variable is well-balanced.</p>
<p>Second measure is the AUC score, which stands for Areas Under Curve of Receiver Operating Characteristic. It requires probabilities as an output of the model. We create the curve by moving the threshold of a minimal probability of a positive prediction from 0 to 1 and calculating the true positive rate to false positive rate ratio. This measure is much less prone to give falsely high scores than accuracy. The worst score in the AUC is <span class="math inline">\(0.5\)</span>. We want to get AUC score as close as possible to <span class="math inline">\(1\)</span>, however, close to <span class="math inline">\(0\)</span> is not bad, we just must invert the labels and we will get a close to <span class="math inline">\(1\)</span> AUC score.</p>
<p>Our last measure is the Area Under the Precision Recall Curve (AUPRC). It is very similar to AUC score, we just use Precision and Recall measures instead of true and false positive rates. It is quite resistant to a bad balance of the target variable. We want to reach the score as close as possible to <span class="math inline">\(1\)</span> and the score of <span class="math inline">\(0\)</span> is the worst possible case.</p>
</div>
<div id="models" class="section level4" number="3.3.4.3">
<h4><span class="header-section-number">3.3.4.3</span> Models</h4>
<p>The main goal of this research is to find a way of enhancing the performance of interpretable models, so now we will choose algorithms for this task. For the first test algorithm we chose the logistic regression. It is one of the simplest classification methods. Like in the linear regression algorithm we fit a function to the training data, just instead of a linear function we use a logistic function for the logistic regression algorithm. So the interpretability amounts to the understanding of how points are spread throughout the space.</p>
<p>The second algorithm that we will use is a decision tree. It’s structure is very intuitive. We start our prediction from the root of a tree. Then we go up to the leaves basing our path on conditions stated in particular nodes. It is similar to how humans make their decisions. If we have to make one big decision we divide it into many smaller decisions which are much easier to make and after answering some yes/no questions we reach the final answer to the question, so to interpret this model we have to understand how these small decisions in every node are made.</p>
<p>Black box models usually have very complex structure, which gives them advantage in recognising complex structures of the data. On the other hand, they are very hard to interpret and explain compared to white box models. There is a package in the R language named rSAFE that meets halfway between black box models and white box models. It is a kind of a hybrid that uses black box models for extracting new features from the data and then using them to fit interpretable models. We will use it for enhancing our interpretable model and compare them to pure white box models.</p>
<p>To have some perspective of what level of quality of predictions we can reach we will test some black box models. The black box model that we will use is ranger. This is a fast implementation of random forests. This is a light and tunable model, so we will try to reach the best performance as we can with this algorithm.</p>
<p>For easily comparable results we will use exactly the same training and testing dataset. However, we do not limit our feature engineering to only one process for all models, because every algorithm can perform better with different features than the rest. What is more, we will focus on augmenting data in a way that will enable us to get as much as we can from simple, interpretable models.</p>
</div>
</div>
<div id="results-13" class="section level3" number="3.3.5">
<h3><span class="header-section-number">3.3.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-12" class="section level3" number="3.3.6">
<h3><span class="header-section-number">3.3.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="can-automated-regression-beat-linear-model" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Can Automated Regression beat linear model?</h2>
<p><em>Authors: Bartłomiej Granat, Szymon Maksymiuk, (Warsaw University of Technology)</em></p>
<div id="abstract-14" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-12" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Introduction and Motivation</h3>
<p>Health-related problems have been a topic of multiple papers throughout the years and machine learning brought some new methods to modern medicine. We care so much about our lives that every single algorithm and method eventually gets tested on some medical data.</p>
<p>What is unique about health data is that black-box models are completely useless in this subject. Almost always doctors know whether a patient is sick or not. What is important to them is the reason <strong>why</strong> he is sick. That’s why explainable machine learning is the key to make all of us healthier. However, making a good explainable model for health data might be close to impossible. Medical problems of all kinds can be very unique, complex, or completely random. That’s why researchers spend numerous hours on improving their explainable models and that’s why <strong>we</strong> decided to test our approach on <code>liver disorders</code> dataset.</p>
<p>The following dataset is well known in the field of machine learning and that’s exactly the reason why we chose it. It is described in the next chapter. Our goal was to find a relatively clean dataset with many models already done by other people. We don’t want to show that properly cleaned data gives better results but to achieve, an explainable model found after a complex analysis that we want to test.</p>
<p>In this paper we do a case study on <code>liver disorders</code> dataset and want to prove that by using automated regression it is possible to build an easy to understand prediction that outperforms black-box models on the real dataset and at the same time achieve similar results to other researchers.</p>
</div>
<div id="data-1" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Data</h3>
<p>The dataset we use to test our hypothesis is a well-known <code>liver-disorders</code> first created by ‘BUPA Medical Research Ltd.’ containing a single male patient as a row. The data consists of 5 features which are the results of blood tests a physician might use to inform diagnosis. There is no ground truth in the data set relating to the presence or absence of a disorder. The target feature is attribute drinks, which are numerical. Some of the researchers tend to split the patients into 2 groups: 0 - patients that drink less than 3 half-pint equivalents of alcoholic beverages per day and 1 - patients that drink more or equal to 3 and focus on a classification problem.</p>
<p>All of the features are numerical. The data is available for 345 patients and contains 0 missing values.</p>
<p>The dataset consists of 7 attributes:</p>
<ol style="list-style-type: decimal">
<li>mcv - mean corpuscular volume</li>
<li>alkphos - alkaline phosphatase</li>
<li>sgpt - alanine aminotransferase</li>
<li>sgot - aspartate aminotransferase</li>
<li>gammagt - gamma-glutamyl transpeptidase</li>
<li>drinks - number of half-pint equivalents of alcoholic beverages drunk per day</li>
<li>selector - field created by the BUPA researchers to split the data into train/test sets</li>
</ol>
<p>For further readings on the dataset and misunderstandings related to the selector column incorrectly treated as target refer to: “McDermott &amp; Forsyth 2016, Diagnosing a disorder in a classification benchmark, Pattern Recognition Letters, Volume 73.”</p>
</div>
<div id="methodology-14" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Methodology</h3>
<p>AutoMl Model <span class="math inline">\(M_{aml}\)</span> and the dataset <span class="math inline">\(D\)</span> that consists of <span class="math inline">\(D_{X} = X\)</span> which is set of independent variables and <span class="math inline">\(D_{y} = y\)</span> - dependent variable (ie. target). We assume that <span class="math inline">\(M_{aml}\)</span> is an unknown function <span class="math inline">\(M_{aml}: \mathbb{R}^{p} \to \mathbb{R}\)</span>, where p is a snumber of features in the <span class="math inline">\(D\)</span> Dataset, that satisfies <span class="math inline">\(y = M_{aml}(X) + \epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is an error vector. Automated regression constructs known vector function <span class="math inline">\(G_{AR} : \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times p}\)</span> where <span class="math inline">\(n\)</span> is a number of observations, that satisfies <span class="math inline">\(y = G_{AR}(X)\beta + \epsilon\)</span> thus it is linear regression model fitted for transformated data.</p>
<p>To find <span class="math inline">\(G_{AR}\)</span> we have to put some constraints. First of all we want it to minimize loss function <span class="math inline">\(L: \mathbb{R}^{n} \to \mathbb{R}\)</span> given by following formula <span class="math inline">\(L : \frac{\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}\sum_{i=1}^{n}(y_{i}-\bar{y_{i}})^{2}}{\sum_{i=1}^{n}(\hat{y_{i}}-\bar{y_{i}})^{2}}\)</span> which can be interpreted as Mean Square Error divided by the R-squred coefficient of determination and stands as a tradeoff between fit and results. Another constraint is a domain of valid transformations of particular variables. For given dataset, described in the previous paragraphs we decided to use:</p>
<ul>
<li>Feature selection
<ul>
<li>XAI feature Importance</li>
<li>AIC/BIC</li>
</ul></li>
<li>Continuous transformation
<ul>
<li>Polynomial transformation</li>
<li>Lograthmic transformation</li>
</ul></li>
<li>Discrete transformation
<ul>
<li>SAFE method</li>
</ul></li>
<li>Feature concatenation
<ul>
<li>Multiplication of pair of features.</li>
</ul></li>
</ul>
<p>Obviously, XAI related methods are conducted using AutoML Model. We’ve decided to omit data imputation as an element of valid transformations dataset because liver-disorders dataset does not meet with the problem of missing values.</p>
<p>The optimization process is conducted based on Bayesian Optimization and the backtracing idea. Each main element of the domain of valid transformations is one step in the process of creation <span class="math inline">\(G_{AR}\)</span> function. Within each step, Bayesian optimization will be used to find the best transformation for the given level. During further steps, if any of transformation did not improve model, ie. <span class="math inline">\(L\)</span> function was only growing, the algorithm takes second, the third, etc. solution from previous steps according to backtracking idea. If for no one of <span class="math inline">\(k\)</span> such iterations, where k is known parameter, a better solution is found, step is omitted.</p>
</div>
<div id="results-14" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-13" class="section level3" number="3.4.6">
<h3><span class="header-section-number">3.4.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models---exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric." class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.</h2>
<p><em>Authors: Łukasz Brzozowski, Wojciech Kretowicz, Kacper Siemaszko (Warsaw University of Technology)</em></p>
<div id="abstract-15" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Abstract</h3>
<p>In this article we present and compare a number of interpretable, non-linear feature engineering techniques used to improve linear regression models performance on Cocrete compressive strength dataset. To assert their interpretability, we introduce a new metric for measuring feature importance, which uses derivatives of feature transformations to trace back original features’ impact. As a result, we obtain a thorough comparison of transformation techniques on two black-box models - Random Forest and Support Vector Machine - and three glass-box models - Decision Tree, Elastic Net, and linear regression - with the focus on the linear regression.</p>
</div>
<div id="introduction-and-motivation-13" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Introduction and Motivation</h3>
<p>Linear regression is one of the simplest and easiest to interpret of the predictive models. While it has already been thorougly analysed over the years, there remain some unsolved questions. One such question is how to transform the data features in order to maximize the model’s effectiveness in predicting the new data.</p>
<p>An example of a known and widely used approach is the Box-Cox transformation of the target variable, which allows one to improve the model’s performance with minimal increase in computational complexity <span class="citation">(Sakia <a href="#ref-boxcox" role="doc-biblioref">1992</a>)</span>. However, the choice of the predictive features’ transformations is often left to intuition and trial-and-error approach. In the article, we wish to compare various methods of features’ transformations and compare the resulting models’ performances while also their differences in feature importance.</p>
<p>Many black box regression models use various kinds of feature engineering during the training process. Unfortunately, even though the models perform better than the interpretable ones, they do not provide information about the transformations used and non-linear dependencies between variables and the target. The goal we want to achieve is extracting features and non-linearities with understandable transformations of the training dataset.</p>
<p>To measure the improvement of used methods we will compare their performance metrics with black box models’ as a ground truth. This will allow us to effectively measure which method brought the simple linear model closer to the black box. Moreover, we will take under consideration the improvement of black box model performance. Thanks to this, our article will not only present the methods for creating highly performant interpretable models, but also improvement of the results of black box model.</p>
</div>
<div id="related-work-9" class="section level3" number="3.5.3">
<h3><span class="header-section-number">3.5.3</span> Related Work</h3>
<p>There exist many papers related to feature engineering. We will shortly present two of them.</p>
<p>One of these papers is “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering” Patricia Arroba, José L. Risco-Martín, Marina Zapater, José M. Moya &amp; José L. Ayala <span class="citation">(Patricia Arroba <a href="#ref-ETFFE" role="doc-biblioref">2015</a>)</span>. This paper describes, how feature transformations in linear regression can be chosen based on the genetic algorithms.</p>
<p>Another one is “Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid” Vinícius Veloso de Melo, Wolfgang Banzhaf <span class="citation">(Melo] and Banzhaf <a href="#ref-VELOSODEMELO" role="doc-biblioref">2018</a>)</span>. Similarly to the previous one, this paper tries to automate feature engineering using evolutionar computation to make a hybrid model - final model is simple linear regression while its features are found by more complex algorithm.</p>
</div>
<div id="methodology-15" class="section level3" number="3.5.4">
<h3><span class="header-section-number">3.5.4</span> Methodology</h3>
<p>The main goal of our research is to compare various methods of transforming the data in order to improve the linear regression’s performance. While we do not aim to derive an explicitly best solution to the problem, we wish to compare some known approaches and propose new ones, simultaneously verifying legitimacy of their usage. The second goal of the research is to compare the achieved models’ performances with black box models to generally compare their effectiveness. An important observation about the linear regression model is that once we transform features in order to improve the model’s performance, it strongly affects its interpretability. We therefore propose a new feature importance measure for linear regression and compare it with the usual methodes where possible.</p>
<div id="transformation-methods" class="section level4" number="3.5.4.1">
<h4><span class="header-section-number">3.5.4.1</span> Transformation methods</h4>
<p>The four methods of feature transformation compared in the article include:</p>
<ol style="list-style-type: decimal">
<li><p>By-hand-transformations - through a trial-and-error approach we derive feature transformations that allow the linear regression models to yield better results. We use our expertise and experience with previous datasets to get possibly best transformations which are available though such process. They include taking the features to up to third power, taking logarithm, sinus, cosinus or square root of the features.</p></li>
<li><p>Brute Force method - this method of data transformation generates huge amount of additional features being transformations of the existing features. We allow taking each feature up to the third power, taking sinus and cosinus of each feature and additionaly a product of each pair of features. The resulting dataset has 69 variables including the target (in comparison with 9 variables in the beginning).</p></li>
<li><p>Bayesian Optimization method <span class="citation">(Bernd Bischl <a href="#ref-BO" role="doc-biblioref">2018</a>)</span> - we treat the task of finding optimal data transformation as an optimization problem. We restrict the number of transformations and their type - we create one additional feature for each feature present in the dataset being its <span class="math inline">\(x\)</span>-th power, where <span class="math inline">\(x\)</span> is calculated through the process of Bayesian optimization. It allows us to keep the dimension of the dataset relatively small while improving the model’s performance. We allowed the exponents to be chosen from interval <span class="math inline">\((1, 4]\)</span>.</p></li>
<li><p>One of our ideas is to use Genetic Programming (GP) to find the best feature transformations. Our goal is to find a set of simple feature transformations (not necessarily unary) that will significantly boost the ordinary linear regression model performance. We will create a set of simple operations such as adding, multiplying, taking a second power, taking logarithm and so on. Each transformation is based only on these operations and specified input variables. Each genetic program tries to minimize MSE of predicting the target, thus trying to save as much information as possible in a single output variable constructed on a subset of all the input features. We will use a variation of the genetic algorithms to create an operation tree minimizing our goal. Before we fit the final model, that is the ordinary linear regression, first we select the variables to transform. The resulting linear regression is calculated on the transformed and selected variables at the very end. To summarize, each operation tree calculates one variable, but this variable may not be included at the end. To avoid strong correlation we decided to use similar trick to the one used in random forest models. Each GP is trained only on some subset of all variables. Bootstraping seems also to be an effective idea, however, we did not implement it. In the case of small number of features in the data, such as in our case, one can consider using all possible subsets of variables of fixed size. Otherwise, they can be chosen on random. This process results in a number of output variables, so we choose the final transformed features using LASSO regression or BIC. This method should find much better solutions without extending dimensionality too much nor generating too complex transformations. The general idea is to automate feature enginnering done traditionally by hand. Another advantage is control of the model’s complexity. We can stimulate how the operation trees are made, e.g. how the operations set looks like. There is also a possibilty of reducing or increasing complexity at will. Modification of this idea is to add a regularization term decreasing survival probability with increasing complexity. At the end, the model could also make a feature selection in the same way - then one of possible operations in the set would be dropping.</p></li>
</ol>
</div>
<div id="feature-importance-metric" class="section level4" number="3.5.4.2">
<h4><span class="header-section-number">3.5.4.2</span> Feature importance metric</h4>
<p>The most natural feature importance metric for linear models such as linear regression and GLM are the absolute values of coefficients. Let <span class="math inline">\(x_1, \dots, x_n\)</span> denote the features (column vectors) and <span class="math inline">\(\hat{y}\)</span> denote the prediction. In linear models we have:
<span class="math display">\[c + \sum_{i=1}^n a_i \cdot x_i = \hat{y}\]</span>
where <span class="math inline">\(c\)</span> is a constant (intercept) and <span class="math inline">\(a_i\)</span> are the coefficients of regression. Formally, the Feature Importance mesaure value of the <span class="math inline">\(i-\)</span>th feature (<span class="math inline">\(FI_i\)</span>) measure is given as:
<span class="math display">\[FI_i = |a_i|\]</span>
We may also notice that:
<span class="math display">\[FI_i = \Big|\frac{\partial \hat{y_i}}{\partial x_i}\Big|\]</span>
We wish to generalize the above equation. Transforming the data in the dataset to improve the model’s performance may be expressed as generating a new dataset, where each column is a function of all the features present in the original dataset. If the newdataset has <span class="math inline">\(m\)</span> features and the new prediction vector is given as <span class="math inline">\(\dot{y}\)</span>, we then have:
<span class="math display">\[d + \sum_{i=1}^m b_i \cdot f_i (x_1, \dots, x_n) = \dot{y}\]</span>
where <span class="math inline">\(d\)</span> is the new intercept constant and <span class="math inline">\(b_i\)</span> are the coefficients of regression. We therefore may use the formula above to derive a new Feature Importance Measure (Derivative Feature Importance, <span class="math inline">\(DFI\)</span>) as:
<span class="math display">\[DFI_{i, j} = \Big|\frac{\partial \dot{y_i}}{\partial x_{i, j}}\Big| = \Big|b_i \cdot \sum_{k=1}^m \frac{\partial f_k}{\partial x_{i, j}}\Big|\]</span>
The above measure is calculated separately for each observation <span class="math inline">\(j\)</span> in the dataset. However, due to the additive properties of derivatives, we may calculated Derivative Feature Importance of the <span class="math inline">\(i\)</span>-th feature as:
<span class="math display">\[DFI_i = \frac{1}{n}\sum_{j=1}^n DFI_{i, j}\]</span>
which is then a global measure of Feature Importance of the <span class="math inline">\(i\)</span>-th feature.</p>
</div>
<div id="dataset-and-model-performance-evaluation" class="section level4" number="3.5.4.3">
<h4><span class="header-section-number">3.5.4.3</span> Dataset and model performance evaluation</h4>
<p>The research is conducted on <em>Concrete_Data</em> dataset from the OpenML database [<a href="https://www.openml.org/d/4353">link</a>]. The data describes the amount of ingredients in the samples - cement, blast furnace slag, fly ash, water, coarse aggregate and fine aggregate - in kilograms per cubic meter; it also contains the drying time of the samples in days, referred to as age. The target variable of the dataset is the compressive strength of each sample in megapascals (MPa), therefore rendering the task to be regressive. The dataset contains 1030 instances with no missing values. There are also no symbolic features, as we aim to investigate continuous transformations of the data. Due to the fact, that we focus on the linear regression model, the data is reduced prior to training. We remove the outliers and influential observarions based on Cook’s distances and standardized residuals.</p>
<p>We use standard and verified methods to compare results of the models. As the target variable is continuous, we may calculate Mean Square Error (MSE), Mean Absolute Error (MAE), and R-squared measures for each model, which provide us with proper and measurable way to compare the models’ performances. The same measures may be applied to black box models. The most natural measure of feature importance for linear regression are the coefficients’ absolute values after training the model - however, such easily interpretable measures are not available for black-box models. We therefore measure their feature importance with permutational feature importance measure and caluclating drop-out loss, easily applicable to any predictive model and therefore not constraining us to choose from a restricted set. In order to provide unbiased results, we calculate the measures’ values during cross-validation process for each model, using various number of fold to present comparative results.</p>
</div>
</div>
<div id="results-15" class="section level3" number="3.5.5">
<h3><span class="header-section-number">3.5.5</span> Results</h3>
<p>Note, that the transformations used in results section are only examples of the presented methods and various realisations may result in various final measures’ values. Considerably, the trial-and-error approach may is quite individual and the results may differ depending on the experimentator. This non-automatic method should be treated as something that can be achieved after long time of tries.</p>
<div id="models-performance" class="section level4" number="3.5.5.1">
<h4><span class="header-section-number">3.5.5.1</span> Models’ performance</h4>
<center>
<div class="figure"><span id="fig:mae3-5"></span>
<img src="images/3-5-1.png" alt="MAE of the models after transformations" width="70%" />
<p class="caption">
FIGURE 3.1: MAE of the models after transformations
</p>
</div>
<div class="figure"><span id="fig:mse3-5"></span>
<img src="images/3-5-2.png" alt="MSE of the models after transformations" width="70%" />
<p class="caption">
FIGURE 3.2: MSE of the models after transformations
</p>
</div>
</center>
<p>The plots presented above show the values of MAE and MSE achieved by each model on the datasets after the mentioned transformations. We may observe that the linear models have significantly reduced their error values after the transformations, while the brute force method yielded best results. However, brute force method generates much more features increasing the resulting dimension of the dataset, thus increasing the time complexity and reducing the interpretability.</p>
<p>The three remaining methods, that is: the Bayesian optimization, the trial-and-error method and the genetic modifications - provided much improvement in comparison with the models’ performance on the plain dataset as well. In this case the final space has much lower dimension. Bayesian optimization results in 16 features, the trial-and-error in 14 features and the genetic modifications result in 8 features, in comparison to 69 features after brute force transformations. All of these methods have very similar quality of predictions, considering both MAE and MSE.</p>
<p>The remaining non-linear white-box model, namely the Decision Tree Regressor, seems to be rather unaffected by any transformations of the dataset. In comparison, both black the boxes: Random Forest and SVM with gaussian kernel, are strongly influenced, though is hard to say when black box’s prediction quality increases and when decreases.</p>
</div>
<div id="feature-importance-comparison" class="section level4" number="3.5.5.2">
<h4><span class="header-section-number">3.5.5.2</span> Feature Importance comparison</h4>
<p>The comparison of Feature Imprortance values will be between permutational Feature Importance calculated on the black-box models - Random Forest and SVM and local and global Derivative Feature Importance calculated on the datasets after brute force transformations and after the Bayesian optimization.</p>
<div class="figure"><span id="fig:fi3-5"></span>
<img src="images/3-5-3.png" alt="Feature Importance of RFR and SVM" width="49%" height="49%" /><img src="images/3-5-4.png" alt="Feature Importance of RFR and SVM" width="49%" height="49%" />
<p class="caption">
FIGURE 3.3: Feature Importance of RFR and SVM
</p>
</div>
<div class="figure"><span id="fig:fi3-5bf"></span>
<img src="images/3-5-bfl.png" alt="Local and global DFI on the dataset after brute force transformations" width="49%" height="49%" /><img src="images/3-5-bfg.png" alt="Local and global DFI on the dataset after brute force transformations" width="49%" height="49%" />
<p class="caption">
FIGURE 3.4: Local and global DFI on the dataset after brute force transformations
</p>
</div>
<div class="figure"><span id="fig:fi3-5by"></span>
<img src="images/3-5-byl.png" alt="Local and global DFI on the dataset after Bayesian transformations" width="49%" height="49%" /><img src="images/3-5-byg.png" alt="Local and global DFI on the dataset after Bayesian transformations" width="49%" height="49%" />
<p class="caption">
FIGURE 3.5: Local and global DFI on the dataset after Bayesian transformations
</p>
</div>
<p>The figure 3.4 present permutational Feature Importance of the black-box models, and the figures 3.5 and 3.6 present <span class="math inline">\(DFI\)</span> values - for one observation and calculated globally, respectively. The local <span class="math inline">\(DFI\)</span> were calculated on the observations nr <span class="math inline">\(573\)</span> and <span class="math inline">\(458\)</span>. We may notice high resemblance of the black-box Feature Importance to the local <span class="math inline">\(DFI\)</span> after Brute Force trtansformations, as well as similarities between all the Feature Importance calculations overall. The order of features in <span class="math inline">\(DFI\)</span> is not precisely equal to the order of feature after permutational Feature Importance of the black-boxes; however, the order still makes sense as far as we can say based on our little knowledge of materials. Moreover, the deviations of Feature Importance measures between various models is quite common in such research.</p>
<p>To summarize, although the deviations between black-box and glass-box models’ Feature Importance are present, we conclude that <span class="math inline">\(DFI\)</span> may provide a new way of calculating Feature Importance for linear models. Its efficiency shall be further investigated in another research. When it comes the presented comparison, the majority of important variables were detected by the <span class="math inline">\(DFI\)</span> method.</p>
</div>
</div>
<div id="summary-and-conclusions-14" class="section level3" number="3.5.6">
<h3><span class="header-section-number">3.5.6</span> Summary and conclusions</h3>
<p>Each of the four methods leads to significant improvement in Linear Regression. All of them are based on completely different ideas and their effectivenes may vary for different tasks. It needs noting that the presented research is conducted on a dataset containing only numerical variables, so similar research for transformations of categorical variables remains to be yet conducted. However, we have presented numerous ways to transform the dataset improving the linear models while at least partially maintaining its interpretability.</p>
<p>The black box models in this case were unsurpassed, though we achieved highly comparable results. The greatest advatange of white box usage is that every person can understand their mechanics and predictions. Therefore, the presented methods may serve as an efficient solution, when one needs to retain simplicity while also offering an improvement to predictions.</p>
<p>Our new metric - <span class="math inline">\(DFI\)</span> - can be used to compare simple (differentiable) feature transformations in linear regression. The analytical deduction suggests, that its performance is accurate, but it shall be investigated further before being applied in general.</p>
<p>In the end, the obtained results are satisfying and should encourage to put more effort into research about new transformation methods and interpretability metrics. The next thing to do is putting more effort into researching the new <span class="math inline">\(DFI\)</span> metric, to improve interpretability of the extended regression models. We hope that our article will inspire a number of interested readers to conduct such research.</p>

</div>
</div>
<div id="surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering</h2>
<p><em>Authors: Witold Merkel, Adam Rydelek, Michał Stawikowski (Warsaw University of Technology)</em></p>
<div id="abstract-16" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Abstract</h3>
<p>Explainability is the most talked about topic of modern predictive models. The article touches on the topic of such models and their benefits. The main focus is to prove that even on complicated data, explainable models can achieve comparable performance to the best black-box models. Not only are there described strategies allowing better results but also greater explainability. The dataset used in experiments is the adult dataset from OpenML which is from Census database. During the experiments there are multiple processing techniques used, SAFE and different imputation methods among others. Every tool used is explained and the results gained from each part are shown and explained. Thanks to the fact that adult dataset is vastly unbalanced there is a perfect opportunity to present techniques which can be used to handle such tasks. All those methods combined allow for a presentation of a clear workflow enhancing explainable models performance with emphasis on decision tree models. The best results we achieved with decision tree model using methods mentioned above. However at first the best score was achieved by logistic regression, which from the start beat the black boxes. On the other hand it was not possible to tune it, to get it any better. For this reason, our final model is a decision tree, that despite starting as one of the worst surpasses all of the other boxes white and black. This shows that everything can be accomplished with adequate feature engineering, while keeping them explainable.</p>
</div>
<div id="introduction-and-motivation-14" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Introduction and Motivation</h3>
<p>Recently, an increase in demand of interpretable models can be seen. Machine learning models have gained in popularity in recent years among many fields of business science, industry and also more and more often in medicine. Interpretability is a quickly growing part of machine learning, and there have been many works examining all of the aspects of interpretations. <span class="citation">(Murdoch <a href="#ref-IMLDEA" role="doc-biblioref">2018</a>)</span> The problem, however, turned out to be blackbox models, which did not provide sufficient information about the motivation in making specific decisions by the models. Some of machine learning models are considered as black boxes. This means that we can get accurate predictions from them, but we give up the ability of clearly explaining or identifying the logic behind these decisions. <span class="citation">(Pandey <a href="#ref-IML" role="doc-biblioref">2019</a>)</span> Interpretability of models is a desirable feature among specialists in fields other than machine learning, it helps them make better decisions, justify their choices, and combine expert knowledge with the model’s indications.</p>
<p>Humans and computers work differently in how they sense, understand and learn. Machines deals with large volume of data and finding hidden patterns in it and people are better at seeing the bigger picture and finding high-level patterns. <span class="citation">(accenture <a href="#ref-UME" role="doc-biblioref">2018</a>)</span> Trust and transparency are also demanded. There are many methods that can help us create an interpretable model. One of the ways to achieve interpretability is to use only a certain subset of algorithms that create interpretable models. Some of the algorithms considered to be interpretable are: linear regression, logistic regression, decision trees or K Nearest Neighbours (KNN). <span class="citation">(Molnar <a href="#ref-christophmonlar" role="doc-biblioref">2019</a>)</span> Another way may be to use blackboxes to create an interpretable model. They can help us during transformation of the original data set or, for example, in selecting variables.</p>
<p>In this article, we will discuss the process of creating an interpretable model whose target effectiveness will be comparable to blackbox models. We will present the whole workflow, during which we will get acquainted with the dataset with which we will work, we will use advanced feature engineering methods and compare the results obtained during all phases of process. An additional problem we will face during work will be unbalanced data and creating a model that will take them into account during prediction. We will use machine learning tools and frameworks available in R and Python.</p>
</div>
<div id="data-2" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Data</h3>
<p>The dataset used is the adult dataset from OpenML. The original data comes from UCI and was extracted by Barry Becker from the 1994 Census database. The task is to predict whether a given adult makes more than $50,000 a year based attributes such as:</p>
<ul>
<li><p>age,</p></li>
<li><p>race,</p></li>
<li><p>sex,</p></li>
<li><p>education,</p></li>
<li><p>native country,</p></li>
<li><p>work class,</p></li>
<li><p>weekly work hours,</p></li>
<li><p>capital gain,</p></li>
<li><p>capital loss,</p></li>
<li><p>proximation for the demographic background of people,</p></li>
<li><p>relationship,</p></li>
<li><p>marital status,</p></li>
<li><p>occupation.</p></li>
</ul>
<p>In the above mentioned dataset we can observe a problem with target class distribution which is vastly unbalanced. The ratio of positive and negative values is around one to four. The dataset has overall of more than forty eight thousand observations and fifteen features, some of which are scarce.</p>
</div>
<div id="related-work-10" class="section level3" number="3.6.4">
<h3><span class="header-section-number">3.6.4</span> Related work</h3>
<p>Many works concerning Explainable Artificial Intelligence have arose during the last few years as the topic got more and more popular. [Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI] <span class="citation">(Alejandro Barredo Arrieta <a href="#ref-EAICTOC" role="doc-biblioref">2019</a>)</span> is a paper about XAI in general and many challenges concerning the topic. The article addresses all kinds of easily explainable models which set our focus on enhancing kNN and decision tree based models. [SAFE ML: Surrogate Assisted Feature Extraction For Model Learing] <span class="citation">(Gosiewska et al. <a href="#ref-gosiewska2019safe" role="doc-biblioref">2019</a>)</span> on the hand focuses on using Black Box models as surrogate models for improving explainable models.</p>
</div>
<div id="methodology-16" class="section level3" number="3.6.5">
<h3><span class="header-section-number">3.6.5</span> Methodology</h3>
<p>As mentioned before we are going to work on an unbalanced dataset. In order to handle this issue and achieve the best possible results during our future work we are going to use two measures: AUPRC and AUC. The former one is designed to take the lack of balance into account. The dataset will be divided into two partitions using stratification in order to handle scarce factor levels. The training part of the dataset is going to be used to compare the effects of many processes used to enhance the results. We are going to use five fold cross-validation. The final results are going to be presented using the test dataset.</p>
<p>Our workflow can be divided into the following steps:</p>
<div id="eda" class="section level4" number="3.6.5.1">
<h4><span class="header-section-number">3.6.5.1</span> EDA</h4>
<p>In this part we have gotten accustomed with the dataset. We started with feature distribution and definition analysis and studied the dependency of other variables on the target class. The column correlation was also taken into account.</p>
<div id="distribution-of-numeric-variables" class="section level5" number="3.6.5.1.1">
<h5><span class="header-section-number">3.6.5.1.1</span> Distribution of numeric variables</h5>
<div class="figure">
<img src="images/3-6-1.png" alt="" />
<p class="caption">Data distribution</p>
</div>
<p>On this plot we can see the distribution of numeric variables. Some of them are normally distributed but there are also highly skewed variables such as capital-gain and capital-loss.</p>
</div>
<div id="correlation-plot-of-numeric-variables" class="section level5" number="3.6.5.1.2">
<h5><span class="header-section-number">3.6.5.1.2</span> Correlation plot of numeric variables</h5>
<div class="figure">
<img src="images/3-6-2.png" alt="" />
<p class="caption">Correlation</p>
</div>
<p>On this plot we can observe that the numeric variables have weak correlations with each other.</p>
</div>
<div id="density-of-observations-by-age-and-education" class="section level5" number="3.6.5.1.3">
<h5><span class="header-section-number">3.6.5.1.3</span> Density of observations by age and education</h5>
<div class="figure">
<img src="images/3-6-3.png" alt="" />
<p class="caption">Density</p>
</div>
<p>We can see the density of observations divided by age and education-num features. Education-num is a numeric variable that depicts the education level in an ordered way.</p>
</div>
<div id="level-of-education-by-target-class" class="section level5" number="3.6.5.1.4">
<h5><span class="header-section-number">3.6.5.1.4</span> Level of education by target class</h5>
<div class="figure">
<img src="images/3-6-4.png" alt="" />
<p class="caption">Education</p>
</div>
<p>We can observe that the highest rate of positive target class is for PHD and professors which does not surprise.</p>
</div>
<div id="work-class-by-target-class" class="section level5" number="3.6.5.1.5">
<h5><span class="header-section-number">3.6.5.1.5</span> Work class by target class</h5>
<div class="figure">
<img src="images/3-6-5.png" alt="" />
<p class="caption">Work class</p>
</div>
<p>The highest rate when divided by work class can be observed for self-employed people.</p>
</div>
<div id="age-by-target-class" class="section level5" number="3.6.5.1.6">
<h5><span class="header-section-number">3.6.5.1.6</span> Age by target class</h5>
<div class="figure">
<img src="images/3-6-6.png" alt="" />
<p class="caption">Age</p>
</div>
<p>The density of people earning more than $50000 is skewed toward the elderly.</p>
</div>
</div>
<div id="initial-data-preparation-1" class="section level4" number="3.6.5.2">
<h4><span class="header-section-number">3.6.5.2</span> Initial Data Preparation</h4>
<p>The main focus of this part is to analyze and input the missing data. The side tasks are handling outliers and transformation of skewed variables using logistic functions. A few most popular imputation methods will be compared. We are not only going to use basic aggregation functions like mean and mode but also other machine learning models and advance imputation methods like <span class="citation">(Stef van Buuren <a href="#ref-MICE" role="doc-biblioref">2011</a>)</span>. The results are going to be compared using a model that is robust to missing data which is a basic decision tree.</p>
<div id="missing-data-by-feature" class="section level5" number="3.6.5.2.1">
<h5><span class="header-section-number">3.6.5.2.1</span> Missing Data by Feature</h5>
<div class="figure">
<img src="images/3-6-7.png" alt="" />
<p class="caption">Missing Data</p>
</div>
<p>Missing data can be observed in three columns. They account up to eight percent of data.</p>
</div>
<div id="missing-data-pattern" class="section level5" number="3.6.5.2.2">
<h5><span class="header-section-number">3.6.5.2.2</span> Missing Data Pattern</h5>
<div class="figure">
<img src="images/3-6-8.png" alt="" />
<p class="caption">Missing Data Pattern</p>
</div>
<p>The bottom figure shows relation between missing variables, when a certain variables is absent in correspondence to other.</p>
</div>
<div id="imputation-results" class="section level5" number="3.6.5.2.3">
<h5><span class="header-section-number">3.6.5.2.3</span> Imputation Results</h5>
<table>
<thead>
<tr class="header">
<th>Imputation type</th>
<th>AUPRC</th>
<th>AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>none</td>
<td>0.6457237</td>
<td>0.8357408</td>
</tr>
<tr class="even">
<td>Mean and mode basic imputation</td>
<td>0.6472785</td>
<td>0.8362379</td>
</tr>
<tr class="odd">
<td>KNN imputation</td>
<td>0.6547221</td>
<td>0.838437</td>
</tr>
<tr class="even">
<td>MICE imputation</td>
<td>0.6452505</td>
<td>0.834577</td>
</tr>
<tr class="odd">
<td>Missing data removal</td>
<td>0.6515376</td>
<td>0.8355305</td>
</tr>
</tbody>
</table>
<p>We can observe that the best results were achieved using KNN imputation so from now on this will be the used method.</p>
</div>
<div id="outliers" class="section level5" number="3.6.5.2.4">
<h5><span class="header-section-number">3.6.5.2.4</span> Outliers</h5>
<div class="figure">
<img src="images/3-6-9.png" alt="" />
<p class="caption">Missing Data</p>
</div>
<p>Some outliers can be observed in fnlwgt columns which is described as proximation for the demographic background of people. The data was cropped to 300000 value.</p>
</div>
<div id="skewness" class="section level5" number="3.6.5.2.5">
<h5><span class="header-section-number">3.6.5.2.5</span> Skewness</h5>
<div class="figure">
<img src="images/3-6-10.png" alt="" />
<p class="caption">Skewness of fnlwgt</p>
</div>
<p>The previously mentioned feature was also skewed. The solution for this problem was a basic logistic transformation.</p>
<div class="figure">
<img src="images/3-6-11.png" alt="" />
<p class="caption">Transformation of fnlwgt</p>
</div>
<p>The transformation was successful and the new feature has an appropriate distribution.</p>
</div>
</div>
<div id="feature-engineering-and-tuning" class="section level4" number="3.6.5.3">
<h4><span class="header-section-number">3.6.5.3</span> Feature Engineering and Tuning</h4>
<p>Firstly we are going to compare a few most popular Machine Learning models on our initially prepared dataset. We picked three popular explainable models: <strong>KNN</strong>, <strong>Decision Tree</strong> and <strong>Logistic Regression</strong>.</p>
<div id="first-comparison-of-models" class="section level5" number="3.6.5.3.1">
<h5><span class="header-section-number">3.6.5.3.1</span> First comparison of models</h5>
<div class="figure">
<img src="images/3-6-12.png" alt="" />
<p class="caption">First results</p>
</div>
<p>The best result was achieved by logistic regression which was surprising and shows how capable explainable models can be even on complex data, non of later modifications had any impact on the model so we decided to exclude it from further considerations. The next best explainable model ranked by AUPRC was <strong>KNN</strong> and <strong>Decision Tree</strong>. Due to the fact that logistic regression obtained a better result than black boxes at the very start, and the <strong>KNN</strong> model is not globally interpretable, we will focus on the decision tree, which is a popular and easily interpretable model. That is the reason why we are going to work mainly with this model and initially with KNN for comparison in our goal to achieve similar results to Black Box models using it, but our final model will be a decision tree. The best Black boxes were <strong>Random Forest</strong> and <strong>Adaboost</strong>, because of that in the later phase of our project we will depend on them to make our results better. During the Feature Engineering we will utilize strategies such as transforming and extracting features using the SAFE algorithm mentioned in the article above. Amongst other strategies we will use variable selection based on random forest feature importance and tuning models with Bayesian optimization - mlrMBO <span class="citation">(Bernd Bischl <a href="#ref-BO" role="doc-biblioref">2018</a>)</span> based on custom measure: AUPRC and not the typical AUC. Another interesting aspect that we are going to look into is changing the type of target to numeric and by doing so changing our task from classification to regression, surprisingly it improves our results.</p>
</div>
<div id="rsafe" class="section level5" number="3.6.5.3.2">
<h5><span class="header-section-number">3.6.5.3.2</span> rSAFE</h5>
<blockquote>
<p>The SAFE ML algorithm uses a complex model as a surrogate. New binary
features are created on the basis of surrogate predictions. These new
features are used to train a simple refined model. (…) method
that uses elastic black-boxes as surrogate models to create a simpler,
less opaque, yet still accurate and interpretable glass-box models.
New models are created on newly engineered features extracted/learned
with the help of a surrogate model.</p>
</blockquote>
<p><span class="citation">(Gosiewska et al. <a href="#ref-gosiewska2019safe" role="doc-biblioref">2019</a>)</span></p>
<p>To extract new variables we will use rSAFE, which will depend on two different Black Box models: Random Forest and Adaboost. Those models were tuned to increase the final result. Based on the initial variables and the new ones created by the rSafe algorithm, we will choose the combination that will be the best for this problem. We will achieve this by analyzing various values ​​of the “regularization penalty” parameter in the data extraction algorithm in the rSafe package and choosing the one that gives us the best result.</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>From AUPRC</th>
<th>To AUPRC</th>
<th>From AUC</th>
<th>To AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>decision tree</td>
<td>0.655</td>
<td>0.674</td>
<td>0.842</td>
<td>0.851</td>
</tr>
<tr class="even">
<td>knn</td>
<td>0.698</td>
<td>0.706</td>
<td>0.859</td>
<td>0.868</td>
</tr>
</tbody>
</table>
<p>From the above table it can be observed that this method gave us a significant improvement in terms of AUC and AUPRC. New variables that were extracted and chosen with penalty regularization are:</p>
<ul>
<li>“age_new” - new levels: (-Inf, 32] and (32, Inf),</li>
<li>“fnlwgt_new” - new levels: (-Inf, 11.70863] and (11.70863, Inf),</li>
<li>"hours.per.week_new - new levels: (-Inf, 40] and (40, Inf),</li>
<li>“race_new” - new levels: White_Asian-Pac-Islander and Amer-Indian-Eskimo_Other_Black.</li>
</ul>
</div>
<div id="change-to-regression" class="section level5" number="3.6.5.3.3">
<h5><span class="header-section-number">3.6.5.3.3</span> Change to regression</h5>
<p>The next approach we chose based on our previous experience with unbalanced datasets is changing our task from classification to regression and by doing so we improved our prediction. Non of later modifications had any impact on the KNN model so we decided to exclude it from further considerations.</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>From AUPRC</th>
<th>To AUPRC</th>
<th>From AUC</th>
<th>To AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>decision tree</td>
<td>0.674</td>
<td>0.716</td>
<td>0.851</td>
<td>0.865</td>
</tr>
</tbody>
</table>
<p>In case of KNN there was no visible improvement.</p>
</div>
<div id="variable-selection" class="section level5" number="3.6.5.3.4">
<h5><span class="header-section-number">3.6.5.3.4</span> Variable selection</h5>
<p>Additionally we carried out a variable selection on those that are currently used. It was based on feature importance from a random forest model, we use ten most important variables from fifteen that were being used before.</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>From AUPRC</th>
<th>To AUPRC</th>
<th>From AUC</th>
<th>To AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>decision tree</td>
<td>0.716</td>
<td>0.720</td>
<td>0.865</td>
<td>0.871</td>
</tr>
</tbody>
</table>
<p>The improvement is slight, but because of this operation we are using a less complicated model, so we decided to use it.</p>
</div>
<div id="tuning" class="section level5" number="3.6.5.3.5">
<h5><span class="header-section-number">3.6.5.3.5</span> Tuning</h5>
<p>Last phase was tuning our decision tree model. We based it on mlrMBO, but our method is adjusted to the task. The optimized measure is AUPRC, this proces is slightly different from optimizing AUC. Before using MBO we select the optimal range of parameters like: “minsplit” - the minimum number of observations that must exist in a node in order for a split to be attempted. and “minbucket” - the minimum number of observations in any terminal node.. After selecting ten optimal values of the first one we select ten optimal for the second. For each of those hundred pairs we use MBO to find the optimal “cp” - complexity parameter. Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. The result we achieved:</p>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>From AUPRC</th>
<th>To AUPRC</th>
<th>From AUC</th>
<th>To AUC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>decision tree</td>
<td>0.720</td>
<td>0.794</td>
<td>0.871</td>
<td>0.912</td>
</tr>
</tbody>
</table>
<p>Results achieved in this phase are really high, we managed to top those from black boxes and the logistic regression one.</p>
</div>
</div>
<div id="results-and-conclusion" class="section level4" number="3.6.5.4">
<h4><span class="header-section-number">3.6.5.4</span> Results and conclusion</h4>
<div class="figure">
<img src="images/3-6-13.png" alt="" />
<p class="caption">Differences between each stage</p>
</div>
<p>From the above chart, we can see that thanks to our actions the performance of a fully interpretable model surpassed all basic black boxes and logistic regression, which from the start had really high results. It proves that it is possible to achieve comparable or even better results with interpretable models than black boxes even on unbalanced datasets, such as “adult”. It requires some work, but there are a lot of methods that make it possible to improve our performance. Some of them may be based on very well-functioning black box models. Probably in comparison with tuned black box model, we would achieve lesser results, but interpretable models have an advantage, people can understand why some choices were made and can be safely use when transparency and ease of understanding are needed. Using models such as logistic regression or decision trees make it possible. In this paper we showed that models built using these algorithms after some modifications are able to get similar results to such powerful models as random forest or adaboost, and even artificial deep neural networks.</p>

</div>
</div>
</div>
<div id="which-neighbours-affected-house-prices-in-the-90s" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Which Neighbours Affected House Prices in the ’90s?</h2>
<p><em>Authors: Hubert Baniecki, Mateusz Polakowski (Warsaw University of Technology)</em></p>
<div id="introduction-2" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Introduction</h3>
<p>Real estate value varies over numerous factors. These may be obvious like location or interior design, but also less apparent like the ethnicity and age of neighbours. Therefore, property price estimation is a demanding job that often requires a lot of experience and market knowledge. Is or was, because nowadays, Artificial Intelligence (AI) surpasses humans in this task. Interested parties more often use tools like supervised Machine Learning (ML) models to precisely evaluate the property value and gain a competitive advantage.</p>
<p>The dilemma is in blindly trusting the prediction given by so-called black-box models. These are ML algorithms that take loads of various real estate data as input and return a house price estimation without giving their reasoning. Black-box complex nature is its biggest strength and weakness at the same time. This trait regularly entails high effectiveness but does not allow for interpretation of model outputs. Because of that, specialists interested in supporting their work with automated ML decision-making are more eager to use white-box models like linear regression or decision trees. These do not achieve state-of-the-art performance efficiently, but instead, provide valuable information about the relationships present in data through model interpretation.</p>
<p>For many years houses have been the most popular properties; thus, they are of particular interest for ordinary people. What exact influence had the demographic characteristics of the house neighbourhood on its price in the ’90s? Although in the absence of current technology, it has been hard to answer such question years ago, now we can.</p>
<p>In this paper, we perform a case study on the actual US. Census data from 1990 and deliver an interpretable white-box model that estimates the median house price by the region. We present multiple approaches to this problem and choose the best model, which achieves similar performance to complex black-boxes. Finally, using its interpretable nature, we answer various questions that give a new life to this historical data.</p>
<p><strong>TODO</strong> add citations</p>
</div>
<div id="related-work-11" class="section level3" number="3.7.2">
<h3><span class="header-section-number">3.7.2</span> Related Work</h3>
<p><strong>TODO</strong></p>
<p><a href="https://arxiv.org/abs/1901.01774" class="uri">https://arxiv.org/abs/1901.01774</a></p>
<p><a href="https://arxiv.org/abs/1808.02547" class="uri">https://arxiv.org/abs/1808.02547</a></p>
<p><a href="https://arxiv.org/abs/1909.00704" class="uri">https://arxiv.org/abs/1909.00704</a></p>
</div>
<div id="data-3" class="section level3" number="3.7.3">
<h3><span class="header-section-number">3.7.3</span> Data</h3>
<p>For this case study we use the <em>house_8L</em> dataset crafted from the data collected in 1990 by the US. Census Bureau.
Each record stands for a distinct US. state while the target value is a median house price in a given region.
The variables are presented in Table <a href="interpretability.html#tab:3-7-dataset">3.1</a>.</p>
<table class="table table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:3-7-dataset">TABLE 3.1: </span>Description of variables present in the house_8L dataset.
</caption>
<thead>
<tr>
<th style="text-align:center;">
Original name
</th>
<th style="text-align:center;">
New name
</th>
<th style="text-align:center;">
Description
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 10em; ">
price
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
price
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
median price of the house in the region
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
P3
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
house_n
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
total number of households
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
H15.1
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
avg_room_n
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
average number of rooms in an owner-occupied Housing Units
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
H5.2
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
forsale_h_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of vacant Housing Units which are for sale only
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
H40.4
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
forsale_6mplus_h_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of vacant-for-sale Housing Units vacant more then 6 months
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
P11.3
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
age_25_64_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of people between 25-64 years of age
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
P16.2
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
family_2plus_h_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of households with 2 or more persons which are family households
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
P19.2
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
black_h_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of households with black Householder
</td>
</tr>
<tr>
<td style="text-align:center;width: 10em; ">
P6.4
</td>
<td style="text-align:center;width: 10em; font-weight: bold;">
asian_p_pct
</td>
<td style="text-align:center;width: 40em; background-color: white !important;">
percentage of people which are of Asian or Pacific Islander race
</td>
</tr>
</tbody>
</table>
<p>Furthermore, we will apply our Metodology (Section 4) on a corresponding <em>house_16H</em> dataset, which has the same target but a different set of variables.
More correlated variables of a higher variance make it significantly harder to estimate the median house price in a given region.
Such validation will allow us to evaluate our model on a more demanding task.
The comprehensive description of used data can be found in <span class="citation">(“Census dataset” <a href="#ref-housepricesdata" role="doc-biblioref">1996</a>)</span>.</p>
</div>
<div id="methodology-17" class="section level3" number="3.7.4">
<h3><span class="header-section-number">3.7.4</span> Methodology</h3>
<p>In this section, we are going to focus on developing the best white-box model, which provides interpretability of features. Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we focus on the residuals while the mean of absolute values of residuals is the easiest to interpret.</p>
<div id="eda-1" class="section level4" number="3.7.4.1">
<h4><span class="header-section-number">3.7.4.1</span> EDA</h4>
<p>The main conclusions from the Exploratory Data Analysis are as follows:</p>
<ol style="list-style-type: decimal">
<li>The target value is very skewed (See Figure <a href="interpretability.html#fig:3-7-eda">3.6</a>).</li>
<li>There are 6 percentage and 2 count variables.</li>
<li>The dataset has over 22k data points.</li>
<li>There are 46 data points with unnaturally looking target value.</li>
<li>There are no missing values.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:3-7-eda"></span>
<img src="images/3-7-eda.png" alt="(L) Histogram of the target values. (R) Examplary variable correlation with the target." width="800" />
<p class="caption">
FIGURE 3.6: (L) Histogram of the target values. (R) Examplary variable correlation with the target.
</p>
</div>
<p>Therefore we decided that:</p>
<ol style="list-style-type: decimal">
<li>We will not transform the skewed target because this might provide less interpretability.</li>
<li>There are not many possibilities for feature engineering.</li>
<li>We can reliably split the data into train and test using 2:1 ratio.</li>
<li>We suspect that the target value of <em>500001</em> is artificially made, so we remove these outliers.</li>
</ol>
<p>Throughout this case study, we use the Mean Absolute Error (MAE) measure to evaluate the model performance, because we later focus on the residuals while the mean of absolute values of residuals is the easiest to interpret.</p>
</div>
<div id="safe" class="section level4" number="3.7.4.2">
<h4><span class="header-section-number">3.7.4.2</span> SAFE</h4>
<p>The first approach was using the SAFE <span class="citation">(Gosiewska et al. <a href="#ref-gosiewska2019safe" role="doc-biblioref">2019</a>)</span> technique to engineer new features and produce a linear regression model. We trained a well-performing black-box <code>ranger</code> <span class="citation">(Wright and Ziegler <a href="#ref-ranger" role="doc-biblioref">2017</a>)</span> model and extracted new interpretable features using its Partial Dependence Profiles <span class="citation">(Friedman <a href="#ref-pdp" role="doc-biblioref">2000</a>)</span>. Then we used these features to craft a new linear model which indeed was better than the baseline linear model by about 10%. It is worth noting that both of these linear models had a hard time succeeding because of target skewness.</p>
</div>
<div id="divide-and-conquer" class="section level4" number="3.7.4.3">
<h4><span class="header-section-number">3.7.4.3</span> Divide-and-conquer</h4>
<p>In this section, we present the main contribution of this paper.
The divide-and-conquer idea has many computer science applications, e.g. in sorting algorithms, natural language processing, or parallel computing.
We decided to make use of its core principles in constructing the method for fitting the enhanced white-box model.
The final result is multiple tree models combined which decisions are easily interpretable.</p>
<p>The proposed algorithm presented in the Figure <a href="interpretability.html#fig:3-7-algorithm">3.7</a> is:</p>
<ol style="list-style-type: decimal">
<li>Divide the target variable with <code>k</code> middle points into <code>k+1</code> groups.</li>
<li>Fit a black-box classifier on train data which predicts the belonging to the <code>i-th</code> group.</li>
<li>Use this classifier to divide the train and test data into <code>k+1</code> train and test subsets.</li>
<li>For every <code>i-th</code> subset fit a white-box estimator of target variable on the <code>i-th</code> train data.</li>
<li>Use the <code>i-th</code> estimator to predict the outcome of the <code>i-th</code> test data.</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:3-7-algorithm"></span>
<img src="images/3-7-algorithm.png" alt="The divide-and-conquer algorithm used to construct an enhanced white-box model." width="800" />
<p class="caption">
FIGURE 3.7: The divide-and-conquer algorithm used to construct an enhanced white-box model.
</p>
</div>
<p><strong>pytanie: czy dodawać wypunktowanie w caption schematu? W paper tak, a w książce lepiej wygląda bez (?)</strong></p>
<p>The final product is a stacked model with one classifier and <code>k+1</code> estimators. For the house price task, we chose <code>k = 1</code>, and the middle point was arbitrary chosen as <code>100k</code>, which divides the data into two groups in about a 10:1 ratio. We used the <code>ranger</code> random forest model as a black-box classifier and the <code>rpart</code> <span class="citation">(Therneau and Atkinson <a href="#ref-rpart" role="doc-biblioref">2019</a>)</span> decision tree model as a white-box estimator. Such a solution allows us to achieve competitive performance with interpretable features.</p>
</div>
</div>
<div id="results-16" class="section level3" number="3.7.5">
<h3><span class="header-section-number">3.7.5</span> Results</h3>
<div id="final-model" class="section level4" number="3.7.5.1">
<h4><span class="header-section-number">3.7.5.1</span> Final model</h4>
<p><strong>TODO</strong> add info about the parameters. describe the plot</p>
<p><img src="images/3-7-tree-cheap.svg" width="800" style="display: block; margin: auto;" /></p>
<p><img src="images/3-7-tree-rich.svg" width="800" style="display: block; margin: auto;" /></p>
</div>
<div id="comparison" class="section level4" number="3.7.5.2">
<h4><span class="header-section-number">3.7.5.2</span> Comparison</h4>
<p>We next compare our stacked model with baseline <code>ranger</code> and <code>rpart</code> models, respectively referred to as black-box and white-box.</p>
<p><strong>TODO</strong> describe the plots</p>
<p><img src="images/3-7-density.png" width="800" style="display: block; margin: auto;" /></p>
<p><img src="images/3-7-boxplot.png" width="800" style="display: block; margin: auto;" /></p>
<table class="table table-bordered" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:3-7-table-results">TABLE 3.2: </span>Comparison of the MAE score for all of the used models on test datasets.
</caption>
<thead>
<tr>
<th style="border-bottom:hidden" colspan="1">
</th>
<th style="border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Dataset (test)
</div>
</th>
</tr>
<tr>
<th style="text-align:center;">
Model
</th>
<th style="text-align:center;">
house_8L
</th>
<th style="text-align:center;">
house_16H
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #8bdcbe !important;">
linear model
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
23.1k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
24.1k
</td>
</tr>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #8bdcbe !important;">
SAFE on ranger
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
21.4k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
22.6k
</td>
</tr>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #8bdcbe !important;">
rpart
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
19.2k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
22.1k
</td>
</tr>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #f05a71 !important;">
xgboost
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #f05a71 !important;">
16k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #f05a71 !important;">
16.5k
</td>
</tr>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #f05a71 !important;">
ranger
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #f05a71 !important;">
14.8k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #f05a71 !important;">
15.6k
</td>
</tr>
<tr>
<td style="text-align:center;width: 20em; font-weight: bold;color: black !important;background-color: #8bdcbe !important;">
stacked model
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
14.6k
</td>
<td style="text-align:center;width: 10em; color: black !important;background-color: #8bdcbe !important;">
15.3k
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="conclusions-1" class="section level3" number="3.7.6">
<h3><span class="header-section-number">3.7.6</span> Conclusions</h3>
<p><strong>TODO</strong></p>

</div>
</div>
<div id="explainable-computer-vision-with-embeddings-and-knn-classifier" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Explainable Computer Vision with embeddings and KNN classifier</h2>
<p><em>Authors: Olaf Werner, Bogdan Jastrzębski (Warsaw University of Technology)</em></p>
<div id="abstract-17" class="section level3" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Abstract</h3>
<hr />
</div>
<div id="introduction-3" class="section level3" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> 3.8.1 Introduction</h3>
<p>Computer vision is widely known use case for neural networks. However neural networks are infamous for their complexity and lack of interpretability. On the other hand simple classifiers like KNN have really poor results for complex tasks like image recognition. In this article we will prove that it is possible to get best of both worlds using emmbeddings.</p>
<hr />
</div>
<div id="data-4" class="section level3" number="3.8.3">
<h3><span class="header-section-number">3.8.3</span> 3.8.2 Data</h3>
<p>We are going to use dataset <a href="https://www.openml.org/d/40996">Fashion-Mnist</a>. Fashion-MNIST is a dataset of Zalando’s article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Classes are following:</p>
<ul>
<li><p>T-shirt/top</p></li>
<li><p>Trouser</p></li>
<li><p>Pullover</p></li>
<li><p>Dress</p></li>
<li><p>Coat</p></li>
<li><p>Sandal</p></li>
<li><p>Shirt</p></li>
<li><p>Sneaker</p></li>
<li><p>Bag</p></li>
<li><p>Ankle boot.</p></li>
</ul>
<hr />
</div>
<div id="methodology-18" class="section level3" number="3.8.4">
<h3><span class="header-section-number">3.8.4</span> 3.8.3 Methodology</h3>
<p>The simplest and one of the most robust classifiers is KNN. It doesn’t generalize information, instead it saves training dataset and during prediction it finds the most similar historical observations and predicts label of a new observation based on their labels. However, it not only doesn’t have the capacity to distinguish important features from not important ones, but also to find more complex interactions between variables.</p>
<p>One way to improve KNN’s performance is to preced closest neighbour computation with transformation of the space of observations, so that the derivative variables are more meaningful. Such beneficial transformation is called embedding. It can be done in various ways.</p>
<p>The question is, whether or not the new classifier is interpretable. We argue, that it is. The main reason is that even if we can’t interpret the embedding part, we can at least provide historical data that our model used to make prediction. Someone could argue, that it can be done with every classifier, just by finding training data that obtains the most similar prediction. However, with our classifier we can say for sure, that prediction were purely made based on the most similar cases in our dataset, which is fundamentally not true about different classifiers.</p>
<p>An embedding can be done made in various ways. In this article we will explore different embedding techniques, including:</p>
<ul>
<li><p>SVD embedding</p></li>
<li><p>Convolutional Autoencoder</p></li>
<li><p>K-means embedding</p></li>
</ul>
<hr />
</div>
<div id="standard-intepretable-models" class="section level3" number="3.8.5">
<h3><span class="header-section-number">3.8.5</span> 3.8.4 Standard Intepretable Models</h3>
<p>In this section we will explore use of standard interpretable models and we will try to answer the question, why they are not useful when it comes to computer vision.</p>
<hr />
<div id="logistic-regression" class="section level4" number="3.8.5.1">
<h4><span class="header-section-number">3.8.5.1</span> 3.8.4.1 Logistic Regression</h4>
<p>Logistic regression is basic classification model. We get probablity of belonging to given class by:</p>
<p><span class="math display">\[{\displaystyle p={\frac {e^{\beta _{0}+\beta _{1}x_{1}+...+\beta _{n}x_{n}}}{e^{\beta _{0}+\beta _{1}x_{1}+...+\beta _{n}x_{n}}+1}}}\]</span></p>
<p>where <span class="math inline">\(\beta _{0},\beta _{1},...,\beta _{n}\)</span> are coefficients of logistic regression. We obtain coefficients using gradient descent. Because we have multiple labels so we train 10 diffrent logistic regression models and use softmax function to normalize probablities of belonging to any particular class.
We then visualize coeficients as images with bright spots indicating that they are importent.
Unfortunately the results look like this:</p>
<div class="figure">
<img src="images/3-8-LOGREG.png" alt="" />
<p class="caption">fig. 1: An example of logistic regression weights</p>
</div>
<p>We get this because logistic regression works more like a sieve.</p>
<hr />
</div>
<div id="decision-trees" class="section level4" number="3.8.5.2">
<h4><span class="header-section-number">3.8.5.2</span> 3.8.4.2 Decision Trees</h4>
<p>Decision trees are very useful interpretable classifiers, however they are suitable when we have very few meaningful dimensions. A tree that does splits based on particular pixels is not a good classifier and it’s explanation provides little knowledge about, why those particular pixels has been chosen. That’s why we will not explore the use of this class of classifiers.</p>
<hr />
</div>
</div>
<div id="our-approach" class="section level3" number="3.8.6">
<h3><span class="header-section-number">3.8.6</span> 3.8.5 Our Approach</h3>
<p>In this section we will show alternative to logistic regression and decision trees, that is more interpretable and in the same time has a capacity to obtain significantly better results.</p>
<hr />
<div id="the-knn-classifier" class="section level4" number="3.8.6.1">
<h4><span class="header-section-number">3.8.6.1</span> 3.8.5.1 The KNN Classifier</h4>
<p>KNN (k nearest neighbours) is a classifier, that doesn’t generalize data. Instead, we keep the training dataset and every time we make a prediction, we calculate distance (for instance euclidean distance) between our new observation and all observations in the training dataset to find k nearest. Prediction is based on their labels.
KNN is a robust classifier, that copes with highly non linear data. It’s also interpretable, because we can always show k nearest neighbours, which are an explanation by themselves. It, however, is not flawless. It for instance poorly scales with the size of the training dataset, while in the same time it need, at least in some domains, very big training dataset, as it doesn’t generalize any information.
We can significantly improve it’s performance by introducing complex similarity functions. If similarity function is interpretable, we obtain highly interpretable classifier. If not, we get semi interpretable classifier, where we cannot tell, why obsevations are similar according to the model, however we can at least show similar training set examples, based on which prediction has been made.
This complex distance functions can be made in many different ways. In this paper we explore functions of a form:
<span class="math display">\[distance(Img1, Img2) = d_e(Embedder(Img1), Embedder(Img2))\]</span>
where <span class="math inline">\(d_e\)</span> is euclidean distance, so we simply compute euclidean distance between embeddings of images. Here’s a scheme of KNN classifier:
<img src="images/3-8-KNNMODEL_1.png" alt="fig. n: The KNN Classifier Architecture" />
As we can see on fig. n, new image firstly gets embedded and then a standard classification with KNN is made. This type of architecture allowes us to create robust and interpretable classifier.</p>
<hr />
</div>
<div id="embedding-techniques" class="section level4" number="3.8.6.2">
<h4><span class="header-section-number">3.8.6.2</span> 3.8.5.2 Embedding techniques</h4>
<p>In this section we will explore different embedding techniques.</p>
<hr />
<div id="k-means" class="section level5" number="3.8.6.2.1">
<h5><span class="header-section-number">3.8.6.2.1</span> 3.8.5.2.1 K-means</h5>
<p>Our problem is supervised one, but we can still use unsupervised aproach to get better results.
We use K-means algorithm (also known as Lloyd’s algorithm) to find subclasses in every class.
Algorithm:</p>
<ol style="list-style-type: decimal">
<li><p>Initiate number of random centroids</p></li>
<li><p>For every observation find nearest centroid</p></li>
<li><p>Calculate average of observations in every group found in point 2</p></li>
<li><p>This averages becomes new centroids</p></li>
<li><p>Repeat points 2 to 4 until all new centroids are at the distance less then <span class="math inline">\(\epsilon\)</span> from old centroids</p></li>
</ol>
<p>We use euclidean distance. Prediction for every new observation is simply class of nearest centroid. Algorithm is interpretable, because we can visualise centroids as images. Thanks to using K-means to find subclasses our images are not blurry.
Also because number of all subclasses is much lower than number of records in data set using KNN only on centroids is much faster. Consider the following dataset:</p>
<div class="figure">
<img src="images/3-8-KMEANS_1.png" alt="" />
<p class="caption">fig : An example of centroid image</p>
</div>
<p>In fact, a good subset of the training data set is enough to create a very good classifier. For instance we can choose:</p>
<div class="figure">
<img src="images/3-8-KMEANS_2.png" alt="" />
<p class="caption">fig : An example of centroid image</p>
</div>
<p>Chosen points approximate sufficiently training data distribution. Notice, that we in fact don’t have to choose particular observations. We can instead chose points in observation space that are similar to observations. This is what k-means algorithm do and so, we can obtain good training data approximation using k-means. Here’s an example of a centroid image:</p>
<div class="figure">
<img src="images/3-8-KLAPEK.png" alt="" />
<p class="caption">fig : An example of a centroid image</p>
</div>
<hr />
</div>
<div id="svd" class="section level5" number="3.8.6.2.2">
<h5><span class="header-section-number">3.8.6.2.2</span> 3.8.5.2.2 SVD</h5>
<p>SVD is a standard method of dimensionality reduction. It is rewriting <span class="math inline">\(m\times n\)</span> matrix
<span class="math inline">\(M\)</span> as <span class="math inline">\(U\Sigma V^T\)</span> where <span class="math inline">\(U\)</span> is <span class="math inline">\(m\times m\)</span> orthonormal matrix, <span class="math inline">\(V^T\)</span> is <span class="math inline">\(n\times n\)</span> orthonormal matrix
and <span class="math inline">\(\Sigma\)</span> is <span class="math inline">\(m\times n\)</span> rectangular diagonal matrix with non-negative real numbers on the diagonal.
We assume that singular values of <span class="math inline">\(\Sigma\)</span> are in descending order. Now by taking first columns of <span class="math inline">\(V^T\)</span> we
get vectors which are the most relevent. Let <span class="math inline">\(V_n\)</span> be matrix, whoose columns are <span class="math inline">\(V\)</span> columns with n greatest eigenvalues. Such matrix is linear transformation matrix, that turns observations into their embeddings. It can be shown, that <span class="math inline">\(V_n\)</span> is the best transformation in L2 norm sense.
<img src="images/3-8-SVDAUTOENC_1.png" alt="fig. : SVD autoencoder diagram" />
We can then visualise this vectors and see which parts of the picture are the most importent.
Also we can reduce number of dimensions for KNN.</p>
<div class="figure">
<img src="images/3-8-HIDDEN.png" alt="" />
<p class="caption">Most importenet vector</p>
</div>
<div class="figure">
<img src="images/3-8-KURTKA.png" alt="" />
<p class="caption">Image from Dataset</p>
</div>
<div class="figure">
<img src="images/3-8-KURTKA_HIDDEN.png" alt="" />
<p class="caption">Image after using vector as filter</p>
</div>
<hr />
</div>
<div id="convolutional-autoencoder" class="section level5" number="3.8.6.2.3">
<h5><span class="header-section-number">3.8.6.2.3</span> 3.8.5.2.3 Convolutional Autoencoder</h5>
<p>We can create semi interpretable model by training a convolutional autoencoder and then creating KNN classifier on pretrained embeddings. As mentioned previously, it has several advantages over KNN, because it uses euclidean distance in more meaningful space. Embedder is not interpretable, but our classifier can at least show us historical observations, that had an impact on prediction, which sometimes is good enough, especially when it can be easily seen why two images are similar and we only want a computer to do humans work. For instance, if we provide 5 images of coala that caused that our image of coala has been interpreted as coala, we maybe don’t know, why those images are similar according to our classifier, however we can see, that they are similar, so further explanation of a model is not required. This model, again, is not fully interpretable.
Our implementation of convolutional autoencoder consist of the following layers:</p>
<ul>
<li>Conv2d:
<ul>
<li>Input Channels: 1</li>
<li>Output Channels: 50</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 50</li>
<li>Output Channels: 50</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 50</li>
<li>Output Channels: 10</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 10</li>
<li>Output Channels: 10</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 10</li>
<li>Output Channels: 1</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 1</li>
<li>Output Channels: 10</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 10</li>
<li>Output Channels: 10</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 10</li>
<li>Output Channels: 50</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 50</li>
<li>Output Channels: 50</li>
<li>filter size: 5</li>
</ul></li>
<li>Conv2d:
<ul>
<li>Input Channels: 50</li>
<li>Output Channels: 1</li>
<li>filter size: 5</li>
</ul></li>
</ul>
<p>along with pooling and unpooling beetween.</p>
<div class="figure">
<img src="images/3-8-CONVAUTOENC_1.png" alt="" />
<p class="caption">fig. : Architecture of the convolutional autoencoder</p>
</div>
<hr />
</div>
</div>
</div>
<div id="black-box-convolutional-neural-networks" class="section level3" number="3.8.7">
<h3><span class="header-section-number">3.8.7</span> 3.8.6 Black-Box Convolutional Neural Networks</h3>
<p>Classical approach in computer vision is to use convolutional neural networks.
A standard artificial neural network sees all variables as being independent
from each other. It doesn’t capture the same patterns across image space, nor
it recognises, that two pixels next to each other are somehow related. Shifted
image is something completly different to a standard neural network from it’s original.
Therefore, training a standard neural network is tricky, it requires very big dataset
and takes a lot of time. There is, however, a smarter approach that has a capacity
to cope with those problems. Namely, convolutional neural networks.</p>
<p>A convolutional neural network is an artificial neural network, that tries to capture spacial dependencies between variables, for instance dimensions of pixels that are close to each other.
It does that via introducing convolution. The easiest interpretation of convolutional neural network is that instead of training training big network that uses all variables (in our case all pixels), we train smaller transformation with smaller number of variables (smaller subset of pixels close to each other), that we use in many different places on the image. In some sense we train filters. Every filter produces a corresponding so called “channel”. After first layer we can continue filtering channels using convolutional layers. We place a danse layer (or a number of them) at the end and it’s result is our prediction. For further reading, please see … .</p>
<p>Having a very good performance, they are impossible to explain. There are some techniques of visualising filters, however more complex networks are generally uninterpretable. Along with standard artificial neural network, we will use it as an instance of robust classifier for comparing results.
Our implementation of convolutional neural networks consist of the following layers:</p>
<ul>
<li>Conv 2d:
<ul>
<li>Input Channels: 1</li>
<li>Output Channels: 50</li>
<li>filter size: 5</li>
</ul></li>
<li>Max Pool:
<ul>
<li>Size: 2</li>
</ul></li>
<li>Conv 2d:
<ul>
<li>Input Channels: 50</li>
<li>Output Channels: 70</li>
<li>filter size: 5</li>
</ul></li>
<li>Max Pool:
<ul>
<li>Size: 2</li>
</ul></li>
<li>Conv 2d:
<ul>
<li>Input Channels: 70</li>
<li>Output Channels: 100</li>
<li>filter size: 5</li>
</ul></li>
<li>Max Pool:
<ul>
<li>Size: 2</li>
</ul></li>
<li>Conv 2d:
<ul>
<li>Input Channels: 100</li>
<li>Output Channels: 150</li>
<li>filter size: 5</li>
</ul></li>
<li>Linear:
<ul>
<li>Input_size: 1350</li>
<li>Output_size: 500</li>
</ul></li>
<li>Linear:
<ul>
<li>Input_size: 200</li>
<li>Output_size: 10</li>
</ul></li>
</ul>
<p>Here’s architecture’s visualisation:
<img src="images/3-8-BlackBoxCONV_1.png" alt="fig. : The architecture of the Convolutional classifier" /></p>
<hr />
</div>
<div id="results-17" class="section level3" number="3.8.8">
<h3><span class="header-section-number">3.8.8</span> Results</h3>
<table>
<thead>
<tr class="header">
<th>Model</th>
<th>ACC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Black-Box Convolutional</td>
<td>0.941</td>
</tr>
<tr class="even">
<td>Logistic regression</td>
<td>0.847</td>
</tr>
<tr class="odd">
<td>KNN base</td>
<td>0.8606</td>
</tr>
<tr class="even">
<td>KNN SVD</td>
<td>0.8001</td>
</tr>
<tr class="odd">
<td>KNN K-means</td>
<td>0.8512</td>
</tr>
<tr class="even">
<td>KNN Convolutional</td>
<td>0.902</td>
</tr>
</tbody>
</table>
</div>
<div id="conclusions-2" class="section level3" number="3.8.9">
<h3><span class="header-section-number">3.8.9</span> Conclusions</h3>
</div>
<div id="bibliography" class="section level3" number="3.8.10">
<h3><span class="header-section-number">3.8.10</span> Bibliography</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-UME">
<p>accenture. 2018. “UNDERSTANDING Machines: EXPLAINABLE Ai,” 19. <a href="https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g">https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf?fbclid=IwAR0ZtyDNzHR8dMUJHPwa0CkuQXgOOE68UQV4JCcBxXudO3dlm14LjqX-B8g</a>.</p>
</div>
<div id="ref-EAICTOC">
<p>Alejandro Barredo Arrieta, Javier Del Ser, Natalia Díaz-Rodríguez. 2019. “Explainable Artificial Intelligence (Xai): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible Ai,” 67. <a href="https://arxiv.org/abs/1910.10045">https://arxiv.org/abs/1910.10045</a>.</p>
</div>
<div id="ref-BO">
<p>Bernd Bischl, Jakob Bossek, Jakob Richter. 2018. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions,” 23. <a href="https://arxiv.org/abs/1703.03373">https://arxiv.org/abs/1703.03373</a>.</p>
</div>
<div id="ref-DALEX">
<p>Biecek, Przemyslaw. 2018. “DALEX: Explainers for Complex Predictive Models in R.” <em>Journal of Machine Learning Research</em> 19 (84): 1–5. <a href="http://jmlr.org/papers/v19/18-416.html">http://jmlr.org/papers/v19/18-416.html</a>.</p>
</div>
<div id="ref-mlr">
<p>Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. 2016b. “mlr: Machine Learning in R.” <em>Journal of Machine Learning Research</em> 17 (170): 1–5. <a href="http://jmlr.org/papers/v17/15-066.html">http://jmlr.org/papers/v17/15-066.html</a>.</p>
</div>
<div id="ref-mlrmbo">
<p>Bischl, Bernd, Jakob Richter, Jakob Bossek, Daniel Horn, Janek Thomas, and Michel Lang. 2017. “MlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions.” <em>arXiv Preprint arXiv:1703.03373</em>.</p>
</div>
<div id="ref-housepricesdata">
<p>“Census dataset.” 1996. <a href="http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html">http://www.cs.toronto.edu/~delve/data/census-house/censusDetail.html</a>.</p>
</div>
<div id="ref-fenton97">
<p>Fenton, N. E., and S. L. Pfleeger. 1997. <em>Software Metrics: A Rigorous &amp; Practical Approach</em>. International Thompson Press.</p>
</div>
<div id="ref-auc">
<p>Flach, Peter, Jose Hernandez-Orallo, and Cèsar Ferri. 2011. “A Coherent Interpretation of Auc as a Measure of Aggregated Classification Performance.” In <em>Proceedings of the 28th International Conference on Machine Learning, ICML 2011</em>, 657–64.</p>
</div>
<div id="ref-pdp">
<p>Friedman, Jerome. 2000. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>The Annals of Statistics</em> 29 (November). <a href="https://doi.org/10.1214/aos/1013203451">https://doi.org/10.1214/aos/1013203451</a>.</p>
</div>
<div id="ref-gosiewska2019safe">
<p>Gosiewska, Alicja, Aleksandra Gacek, Piotr Lubon, and Przemyslaw Biecek. 2019. “SAFE Ml: Surrogate Assisted Feature Extraction for Model Learning.” <a href="http://arxiv.org/abs/1902.11035">http://arxiv.org/abs/1902.11035</a>.</p>
</div>
<div id="ref-halstead77">
<p>Halstead, M. H. 1977. <em>Elements of Software Science</em>. Elsevier.</p>
</div>
<div id="ref-smote">
<p>Hu, Feng, and Hang Li. 2013. “A Novel Boundary Oversampling Algorithm Based on Neighborhood Rough Set Model: NRSBoundary-Smote.” <em>Mathematical Problems in Engineering</em> 2013 (November). <a href="https://doi.org/10.1155/2013/694809">https://doi.org/10.1155/2013/694809</a>.</p>
</div>
<div id="ref-drake">
<p>Landau, William Michael. 2018. “The Drake R Package: A Pipeline Toolkit for Reproducibility and High-Performance Computing.” <em>Journal of Open Source Software</em> 3 (21). <a href="https://doi.org/10.21105/joss.00550">https://doi.org/10.21105/joss.00550</a>.</p>
</div>
<div id="ref-mccabe76">
<p>McCabe, T. J. 1976. “A Complexity Measure.” <em>IEEE Transactions on Software Engineering</em> 2 (4): 308–20.</p>
</div>
<div id="ref-VELOSODEMELO">
<p>Melo], Vinícius [Veloso de, and Wolfgang Banzhaf. 2018. “Automatic Feature Engineering for Regression Models with Machine Learning: An Evolutionary Computation and Statistics Hybrid.” <em>Information Sciences</em> 430-431: 287–313. <a href="https://doi.org/https://doi.org/10.1016/j.ins.2017.11.041">https://doi.org/https://doi.org/10.1016/j.ins.2017.11.041</a>.</p>
</div>
<div id="ref-christophmonlar">
<p>Molnar, Christoph. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>. <a href="https://christophm.github.io/interpretable-ml-book/simple.html">https://christophm.github.io/interpretable-ml-book/simple.html</a>.</p>
</div>
<div id="ref-IMLDEA">
<p>Murdoch, Kumbier, Singh. 2018. “Interpretable Machine Learning: Definitions, Methods, and Applications,” 2. <a href="https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ">https://arxiv.org/pdf/1901.04592.pdf?fbclid=IwAR2frcHrhLc4iaH5-TmKKq263NVvAKHtG4uQoiVNDeLAG3QFzdje-yzZjiQ</a>.</p>
</div>
<div id="ref-IML">
<p>Pandey. 2019. “Interpretable Machine Learning: Extracting Human Understandable Insights from Any Machine Learning Model,” April. <a href="https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b">https://towardsdatascience.com/interpretable-machine-learning-1dec0f2f3e6b</a>.</p>
</div>
<div id="ref-ETFFE">
<p>Patricia Arroba, Marina Zapater, José L. Risco-Martín. 2015. “Enhancing Regression Models for Complex Systems Using Evolutionary Techniques for Feature Engineering.” <em>Journal of Grid Computing</em> 13: 409–23. <a href="https://doi.org/10.1007/s10723-014-9313-8">https://doi.org/10.1007/s10723-014-9313-8</a>.</p>
</div>
<div id="ref-boxcox">
<p>Sakia, R. M. 1992. “The Box-Cox Transformation Technique: A Review.” <em>Journal of the Royal Statistical Society. Series D (the Statistician)</em> 41 (2): 169–78. <a href="http://www.jstor.org/stable/2348250">http://www.jstor.org/stable/2348250</a>.</p>
</div>
<div id="ref-Sayyad-Shirabad+Menzies:2005">
<p>Sayyad Shirabad, J., and T. J. Menzies. 2005. “The PROMISE Repository of Software Engineering Databases.” School of Information Technology and Engineering, University of Ottawa, Canada. <a href="http://promise.site.uottawa.ca/SERepository">http://promise.site.uottawa.ca/SERepository</a>.</p>
</div>
<div id="ref-MICE">
<p>Stef van Buuren, Karin Groothuis-Oudshoorn. 2011. “Mice: Multivariate Imputation by Chained Equations in R.” <a href="https://www.jstatsoft.org/article/view/v045i03">https://www.jstatsoft.org/article/view/v045i03</a>.</p>
</div>
<div id="ref-rpart">
<p>Therneau, Terry, and Beth Atkinson. 2019. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-OpenML2013">
<p>Vanschoren, Joaquin, Jan N. van Rijn, Bernd Bischl, and Luis Torgo. 2013. “OpenML: Networked Science in Machine Learning.” <em>SIGKDD Explorations</em> 15 (2): 49–60. <a href="https://doi.org/10.1145/2641190.2641198">https://doi.org/10.1145/2641190.2641198</a>.</p>
</div>
<div id="ref-ranger">
<p>Wright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” <em>Journal of Statistical Software</em> 77 (1): 1–17. <a href="https://doi.org/10.18637/jss.v077.i01">https://doi.org/10.18637/jss.v077.i01</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="imputation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="acknowledgements.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mini-pw/2020L-WB-Book/edit/master/3-0-interpretability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
