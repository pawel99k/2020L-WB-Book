<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Reproducibility of scientific papers | ML Case Studies</title>
  <meta name="description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="generator" content="bookdown 0.19 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Reproducibility of scientific papers | ML Case Studies" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.png" />
  <meta property="og:description" content="Case studies for reproducibility, imputation, and interpretability" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Reproducibility of scientific papers | ML Case Studies" />
  
  <meta name="twitter:description" content="Case studies for reproducibility, imputation, and interpretability" />
  <meta name="twitter:image" content="images/cover.png" />



<meta name="date" content="2020-05-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="imputation.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint/kePrint.js"></script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><h3>ML Case Studies</h3></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="reproducibility.html"><a href="reproducibility.html"><i class="fa fa-check"></i><b>1</b> Reproducibility of scientific papers</a>
<ul>
<li class="chapter" data-level="1.1" data-path="reproducibility.html"><a href="reproducibility.html#title-of-the-article"><i class="fa fa-check"></i><b>1.1</b> Title of the article</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract"><i class="fa fa-check"></i><b>1.1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.1.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation"><i class="fa fa-check"></i><b>1.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.1.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work"><i class="fa fa-check"></i><b>1.1.3</b> Related Work</a></li>
<li class="chapter" data-level="1.1.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology"><i class="fa fa-check"></i><b>1.1.4</b> Methodology</a></li>
<li class="chapter" data-level="1.1.5" data-path="reproducibility.html"><a href="reproducibility.html#results"><i class="fa fa-check"></i><b>1.1.5</b> Results</a></li>
<li class="chapter" data-level="1.1.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions"><i class="fa fa-check"></i><b>1.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="reproducibility.html"><a href="reproducibility.html#how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers"><i class="fa fa-check"></i><b>1.2</b> How to measure reproducibility? Classification of problems with reproducing scientific papers</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-1"><i class="fa fa-check"></i><b>1.2.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction"><i class="fa fa-check"></i><b>1.2.2</b> Introduction</a></li>
<li class="chapter" data-level="1.2.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-1"><i class="fa fa-check"></i><b>1.2.3</b> Related Work</a></li>
<li class="chapter" data-level="1.2.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-1"><i class="fa fa-check"></i><b>1.2.4</b> Methodology</a></li>
<li class="chapter" data-level="1.2.5" data-path="reproducibility.html"><a href="reproducibility.html#results-1"><i class="fa fa-check"></i><b>1.2.5</b> Results</a></li>
<li class="chapter" data-level="1.2.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-1"><i class="fa fa-check"></i><b>1.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="reproducibility.html"><a href="reproducibility.html#aging-articles.-how-time-affects-reproducibility-of-scientific-papers"><i class="fa fa-check"></i><b>1.3</b> Aging articles. How time affects reproducibility of scientific papers?</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-2"><i class="fa fa-check"></i><b>1.3.1</b> Abstract</a></li>
<li class="chapter" data-level="1.3.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-1"><i class="fa fa-check"></i><b>1.3.2</b> Introduction</a></li>
<li class="chapter" data-level="1.3.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-2"><i class="fa fa-check"></i><b>1.3.3</b> Methodology</a></li>
<li class="chapter" data-level="1.3.4" data-path="reproducibility.html"><a href="reproducibility.html#results-2"><i class="fa fa-check"></i><b>1.3.4</b> Results</a></li>
<li class="chapter" data-level="1.3.5" data-path="reproducibility.html"><a href="reproducibility.html#conclusions"><i class="fa fa-check"></i><b>1.3.5</b> Conclusions</a></li>
<li class="chapter" data-level="1.3.6" data-path="reproducibility.html"><a href="reproducibility.html#summary"><i class="fa fa-check"></i><b>1.3.6</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="reproducibility.html"><a href="reproducibility.html#ways-to-reproduce-articles-in-terms-of-release-date-and-magazine"><i class="fa fa-check"></i><b>1.4</b> Ways to reproduce articles in terms of release date and magazine</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-3"><i class="fa fa-check"></i><b>1.4.1</b> Abstract</a></li>
<li class="chapter" data-level="1.4.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-1"><i class="fa fa-check"></i><b>1.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.4.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-2"><i class="fa fa-check"></i><b>1.4.3</b> Related Work</a></li>
<li class="chapter" data-level="1.4.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-3"><i class="fa fa-check"></i><b>1.4.4</b> Methodology</a></li>
<li class="chapter" data-level="1.4.5" data-path="reproducibility.html"><a href="reproducibility.html#results-3"><i class="fa fa-check"></i><b>1.4.5</b> Results</a></li>
<li class="chapter" data-level="1.4.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-2"><i class="fa fa-check"></i><b>1.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-of-outdated-articles-about-up-to-date-r-packages"><i class="fa fa-check"></i><b>1.5</b> Reproducibility of outdated articles about up-to-date R packages</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-4"><i class="fa fa-check"></i><b>1.5.1</b> Abstract</a></li>
<li class="chapter" data-level="1.5.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-2"><i class="fa fa-check"></i><b>1.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.5.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-3"><i class="fa fa-check"></i><b>1.5.3</b> Related Work</a></li>
<li class="chapter" data-level="1.5.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-4"><i class="fa fa-check"></i><b>1.5.4</b> Methodology</a></li>
<li class="chapter" data-level="1.5.5" data-path="reproducibility.html"><a href="reproducibility.html#results-4"><i class="fa fa-check"></i><b>1.5.5</b> Results</a></li>
<li class="chapter" data-level="1.5.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-3"><i class="fa fa-check"></i><b>1.5.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="reproducibility.html"><a href="reproducibility.html#correlation-between-reproducibility-of-components-of-research-papers-and-their-purpose"><i class="fa fa-check"></i><b>1.6</b> Correlation between reproducibility of components of research papers and their purpose</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-5"><i class="fa fa-check"></i><b>1.6.1</b> Abstract</a></li>
<li class="chapter" data-level="1.6.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-3"><i class="fa fa-check"></i><b>1.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.6.3" data-path="reproducibility.html"><a href="reproducibility.html#related-work-4"><i class="fa fa-check"></i><b>1.6.3</b> Related Work</a></li>
<li class="chapter" data-level="1.6.4" data-path="reproducibility.html"><a href="reproducibility.html#methodology-5"><i class="fa fa-check"></i><b>1.6.4</b> Methodology</a></li>
<li class="chapter" data-level="1.6.5" data-path="reproducibility.html"><a href="reproducibility.html#results-5"><i class="fa fa-check"></i><b>1.6.5</b> Results</a></li>
<li class="chapter" data-level="1.6.6" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-4"><i class="fa fa-check"></i><b>1.6.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="reproducibility.html"><a href="reproducibility.html#how-active-development-affects-reproducibility"><i class="fa fa-check"></i><b>1.7</b> How active development affects reproducibility</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-6"><i class="fa fa-check"></i><b>1.7.1</b> Abstract</a></li>
<li class="chapter" data-level="1.7.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-4"><i class="fa fa-check"></i><b>1.7.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.7.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-6"><i class="fa fa-check"></i><b>1.7.3</b> Methodology</a></li>
<li class="chapter" data-level="1.7.4" data-path="reproducibility.html"><a href="reproducibility.html#results-6"><i class="fa fa-check"></i><b>1.7.4</b> Results</a></li>
<li class="chapter" data-level="1.7.5" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-5"><i class="fa fa-check"></i><b>1.7.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="reproducibility.html"><a href="reproducibility.html#reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language"><i class="fa fa-check"></i><b>1.8</b> Reproducibility differences of articles published in various journals and using R or Python language</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="reproducibility.html"><a href="reproducibility.html#abstract-7"><i class="fa fa-check"></i><b>1.8.1</b> Abstract</a></li>
<li class="chapter" data-level="1.8.2" data-path="reproducibility.html"><a href="reproducibility.html#introduction-and-motivation-5"><i class="fa fa-check"></i><b>1.8.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="1.8.3" data-path="reproducibility.html"><a href="reproducibility.html#methodology-7"><i class="fa fa-check"></i><b>1.8.3</b> Methodology</a></li>
<li class="chapter" data-level="1.8.4" data-path="reproducibility.html"><a href="reproducibility.html#results-7"><i class="fa fa-check"></i><b>1.8.4</b> Results</a></li>
<li class="chapter" data-level="1.8.5" data-path="reproducibility.html"><a href="reproducibility.html#summary-and-conclusions-6"><i class="fa fa-check"></i><b>1.8.5</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="imputation.html"><a href="imputation.html"><i class="fa fa-check"></i><b>2</b> Imputation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="imputation.html"><a href="imputation.html#default-imputation-efficiency-comparision"><i class="fa fa-check"></i><b>2.1</b> Default imputation efficiency comparision</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="imputation.html"><a href="imputation.html#abstract-8"><i class="fa fa-check"></i><b>2.1.1</b> Abstract</a></li>
<li class="chapter" data-level="2.1.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-6"><i class="fa fa-check"></i><b>2.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.1.3" data-path="imputation.html"><a href="imputation.html#related-work-5"><i class="fa fa-check"></i><b>2.1.3</b> Related Work</a></li>
<li class="chapter" data-level="2.1.4" data-path="imputation.html"><a href="imputation.html#methodology-8"><i class="fa fa-check"></i><b>2.1.4</b> Methodology</a></li>
<li class="chapter" data-level="2.1.5" data-path="imputation.html"><a href="imputation.html#results-8"><i class="fa fa-check"></i><b>2.1.5</b> Results</a></li>
<li class="chapter" data-level="2.1.6" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-7"><i class="fa fa-check"></i><b>2.1.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="imputation.html"><a href="imputation.html#various-data-imputation-techniques-in-r"><i class="fa fa-check"></i><b>2.2</b> Various data imputation techniques in R</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="imputation.html"><a href="imputation.html#abstract-9"><i class="fa fa-check"></i><b>2.2.1</b> Abstract</a></li>
<li class="chapter" data-level="2.2.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-7"><i class="fa fa-check"></i><b>2.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.2.3" data-path="imputation.html"><a href="imputation.html#methodology-9"><i class="fa fa-check"></i><b>2.2.3</b> Methodology</a></li>
<li class="chapter" data-level="2.2.4" data-path="imputation.html"><a href="imputation.html#results-9"><i class="fa fa-check"></i><b>2.2.4</b> Results</a></li>
<li class="chapter" data-level="2.2.5" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-8"><i class="fa fa-check"></i><b>2.2.5</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="imputation.html"><a href="imputation.html#comparison-of-efficiency-of-various-data-imputation-techniques"><i class="fa fa-check"></i><b>2.3</b> Comparison of efficiency of various data imputation techniques</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="imputation.html"><a href="imputation.html#abstract-10"><i class="fa fa-check"></i><b>2.3.1</b> Abstract</a></li>
<li class="chapter" data-level="2.3.2" data-path="imputation.html"><a href="imputation.html#introduction-and-motivation-8"><i class="fa fa-check"></i><b>2.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="2.3.3" data-path="imputation.html"><a href="imputation.html#related-work-6"><i class="fa fa-check"></i><b>2.3.3</b> Related Work</a></li>
<li class="chapter" data-level="2.3.4" data-path="imputation.html"><a href="imputation.html#methodology-10"><i class="fa fa-check"></i><b>2.3.4</b> Methodology</a></li>
<li class="chapter" data-level="2.3.5" data-path="imputation.html"><a href="imputation.html#results-10"><i class="fa fa-check"></i><b>2.3.5</b> Results</a></li>
<li class="chapter" data-level="2.3.6" data-path="imputation.html"><a href="imputation.html#summary-and-conclusions-9"><i class="fa fa-check"></i><b>2.3.6</b> Summary and conclusions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>3</b> Interpretability</a>
<ul>
<li class="chapter" data-level="3.1" data-path="interpretability.html"><a href="interpretability.html#building-an-explainable-model-for-ordinal-classification.-meeting-black-box-model-performance-levels."><i class="fa fa-check"></i><b>3.1</b> Building an explainable model for ordinal classification. Meeting black box model performance levels.</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="interpretability.html"><a href="interpretability.html#abstract-11"><i class="fa fa-check"></i><b>3.1.1</b> Abstract</a></li>
<li class="chapter" data-level="3.1.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-9"><i class="fa fa-check"></i><b>3.1.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.1.3" data-path="interpretability.html"><a href="interpretability.html#related-work-7"><i class="fa fa-check"></i><b>3.1.3</b> Related Work</a></li>
<li class="chapter" data-level="3.1.4" data-path="interpretability.html"><a href="interpretability.html#methodology-11"><i class="fa fa-check"></i><b>3.1.4</b> Methodology</a></li>
<li class="chapter" data-level="3.1.5" data-path="interpretability.html"><a href="interpretability.html#results-11"><i class="fa fa-check"></i><b>3.1.5</b> Results</a></li>
<li class="chapter" data-level="3.1.6" data-path="interpretability.html"><a href="interpretability.html#model-explanantion"><i class="fa fa-check"></i><b>3.1.6</b> Model explanantion</a></li>
<li class="chapter" data-level="3.1.7" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-10"><i class="fa fa-check"></i><b>3.1.7</b> Summary and conclusions</a></li>
<li class="chapter" data-level="3.1.8" data-path="interpretability.html"><a href="interpretability.html#references"><i class="fa fa-check"></i><b>3.1.8</b> References</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="interpretability.html"><a href="interpretability.html#predicting-code-defects-using-interpretable-static-measures."><i class="fa fa-check"></i><b>3.2</b> Predicting code defects using interpretable static measures.</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="interpretability.html"><a href="interpretability.html#abstract-12"><i class="fa fa-check"></i><b>3.2.1</b> Abstract</a></li>
<li class="chapter" data-level="3.2.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-10"><i class="fa fa-check"></i><b>3.2.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.2.3" data-path="interpretability.html"><a href="interpretability.html#dataset"><i class="fa fa-check"></i><b>3.2.3</b> Dataset</a></li>
<li class="chapter" data-level="3.2.4" data-path="interpretability.html"><a href="interpretability.html#methodology-12"><i class="fa fa-check"></i><b>3.2.4</b> Methodology</a></li>
<li class="chapter" data-level="3.2.5" data-path="interpretability.html"><a href="interpretability.html#results-12"><i class="fa fa-check"></i><b>3.2.5</b> Results</a></li>
<li class="chapter" data-level="3.2.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-11"><i class="fa fa-check"></i><b>3.2.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="interpretability.html"><a href="interpretability.html#using-interpretable-machine-learning-models-in-the-higgs-boson-detection."><i class="fa fa-check"></i><b>3.3</b> Using interpretable Machine Learning models in the Higgs boson detection.</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="interpretability.html"><a href="interpretability.html#abstract-13"><i class="fa fa-check"></i><b>3.3.1</b> Abstract</a></li>
<li class="chapter" data-level="3.3.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-11"><i class="fa fa-check"></i><b>3.3.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.3.3" data-path="interpretability.html"><a href="interpretability.html#related-work-8"><i class="fa fa-check"></i><b>3.3.3</b> Related Work</a></li>
<li class="chapter" data-level="3.3.4" data-path="interpretability.html"><a href="interpretability.html#methodology-13"><i class="fa fa-check"></i><b>3.3.4</b> Methodology</a></li>
<li class="chapter" data-level="3.3.5" data-path="interpretability.html"><a href="interpretability.html#results-13"><i class="fa fa-check"></i><b>3.3.5</b> Results</a></li>
<li class="chapter" data-level="3.3.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-12"><i class="fa fa-check"></i><b>3.3.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="interpretability.html"><a href="interpretability.html#can-automated-regression-beat-linear-model"><i class="fa fa-check"></i><b>3.4</b> Can Automated Regression beat linear model?</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="interpretability.html"><a href="interpretability.html#abstract-14"><i class="fa fa-check"></i><b>3.4.1</b> Abstract</a></li>
<li class="chapter" data-level="3.4.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-12"><i class="fa fa-check"></i><b>3.4.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.4.3" data-path="interpretability.html"><a href="interpretability.html#data-1"><i class="fa fa-check"></i><b>3.4.3</b> Data</a></li>
<li class="chapter" data-level="3.4.4" data-path="interpretability.html"><a href="interpretability.html#methodology-14"><i class="fa fa-check"></i><b>3.4.4</b> Methodology</a></li>
<li class="chapter" data-level="3.4.5" data-path="interpretability.html"><a href="interpretability.html#results-14"><i class="fa fa-check"></i><b>3.4.5</b> Results</a></li>
<li class="chapter" data-level="3.4.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-13"><i class="fa fa-check"></i><b>3.4.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="interpretability.html"><a href="interpretability.html#interpretable-non-linear-feature-engineering-techniques-for-linear-regression-models---exploration-on-concrete-compressive-strength-dataset-with-a-new-feature-importance-metric."><i class="fa fa-check"></i><b>3.5</b> Interpretable, non-linear feature engineering techniques for linear regression models - exploration on concrete compressive strength dataset with a new feature importance metric.</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="interpretability.html"><a href="interpretability.html#abstract-15"><i class="fa fa-check"></i><b>3.5.1</b> Abstract</a></li>
<li class="chapter" data-level="3.5.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-13"><i class="fa fa-check"></i><b>3.5.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.5.3" data-path="interpretability.html"><a href="interpretability.html#related-work-9"><i class="fa fa-check"></i><b>3.5.3</b> Related Work</a></li>
<li class="chapter" data-level="3.5.4" data-path="interpretability.html"><a href="interpretability.html#methodology-15"><i class="fa fa-check"></i><b>3.5.4</b> Methodology</a></li>
<li class="chapter" data-level="3.5.5" data-path="interpretability.html"><a href="interpretability.html#results-15"><i class="fa fa-check"></i><b>3.5.5</b> Results</a></li>
<li class="chapter" data-level="3.5.6" data-path="interpretability.html"><a href="interpretability.html#summary-and-conclusions-14"><i class="fa fa-check"></i><b>3.5.6</b> Summary and conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="interpretability.html"><a href="interpretability.html#surpassing-black-box-models-performance-on-unbalanced-data-with-an-interpretable-one-using-advanced-feature-engineering"><i class="fa fa-check"></i><b>3.6</b> Surpassing black box model’s performance on unbalanced data with an interpretable one using advanced feature engineering</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="interpretability.html"><a href="interpretability.html#abstract-16"><i class="fa fa-check"></i><b>3.6.1</b> Abstract</a></li>
<li class="chapter" data-level="3.6.2" data-path="interpretability.html"><a href="interpretability.html#introduction-and-motivation-14"><i class="fa fa-check"></i><b>3.6.2</b> Introduction and Motivation</a></li>
<li class="chapter" data-level="3.6.3" data-path="interpretability.html"><a href="interpretability.html#data-2"><i class="fa fa-check"></i><b>3.6.3</b> Data</a></li>
<li class="chapter" data-level="3.6.4" data-path="interpretability.html"><a href="interpretability.html#related-work-10"><i class="fa fa-check"></i><b>3.6.4</b> Related work</a></li>
<li class="chapter" data-level="3.6.5" data-path="interpretability.html"><a href="interpretability.html#methodology-16"><i class="fa fa-check"></i><b>3.6.5</b> Methodology</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="interpretability.html"><a href="interpretability.html#which-neighbours-affected-house-prices-in-the-90s"><i class="fa fa-check"></i><b>3.7</b> Which Neighbours Affected House Prices in the ’90s?</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="interpretability.html"><a href="interpretability.html#introduction-2"><i class="fa fa-check"></i><b>3.7.1</b> Introduction</a></li>
<li class="chapter" data-level="3.7.2" data-path="interpretability.html"><a href="interpretability.html#related-work-11"><i class="fa fa-check"></i><b>3.7.2</b> Related Work</a></li>
<li class="chapter" data-level="3.7.3" data-path="interpretability.html"><a href="interpretability.html#data-3"><i class="fa fa-check"></i><b>3.7.3</b> Data</a></li>
<li class="chapter" data-level="3.7.4" data-path="interpretability.html"><a href="interpretability.html#methodology-17"><i class="fa fa-check"></i><b>3.7.4</b> Methodology</a></li>
<li class="chapter" data-level="3.7.5" data-path="interpretability.html"><a href="interpretability.html#results-16"><i class="fa fa-check"></i><b>3.7.5</b> Results</a></li>
<li class="chapter" data-level="3.7.6" data-path="interpretability.html"><a href="interpretability.html#conclusions-1"><i class="fa fa-check"></i><b>3.7.6</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="interpretability.html"><a href="interpretability.html#explainable-computer-vision-with-embeddings-and-knn-classifier"><i class="fa fa-check"></i><b>3.8</b> Explainable Computer Vision with embeddings and KNN classifier</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="interpretability.html"><a href="interpretability.html#abstract-17"><i class="fa fa-check"></i><b>3.8.1</b> Abstract</a></li>
<li class="chapter" data-level="3.8.2" data-path="interpretability.html"><a href="interpretability.html#introduction-3"><i class="fa fa-check"></i><b>3.8.2</b> 3.8.1 Introduction</a></li>
<li class="chapter" data-level="3.8.3" data-path="interpretability.html"><a href="interpretability.html#data-4"><i class="fa fa-check"></i><b>3.8.3</b> 3.8.2 Data</a></li>
<li class="chapter" data-level="3.8.4" data-path="interpretability.html"><a href="interpretability.html#methodology-18"><i class="fa fa-check"></i><b>3.8.4</b> 3.8.3 Methodology</a></li>
<li class="chapter" data-level="3.8.5" data-path="interpretability.html"><a href="interpretability.html#standard-intepretable-models"><i class="fa fa-check"></i><b>3.8.5</b> 3.8.4 Standard Intepretable Models</a></li>
<li class="chapter" data-level="3.8.6" data-path="interpretability.html"><a href="interpretability.html#our-approach"><i class="fa fa-check"></i><b>3.8.6</b> 3.8.5 Our Approach</a></li>
<li class="chapter" data-level="3.8.7" data-path="interpretability.html"><a href="interpretability.html#black-box-convolutional-neural-networks"><i class="fa fa-check"></i><b>3.8.7</b> 3.8.6 Black-Box Convolutional Neural Networks</a></li>
<li class="chapter" data-level="3.8.8" data-path="interpretability.html"><a href="interpretability.html#results-17"><i class="fa fa-check"></i><b>3.8.8</b> Results</a></li>
<li class="chapter" data-level="3.8.9" data-path="interpretability.html"><a href="interpretability.html#conclusions-2"><i class="fa fa-check"></i><b>3.8.9</b> Conclusions</a></li>
<li class="chapter" data-level="3.8.10" data-path="interpretability.html"><a href="interpretability.html#bibliography"><i class="fa fa-check"></i><b>3.8.10</b> Bibliography</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>4</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">ML Case Studies</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reproducibility" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Reproducibility of scientific papers</h1>
<p>This chapter contains a wide range of studies on the reproducibility of scientific articles.
Each subsection is a self-contained paper answering a different research problem.</p>
<p>Please, note that each subsection contains a work of different authors and therefore subsections may differ at some points, for example, definitions of reproducibility used in particular studies.</p>

<div id="title-of-the-article" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Title of the article</h2>
<p><em>Authors: Author 1, Author 2, Author 3 (University)</em></p>
<div id="abstract" class="section level3" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation" class="section level3" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> Introduction and Motivation</h3>
</div>
<div id="related-work" class="section level3" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Related Work</h3>
</div>
<div id="methodology" class="section level3" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Methodology</h3>
</div>
<div id="results" class="section level3" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Results</h3>
</div>
<div id="summary-and-conclusions" class="section level3" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="how-to-measure-reproducibility-classification-of-problems-with-reproducing-scientific-papers" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> How to measure reproducibility? Classification of problems with reproducing scientific papers</h2>
<p><em>Authors: Paweł Koźmiński, Anna Urbala, Wojciech Szczypek (Warsaw University of Technology)</em></p>
<div id="abstract-1" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Abstract</h3>
<p>After quite short time the computational aspects of scientific papers become obsolete or inexecutable (e.g. codes are not valid with current environment, resources are inaccessible, some classes require additional parameters). The extent of such a loss of timeliness of a paper may vary a lot. We develop a scale suitable for comparing reproducibility of various papers. This scale is based on enumerating and subjective evaluation of the impact of irreproducibilities on the reception of the paper.</p>
</div>
<div id="introduction" class="section level3" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Introduction</h3>
<p>The idea of reproducibility of scientific researches is crucial especially in the area of data science. It has become more important along with the development of methods and algorithms used in machine learning as they are more and more complex and complicated. This issue concerns users of all types: students, scientists, developers. Moreover, attaching code used in a paper, helps readers to focus on the real content rather than sophisticated explanations and descriptions included in the article. It is also valuable because the users can use the code as examples of using the package. <br></p>
<p>However problem of the reproducibility is much more complex, because there is no explicit way of measuring it. It means that most of its definitions divide articles into 2 groups - reproducible and irreproducible. Thus, finding an appropriate reproducibility metrics, which would have wider set of values would result in changing the way reproducability is perceived. As a result such a metric would provide much more information for a person who would be interested in reproducing an article.</p>
<div id="definition" class="section level4" number="1.2.2.1">
<h4><span class="header-section-number">1.2.2.1</span> Definition</h4>
<p>Reproducibility as a problem has been addressed by scientists of various fields of studies. The exact definition also differs among areas of studies. For instance, Patrick Vandewall in 2009 suggested a definition of a reproducible research work: “A research work is called reproducible if all information relevant to the work, including, but not limited to, text, data and code, is made available, such that an independent researcher can reproduce the results” <span class="citation">(Vandewalle, Kovacevic, and Vetterli <a href="#ref-vandewalle2009reproducible" role="doc-biblioref">2009</a>)</span>. On the other hand, Association for Computing Machinery <span class="citation">(Computing Machinery <a href="#ref-ACMBadging2018" role="doc-biblioref">2018</a>)</span> divides the problem into three tasks as follows:<br></p>
<ul>
<li><p><strong>Repeatability</strong> (Same team, same experimental setup):<br>
The measurement can be obtained with stated precision by the same team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same location on multiple trials. For computational experiments, this means that a researcher can reliably repeat her own computation.</p></li>
<li><p><strong>Replicability</strong> (Different team, same experimental setup):<br>
The measurement can be obtained with stated precision by a different team using the same measurement procedure, the same measuring system, under the same operating conditions, in the same or a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using the author’s own artifacts.</p></li>
<li><p><strong>Reproducibility</strong> (Different team, different experimental setup):<br>
The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same result using artifacts which they develop completely independently.
<br></p></li>
</ul>
<p>For the needs of this chapter we will use the Vandewalle’s definition and treat papers as fully reproducible only when they meet the conditions listed there.</p>
</div>
</div>
<div id="related-work-1" class="section level3" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> Related Work</h3>
<p>Reproducibility is a hot topic. “Open Science in Software Engineering” <span class="citation">(Fernández et al. <a href="#ref-Fernndez2019OpenSI" role="doc-biblioref">2019</a>)</span> describes the essence of <em>open source</em>, <em>open data</em>, <em>open access</em> and other <em>openness</em>. The article mentions that ability to reproduce work is important for the value of research. <em>Open Science</em> has many positive effects: increases access and citation counts, supports cooperation through open repositories. “Reproducibility Guide” <span class="citation">(“Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing” <a href="#ref-repro-guide" role="doc-biblioref">2014</a>)</span> contains a lot of informations and tips on how to make research easier to reproduce. The guide also contains the list of tools that can make our research more reproducible (for example version control and automation. And the most important for us: it includes the checklist of questions that help verify the ability to reproduce. Edward Raff emphasizes the word <em>independent</em> in his article <span class="citation">(Raff <a href="#ref-RaffTheGradient2020" role="doc-biblioref">2020</a>)</span>. <em>Independent reproducibility</em> means that we can obtain similar results independently of the author and his code.</p>
<p>These articles highlight various aspects of reproducibility. We want to verify how the authors care about reproducibility, what are their biggest reproduction issues and what type of problems can we encounter reproducing articles.</p>
</div>
<div id="methodology-1" class="section level3" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> Methodology</h3>
<p>We considered plenty of papers from many issues of <a href="https://journal.r-project.org/">The R Journal</a> - one of the most popular magazines concerning scientific researches, including new R packages. The journal stands out from the magazines because the researches usually upload supplementary material with their articles so it is very easy to check if the code can be reproduced in right way. The articles we checked during our research were published in various years - the newest comes from December 2019 while the oldest is from 2009. We have to admit that the majority of articles could be reproduced without any problems. For the needs of this article we mention only papers where any problems occured. <br>
As we faced the problem of measuring reproducibility we discussed many ways of grading its level. One of the ideas was to create a unified measure of value that would calculate the ratio of functions that managed to execute. We quickly noticed that this approach is not appropriate as sometimes it is not fair to dock the mark by the same value in various examples. For instance we could meet a minor problem that interefered with executing an additional feature and, on the other hand, a vast problem that was a reason that we could not produce an important plot at all. Moreover, sometimes the success of executing the function, was not only defined by completing the work without errors but by the output’s quality. Sometimes the plots were produced without any number values what made them absolutely useless or without one minor annotation which still allowed to make similar conclusions as authors.<br>
These were the reasons why we did not decide to create a simple numeric measure of reproducibility, which probably would be very convenient for data scientists, especially statisticians. When we were checking the articles in terms of reproducibility we noticed that the problems we are facing can be group into a few categories of similar ones. It was on impulse to propose six major categories that can be faced during try of reproducing the results presented in a scientific paper:</p>
<ul>
<li><p><strong>No access to external resources</strong><br>
Some parts of code require access to external resources, for example third-party API or data downloaded from web. If the data was removed from the website, we may have a problem (or it can be impossible!) reproducing the results.</p></li>
<li><p><strong>No compatibility with current versions of used packages</strong><br>
Some packages are deprecated or only available in the older version of R. It can cause problems and is unacceptable.</p></li>
<li><p><strong>Code vulnerable to user settings (especially graphic issues)</strong><br>
The output often depends on the environment settings. For example the scale of the graphics can make it illegible and useless. There were cases that the code attached to article produced completely different figure than the presented one.</p></li>
<li><p><strong>Additional configuration required</strong><br>
Some packages require a non-standard installation. To use some features it can be required to install system packages. Sometimes it is also required to take additional steps (configure access to API e.t.c.).</p></li>
<li><p><strong>Randomness problems</strong><br>
Some functionalities are based on randomness. Sometimes changing the seed may change the results and make it difficult to draw correct conslusions.</p></li>
<li><p><strong>No access to source codes</strong> <br>
Some results shown in an article could not be reproduced because the codes had not been attached to or included in the article.</p></li>
</ul>
<p>We developed a 6-point scale (0 - completely irreproducible, 5 - fully reproducible) evaluating in what degree these problems belong to each category. Points are assigned subjectively depending on our feeling of the severity of the problem. When a category did not appear in the article, it was signed as N/A - not applicable. To minimilize the affect of the personal feels, every article was checked independently by at least two persons.</p>
<div id="mark-examples" class="section level4" number="1.2.4.1">
<h4><span class="header-section-number">1.2.4.1</span> Mark examples</h4>
<p>Most of the articles were reproducible to some extent. None of them were fully irreproducible. However there were few examples, where inability of compiling the first chunks of code resulted in very low marks for the article and thus giving it up with no further research being carried through. Perfect example of such behaviour can be found in an article “MCMC for Generalized Linear Mixed Models with glmmBUGS”<span class="citation">(Brown and Zhou <a href="#ref-RJ-2010-003" role="doc-biblioref">2010</a>)</span>, where all of the following code chunks depended on first ones, which couldn’t be compiled. The reason was that the function, which was resposible for making crucial calculations couldn’t find a registry address. It ended up with displaying both the error and warning message.
Second thing which led to lowering the mark was difficulty with code availibility. There were articles, for instance “RealVAMS: An R Package for Fitting a Multivariate Value-added Model (VAM)” <span class="citation">(Broatch, Green, and Karl <a href="#ref-RJ-2018-033" role="doc-biblioref">2018</a>)</span>, where there were no source codes for all of the figures, which were used in article. Moreover the figures were the main part of the article, thus we decided to lower the mark for access to the source code. Fortunately our team of scientists managed to reproduce the results, despite lack of source code. This article was also an example of having attached obsolete data. It resulted in poor similarity of graphs and plots between the figures we made ourselves and those, which were used in article. Majority of articles were given very satsfying marks, beacuse there were only a few things we could complain about. Fortunately they didin’t have such an impact on reproducibility itself, but rather were annoying for someone who wanted to achieve the same results. The perfect example of such an article is “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution” <span class="citation">(Wilhelm and Manjunath <a href="#ref-tmvtnorm" role="doc-biblioref">2010</a>)</span>. The code had to be manually copied from the article and then reformatted before pasting in into the R console. It’s not a major obstacle, but it may lead to some syntax mistakes and enlengthen the time needed to reproduce the results.</p>
</div>
</div>
<div id="results-1" class="section level3" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> Results</h3>
<p>During the research, our team of scientists examined 16 scientific articles published in The R Journal in terms of reproducibility. As stated before, we decided to divide a mark into six categories and check the level of correctness with the results described in the paper. Every category was graded in six-point scale provided always that a category might not apply in a paper. However, we did not measure the effectivity or functionality of the code as it was not in the scope of this research. To avoid the effect of subjectivity, all articles were graded by at least two of us. Later we calculated the average of the marks. When any category was marked as “not applicable” by at least one marker, it was not taken into consideration in the final summary.</p>
<p>The list of articles checked is <a href="https://github.com/niladrem/R-reprodukowalnosc/blob/master/README.md">here</a>.</p>
<p>The summary of our marks is presented in the boxplot.</p>
<div class="figure">
<img src="https://raw.githubusercontent.com/niladrem/R-reprodukowalnosc/master/Summary_of_marking_articles.png" alt="" />
<p class="caption">Articles’ marks distribution</p>
</div>
<p>As we can see in the plot, packages dependent on external resources are in a minority. Despite that, when we examined one, there was often a problem with dependencies. It could be caused by many reasons, e.g. external data sources or other packages. On the other hand, availability of the source code and graphical aspects of articles turned out to be the most reproducible categories. Resistance to randomness was one of the most interesting categories in our opinion and it turned out to be a category with high variation of grades. Many authors coped with the problem by setting random seeds but sometimes the differences were unavoidable.</p>
</div>
<div id="summary-and-conclusions-1" class="section level3" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="aging-articles.-how-time-affects-reproducibility-of-scientific-papers" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Aging articles. How time affects reproducibility of scientific papers?</h2>
<p><em>Authors: Paweł Morgen, Piotr Sieńko, Konrad Welkier (Warsaw University of Technology)</em></p>
<div id="abstract-2" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Abstract</h3>
<p>Reproduction of a code presented in scientific papers tend to be a laborious yet important process since it enables readers a better understanding of the tools proposed by the authors. While recreating an article various difficulties are faced what can result in calling the paper irreproducible. Some reasons why such situations occur stem from the year when the article was published (for example usage of no more supported packages). The purpose of the following paper is to prove whether this is a general trend which means answering the question: is the year when the article was published related to the reproducibility of the paper. To do so a package CodeExtractorR was created that enables extracting code from PDF files. By using this tool a significant number of articles could be analyzed and therefore results received enabled us to give an objective answer to the stated question.</p>
</div>
<div id="introduction-1" class="section level3" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> Introduction</h3>
<p>Every article published in a scientific journal is aimed at improving our knowledge in a certain field. To prove their theories, authors should provide papers with detailed, working examples and extensive supplementary materials to reproduce results. Unfortunately, these conditions are not always fulfilled. In such a case, other researchers are not able to verify and accept the solutions presented by the author. Moreover, the article is not only useless for the scientific community but also for business recipients.</p>
<p>Over the years, several different definitions of reproducibility have been proposed. According to <span class="citation">Gentleman and Temple Lang (<a href="#ref-gentleman2007statistical" role="doc-biblioref">2007</a>)</span>, reproducible research are papers with accompanying software tools that allow the reader to directly reproduce methods that are presented in the research paper. Other authors suggest that scientific paper is reproducible only if text, data and code are made available and allow an independent researcher to recreate the results <span class="citation">(Vandewalle, Kovacevic, and Vetterli <a href="#ref-vandewalle2009reproducible" role="doc-biblioref">2009</a>)</span>. Second definition emphasizes the importance of accessibility to data used in researches, therefore it seems to be more suitable and complete interpretation of reproducibility. In addition, in this article, we used scale based on the spectrum of reproducibility, proposed by <span class="citation">Peng (<a href="#ref-Peng1226" role="doc-biblioref">2011</a>)</span>. In his work, he also mentioned reproducibility as a minimal requirement for assessing the scientific value of the paper. In the past few years, computing has become an essential part of scientific workflow. Some best practices for writing and publishing reproducible scientific article were presented by <span class="citation">Stodden et al. (<a href="#ref-Stodden2013SettingTD" role="doc-biblioref">2013</a>)</span>. Furthermore, she made a brief overview of existing tools and software that facilitate this task. Similar issue was closely described by <span class="citation">Kitzes, Turek, and Deniz (<a href="#ref-kitzes2017practice" role="doc-biblioref">2017</a>)</span>. Tools created solely for reproducibility in R were proposed by <span class="citation">Marwick (<a href="#ref-marwickrrtools" role="doc-biblioref">n.d.</a>)</span> in package rrtools.</p>
<p>Although many articles focus on software or framework solutions for reproducibility problems, analysis of scientific papers reproducibility in the context of release date has, to the best of the authors’ knowledge, not been described before. The intention of such research is to find correlations between age of article and its reproducibility. Authors believe that finding these dependencies will allow to calculate the estimated life span of data science article. Furthermore, as replicability helps with applying proposed methods and tools, its approximated level might be helpful in estimating usefulness of every scientific article.</p>
</div>
<div id="methodology-2" class="section level3" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> Methodology</h3>
<p>The first issue that should be touched upon, while considering the methodology behind preparing this article, is the scale used to assess the reproducibility of the papers. In the Introduction it was already mentioned how the scale was created but a more detailed description is required. The authors decided that the scale should consist of 4 levels (from 0 up to 3):</p>
<ol style="list-style-type: decimal">
<li>The 0 grade was given in case when no chunk of code gave the anticipated results and no figure was reproduced successfully (in practice such situation occurred mainly when the package described in the article was no more available).</li>
<li>The 1 grade means that at least one example gave the results that the authors waited for, while it also includes situations when about half of the code in the chunks behaved as expected.</li>
<li>The 2 grade was awarded to the articles that were reproducible “in the majority”
what also means that they were not reproducible in 100%.</li>
<li>The 3 grade was received by the articles that were fully reproducible and no problems were encountered in the process. Such a result was highly anticipated by the authors but the criteria for this grade were rather strict.</li>
</ol>
<p>The second issue that also played a vital role in the authors’ work was the scope of the analysis. It was decided that in order to maintain “other thing equal” according to a well-known Latin phrase “ceteris paribus” only one online journal – The R Journal – was taken into account. Being equipped with a tool for faster reproduction of articles – the CodeExtractoR package – the authors agreed to examine about 20 articles that were published across a few years. Such a great number of papers meant that the approach taken could be described as holistic. It is also worth mentioning that usually 30 articles from each year were analyzed (at least whenever it was possible). Finally, it should be noted that in the case of the date when they were published the examined articles range from 2009 up to 2019. The third and final issue that should be considered in this part of the article focuses on the measures undertaken by the authors in order to tackle the problem of biased assessment. Although the scale that was proposed was not totally dependent on the person who was using it, it still left someplace for personal liking and disliking of the paper. As a way of marginalization of this trend the authors have taken part in many conversations when the facts that led to specific grading of the articles were shared. This enabled awarding the grades even more fairly. However, the final measure was much more simple and it was believed to be much more effective as well when compared to the previous one. The articles for each year were divided into 3 groups and assigned to one author each. Thus each author has examined the papers from the whole range of release dates that were taken into account.</p>
</div>
<div id="results-2" class="section level3" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> Results</h3>
<p>Specific results are presented in <em>Table 1.1</em>, which shows the number of examined articles from 2009 up to 2019, grouped by received grade. The column “Grade” represents the 0 - III scale of reproducibility. The rest of the columns shows a number of papers that achieved a particular grade in each year.</p>
<p>
 
</p>
<table>
<thead>
<tr class="header">
<th>Grade</th>
<th>09’</th>
<th>10’</th>
<th>11’</th>
<th>12’</th>
<th>13’</th>
<th>14’</th>
<th>15’</th>
<th>16’</th>
<th>17’</th>
<th>18’</th>
<th>19’</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3</td>
<td>2</td>
<td>6</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>5</td>
<td>9</td>
<td>5</td>
<td>9</td>
<td>9</td>
</tr>
<tr class="even">
<td>I</td>
<td>4</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>3</td>
<td>6</td>
<td>6</td>
<td>10</td>
<td>3</td>
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td>II</td>
<td>3</td>
<td>7</td>
<td>8</td>
<td>4</td>
<td>13</td>
<td>6</td>
<td>10</td>
<td>7</td>
<td>14</td>
<td>6</td>
<td>6</td>
</tr>
<tr class="even">
<td>III</td>
<td>6</td>
<td>7</td>
<td>6</td>
<td>5</td>
<td>8</td>
<td>10</td>
<td>9</td>
<td>4</td>
<td>8</td>
<td>13</td>
<td>13</td>
</tr>
<tr class="odd">
<td>SUM</td>
<td>16</td>
<td>18</td>
<td>20</td>
<td>15</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>
 
</p>
<p>To better illustrate obtained data, in <em>Figure 1.1</em> we have split the results into two groups - the articles which have 0 or I class labeled as “Non-reproducible” and articles with grade II and III as “Reproducible”. It is important to remember that from 2009 to 2012, the overall number of papers oscillated around 18 per year. After 2013, the number of researched articles was constant.</p>
<p>
 
</p>
<div class="figure"><span id="fig:unnamed-chunk-1"></span>
<img src="book_files/figure-html/unnamed-chunk-1-1.svg" alt="Number of papers by publication year" width="672" />
<p class="caption">
FIGURE 1.1: Number of papers by publication year
</p>
</div>
<p>
 
</p>
<p><em>Figure 1.2</em> shows the results in the original 4-level scale. Although, the number of papers varies throughout the years in every reproducibility class, it is observable that intermediate ones - I and II, are less common in the oldest and newest papers. In addition, results in 2018 and 2019 are identical.</p>
<p>
 
</p>
<div class="figure"><span id="fig:unnamed-chunk-2"></span>
<img src="book_files/figure-html/unnamed-chunk-2-1.svg" alt="Number of papers by class and publication year" width="672" />
<p class="caption">
FIGURE 1.2: Number of papers by class and publication year
</p>
</div>
<p>
 
</p>
<p>After calculating the percentage of each class in a specific year ( <em>Figure 1.3</em>, <em>Figure 1.4</em>), it is observed, that in the two oldest examined years - 2009 and 2010 - a ratio of completely unreproducible papers (with 0 or I class ) is surprisingly low. Furthermore, papers with III class of reproducibility are nearly 40% of all articles in these years.</p>
<p>
 
</p>
<div class="figure"><span id="fig:unnamed-chunk-3"></span>
<img src="book_files/figure-html/unnamed-chunk-3-1.svg" alt="Ratio of each class throughout years" width="672" />
<p class="caption">
FIGURE 1.3: Ratio of each class throughout years
</p>
</div>
<p>
 
</p>
<p>Except for 2019, 2018 and 2016, percentage of fully reproducible papers (III class) is stable. In the newest articles, this percentage is slightly higher. Year 2016 is the only one, where unreproducible papers were in the majority. Only in 3 cases, percentage of reproducible articles dropped below 60%.</p>
<p>
 
</p>
<div class="figure"><span id="fig:unnamed-chunk-4"></span>
<img src="book_files/figure-html/unnamed-chunk-4-1.svg" alt="Summarized results throughout years" width="672" />
<p class="caption">
FIGURE 1.4: Summarized results throughout years
</p>
</div>
<p>
 
</p>
</div>
<div id="conclusions" class="section level3" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> Conclusions</h3>
</div>
<div id="summary" class="section level3" number="1.3.6">
<h3><span class="header-section-number">1.3.6</span> Summary</h3>

</div>
</div>
<div id="ways-to-reproduce-articles-in-terms-of-release-date-and-magazine" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Ways to reproduce articles in terms of release date and magazine</h2>
<p><em>Authors: Mikołaj Malec, Maciej Paczóski, Bartosz Rożek</em></p>
<div id="abstract-3" class="section level3" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-1" class="section level3" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Introduction and Motivation</h3>
<p>Reproducibility is a topic which is quite diminished in today’s science world. Scientific articles should be current as long as possible. Their results should be achievable by reader and be the same. Thanks to that science and business world can take advantage of them.The more article is difficult to reproduce, the chance of using knowledge coming from it is smaller. Many researchers tried to define or give principles for reproducibility. There is article published in 2016: “What does research reproducibility mean?” <span class="citation">(Goodman, Fanelli, and Ioannidis <a href="#ref-Goodman2016" role="doc-biblioref">2016</a><a href="#ref-Goodman2016" role="doc-biblioref">b</a>)</span> which tried to warn about reproducibility crisis. Article in 2017: “Computational reproducibility in archaeological research: basic principles and a case study of their implementation” <span class="citation">(Marwick <a href="#ref-Marwick2016" role="doc-biblioref">2016</a>)</span>, compered computational reproducibility to archaeological research and give guidelines for researches to use reproducibility in computing research. But these are just two of many articles about reproducibility. Some articles are about tools and techniques for computational reproducibility <span class="citation">(Piccolo and Frampton <a href="#ref-Piccolo2016" role="doc-biblioref">2016</a>)</span>. They encourage researchers to compute data using environments like Jupiter <span class="citation">(Thomas et al. <a href="#ref-Kluyver2016" role="doc-biblioref">2016</a>)</span> or R markdown <span class="citation">(Marwick, Boettiger, and Mullen <a href="#ref-Marwick2017" role="doc-biblioref">2017</a>)</span>. Thanks to that readers can reproduce finding on their own. What’s new about our approach to the subject of reproducibility is focusing on how can release date and magazine affect the amount of work needed to fully reproduce code or is it even possible. A comprehensive comparison of scientific magazines in terms of reproducibility is yet to be created and this article is our best effort to make it happen.
Mikołaj Malec</p>
</div>
<div id="related-work-2" class="section level3" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Related Work</h3>
</div>
<div id="methodology-3" class="section level3" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> Methodology</h3>
</div>
<div id="results-3" class="section level3" number="1.4.5">
<h3><span class="header-section-number">1.4.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-2" class="section level3" number="1.4.6">
<h3><span class="header-section-number">1.4.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="reproducibility-of-outdated-articles-about-up-to-date-r-packages" class="section level2" number="1.5">
<h2><span class="header-section-number">1.5</span> Reproducibility of outdated articles about up-to-date R packages</h2>
<p><em>Authors: Zuzanna Mróz, Aleksander Podsiad, Michał Wdowski (Warsaw University of Technology)</em></p>
<div id="abstract-4" class="section level3" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-2" class="section level3" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Introduction and Motivation</h3>
<p>The problem of the inability to reproduce the results of research presented in a scientific article may result from a number of reasons - at each stage of design, implementation, analysis and description of research results we must remember the problem of reproducibility - without sufficient attention paid to it, there is no chance to ensure the possibility of reproducing the results obtained by one team at a later time and by other people who often do not have full knowledge of the scope presented in the article. Reproducibility is a problem in both business and science. Science, because it allows credibility of research results <span class="citation">(McNutt <a href="#ref-McNutt679" role="doc-biblioref">2014</a>)</span>. Business, because we care about the correct operation of technology in any environment <span class="citation">(Anda, Sjøberg, and Mockus <a href="#ref-Anda407" role="doc-biblioref">2009</a>)</span>.
As cited from “What does research reproducibility mean?” <span class="citation">(Goodman, Fanelli, and Ioannidis <a href="#ref-Goodman2016" role="doc-biblioref">2016</a><a href="#ref-Goodman2016" role="doc-biblioref">b</a>)</span>;
“Although the importance of multiple studies corroborating a given result is acknowledged in virtually all of the sciences, the modern use of “reproducible research” was originally applied not to corroboration, but to transparency, with application in the computational sciences. Computer scientist Jon Claerbout coined the term and associated it with a software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables. This concept has been carried forward into many data-intensive domains, including epidemiology, computational biology, economics, and clinical trials. According to a U.S. National Science Foundation (NSF) subcommittee on replicability in science, “reproducibility refers to the ability of a researcher to duplicate the results of a prior study using the same materials as were used by the original investigator. That is, a second researcher might use the same raw data to build the same analysis files and implement the same statistical analysis in an attempt to yield the same results…. Reproducibility is a minimum necessary condition for a finding to be believable and informative.”</p>
</div>
<div id="related-work-3" class="section level3" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> Related Work</h3>
<p>Other notable articles about reproducibility include “Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” <span class="citation">(Anda, Sjøberg, and Mockus <a href="#ref-Anda407" role="doc-biblioref">2009</a>)</span>, “Reproducible Research in Computational Science” <span class="citation">(Peng <a href="#ref-Peng1226" role="doc-biblioref">2011</a>)</span> and “A statistical definition for reproducibility and replicability” <span class="citation">(Patil, Peng, and Leek <a href="#ref-Patil066803" role="doc-biblioref">2016</a>)</span>.
“Variability and Reproducibility in Software Engineering: A Study of Four Companies that Developed the Same System” focuses on the variability and reproducibility of the outcome of complete software development projects that were carried out by professional developers.
“Reproducible Research in Computational Science” is about limitations in our ability to evaluate published findings and how reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.
“A statistical definition for reproducibility and replicability” provides formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.
In our article we focus on the reproduction of old scientific articles on R and packages, which are still being developed. We want to explore how the passage of time affects the ability to reproduce results using the currently available updated tools. We are therefore testing backward compatibility for different packages and checking what affects the reproducibility of the code.
We were unable to find scientific articles on this exact issue. There are articles that give ways to measure reproducibility, as well as articles about packages that help with reproduction. But there are yet no articles that summarize the set of packages in terms of their reproducibility.</p>
</div>
<div id="methodology-4" class="section level3" number="1.5.4">
<h3><span class="header-section-number">1.5.4</span> Methodology</h3>
<p>We have checked 13 articles with 16 R packages from at least 10 years ago to ensure that the code chunks match these categories:</p>
<ul>
<li>ade4: Implementing the Duality Diagram for Ecologists <span class="citation">(Dray and Dufour <a href="#ref-ade4" role="doc-biblioref">2007</a>)</span></li>
<li>untb: an R Package For Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity <span class="citation">(Hankin <a href="#ref-untb" role="doc-biblioref">2007</a>)</span></li>
<li>bio.infer: Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition <span class="citation">(Yuan <a href="#ref-bio" role="doc-biblioref">2007</a>)</span></li>
<li>pls: principal Component and Partial Least Squares Regression in R <span class="citation">(Mevik and Wehrens <a href="#ref-pls" role="doc-biblioref">2007</a>)</span></li>
<li>EMD: Empirical Mode Decomposition and Hilbert Spectrum <span class="citation">(Kim and Oh <a href="#ref-EMD" role="doc-biblioref">2009</a>)</span></li>
<li>AdMit: Adaptive Mixtures of Student-t Distributions <span class="citation">(Ardia, Hoogerheide, and Dijk <a href="#ref-admit" role="doc-biblioref">2009</a>)</span></li>
<li>asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples <span class="citation">(Coeurjolly et al. <a href="#ref-asymptest" role="doc-biblioref">2009</a>)</span></li>
<li>PMML: An Open Standard for Sharing Models <span class="citation">(Guazzelli et al. <a href="#ref-PMML" role="doc-biblioref">2009</a>)</span></li>
<li>neuralnet: Training of Neural Networks <span class="citation">(Günther and Fritsch <a href="#ref-neuralnet" role="doc-biblioref">2010</a>)</span></li>
<li>mvtnorm: New Numerical Algorithm for Multivariate Normal Probabilities <span class="citation">(Mi, Miwa, and Hothorn <a href="#ref-mvtnorm" role="doc-biblioref">2009</a>)</span></li>
<li>tmvtnorm: A Package for the TruncatedMultivariate Normal Distribution <span class="citation">(Wilhelm and Manjunath <a href="#ref-tmvtnorm" role="doc-biblioref">2010</a>)</span></li>
<li>party: A New, Conditional Variable-Importance Measure for Random Forests Available in the party Package <span class="citation">(Strobl, Hothorn, and Zeileis <a href="#ref-party" role="doc-biblioref">2009</a>)</span></li>
<li>deSolve, bvpSolve, ReacTran and RootSolve: R packages introducing methods of solving differential equations in R <span class="citation">(Soetaert, Petzoldt, and Setzer <a href="#ref-solve" role="doc-biblioref">2010</a>)</span></li>
</ul>
<p>In our research on the subject we have decided to divide the code from the articles into chunks, according to the principle that each chunk has its own output, to which we give an evaluation according to the criteria we have set. In the process of testing and reproducing various articles, we have identified five categories, and marked them as follows:</p>
<ul>
<li>NO REP (no reproducibility, either due to changes through time or problems with the article that had already been there, regardless of differences in R across years),</li>
<li>HAD TO CHANGE STH (when we had to modify the code to produce correct results that will work in our current R version and can be neatly displayed in a document generated by markdown),</li>
<li>MOSTLY REP (when the results were not ideally identical to the original, but in our opinion the chunks were working according to their purpose in the context of their article),</li>
<li>HAD TO CHANGE &amp; STILL SOMEWHAT DIFFERENT (the code had to be changed and the results were not perfect, but they were correct in the terms of the aforementioned category “MOSTLY REP”, but we could consider them as satisfactory),</li>
<li>FULLY REP (no reproductive problems - the results were identical to original results shown in the article).</li>
</ul>
<p>These criteria can be considered not subjective, but setting such boundaries does not cause confusion in categorisation, thus we decided to use them in order to research and describe the introduced number of articles about R packages from at least 10 years ago.</p>
</div>
<div id="results-4" class="section level3" number="1.5.5">
<h3><span class="header-section-number">1.5.5</span> Results</h3>
<p><a href="1-5-files/raport.html">Here</a> can be seen the in-depth report. Below are the summarised results of our research.</p>
<table>
<caption><span id="tab:unnamed-chunk-5">TABLE 1.1: </span>Chunk classifications</caption>
<thead>
<tr class="header">
<th align="left">NAZWA</th>
<th align="right">NOREP</th>
<th align="right">CHANGE</th>
<th align="right">MOSTLY</th>
<th align="right">CH.SD</th>
<th align="right">FULLY</th>
<th align="right">SUM</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ade4</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">7</td>
</tr>
<tr class="even">
<td align="left">untb</td>
<td align="right">7</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">12</td>
<td align="right">21</td>
</tr>
<tr class="odd">
<td align="left">bio.infer</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">14</td>
</tr>
<tr class="even">
<td align="left">pls</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">11</td>
<td align="right">0</td>
<td align="right">21</td>
<td align="right">32</td>
</tr>
<tr class="odd">
<td align="left">EMD</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">15</td>
<td align="right">16</td>
</tr>
<tr class="even">
<td align="left">AdMit</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="left">asympTest</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">5</td>
</tr>
<tr class="even">
<td align="left">PMML</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">4</td>
</tr>
<tr class="odd">
<td align="left">neuralnet</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">8</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">9</td>
</tr>
<tr class="even">
<td align="left">mvtnorm</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">tmvtnorm</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">6</td>
<td align="right">6</td>
</tr>
<tr class="even">
<td align="left">party</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">deSolve</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">bvpSolve</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="left">ReacTran</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">RootSolve</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">5</td>
<td align="right">6</td>
</tr>
<tr class="odd">
<td align="left">SUM</td>
<td align="right">9</td>
<td align="right">6</td>
<td align="right">39</td>
<td align="right">1</td>
<td align="right">82</td>
<td align="right">137</td>
</tr>
</tbody>
</table>
<p>As it can be seen, the vast majority of chunks are fully reproducible. Even if the chunk is not executed identically to the original one, in most cases it differs only slightly, and the package itself serves its purpose. 88.3% (121/137) of the chunks are executed perfectly or correctly (within our subjective category of being acceptably incorrect), while 93.4% (128/137) of the chunks are working well enough not to throw errors. In practice, only 6.6% (9/137) of chunks were completely irreproducible, which would seem surprising for more-than-a-decade-old articles.</p>
<p>However, given that we have focused particularly on packages that are still being developed, this is quite a feasible result. This can be seen quite clearly by the percentage of the chunks that required minor changes or slightly differed from the results shown in the article - there were 33.6% (46/137), which is clearly a result of the updates or changes that occured in the ever evolving R environment. Of course during our research we stumbled upon numerous packages that have not been updated since years or that have even been deleted from CRAN repository, so they were not within our field of interest. Nonetheless, we would like to emphasize that the results should not suggest that one-decade-old articles are reproducible.</p>
<p>In some of the articles we found specific types of problems:</p>
<ul>
<li>There was no access to data or objects referred to in later calculations,</li>
<li>The results were similar to the original, but the differences were most often due to the random generation of objects. This error was usually reduced later, when the package created some kind of data summary - then the result had a very small relative error with the original result.</li>
<li>The names of individual variables or some of their attributes changed (e.g. column names in the data frame).</li>
</ul>
</div>
<div id="summary-and-conclusions-3" class="section level3" number="1.5.6">
<h3><span class="header-section-number">1.5.6</span> Summary and conclusions</h3>
<p>To sum up, in most cases the packages we examined performed their tasks correctly. The packages themselves have of course changed, but its idea remained the same. Usually new features or improvements were added, but the idea behind the package was the same as it used to be. As a result, most of the packages still managed to cope with the problems of the old ones, in reproduction usually suffering from missing external data or unavoidable changes in the R language itself. All in all, almost in all cases the package does the job in spirit, differing from its old good ways only in vague confusion caused by neverending winds of change.</p>
<p>It can therefore be concluded that most packages that we’ve checked are fully backward compatible, which is good programming practice. In order to increase the reproducibility of articles, this should definitely be taken care of. Additionally, authors should include supplements to their articles, that always help you understand and copy the code.</p>

</div>
</div>
<div id="correlation-between-reproducibility-of-components-of-research-papers-and-their-purpose" class="section level2" number="1.6">
<h2><span class="header-section-number">1.6</span> Correlation between reproducibility of components of research papers and their purpose</h2>
<p><em>Authors: Przemysław Chojecki, Kacper Staroń, Jakub Szypuła (Warsaw University of Technology)</em></p>
<div id="abstract-5" class="section level3" number="1.6.1">
<h3><span class="header-section-number">1.6.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-3" class="section level3" number="1.6.2">
<h3><span class="header-section-number">1.6.2</span> Introduction and Motivation</h3>
<p>It is common knowledge that reproducibility is a way for science to evolve. It is the heart of the scientific method to revisit pre-existing measurements and to try to reproduce its results. However, the term „reproducibility” itself, as well it is crucial to the scientific methodology, it can be also universal at the expense of unambiguousness and usability.<br />
For the purpose of this paper we will have recourse to the definition introduced by ACM:<br />
Reproducibility - The measurement can be obtained with stated precision by a different team, a different measuring system, in a different location on multiple trials. For computational experiments, this means that an independent group can obtain the same resultusing artefacts which they develop completely independently.<br />
This particular definition ilustrates perfectly how in the course of establishing the meaning of term „Reproducibility”, the level of importance of auxiliary measurements and settings of the experiment to the overall results is omitted. It is notably significant misconception especially in the experiments from the field of computional science, when reproducing or even maintaining precise operating conditions is usually impossible.<br />
In the following chapters we will attempt to perform an analysis of reproducibility of the papers submitted to the RJournal, regarding especially presumed objectives of enclosed auxilliary computations and artifacts (i. e. code chunks) in overarching structure of a given paper.<br />
</p>
</div>
<div id="related-work-4" class="section level3" number="1.6.3">
<h3><span class="header-section-number">1.6.3</span> Related Work</h3>
<p>Although there are many research papers related to this article, the following three could be perceived as a “basis” for our study.<br />
1. Daniel Mendez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold provides a definition of reproducibility this article uses, and distinguishes it from replicability<span class="citation">(Fern’andez et al. <a href="#ref-Mendez2019defRepr" role="doc-biblioref">2019</a>)</span>.<br />
2. Steven N. Goodman, Daniele Fanelli and John P. A. Ioannidis defines multiple interpretations of reproducibility. It further divides and classifies reproducibility, and provides a basis on how one can do it<span class="citation">(Goodman, Fanelli, and Ioannidis <a href="#ref-Goodman341ps12" role="doc-biblioref">2016</a><a href="#ref-Goodman341ps12" role="doc-biblioref">a</a>)</span>.<br />
</p>
<p>In their search the authors have not encountered other research papers that study the aspect of reproducibility this article focuses on. If said papers do not actually exist, then this article could provide insights on previously unexamined aspects of reproducibility.<br />
</p>
</div>
<div id="methodology-5" class="section level3" number="1.6.4">
<h3><span class="header-section-number">1.6.4</span> Methodology</h3>
<div id="general-approach" class="section level4" number="1.6.4.1">
<h4><span class="header-section-number">1.6.4.1</span> General approach</h4>
<p>The methodology presented in the following section is a direct consequence of how we approach scientific article as an experience devised by an author for the reader. Our focus is to determine, how author alter this experience by using code chunks instead of plain text. In other words, we ask a question “Why have the authors used the <code>R</code> code?”
Assumption that execution of enclosed code is an integral part of said experience and by extension code chunks supposed to be reproducible for reader to percept the article as intended seems to be reasonable.
However it may not be correct in every case.
Let us consider a situation, where generated output is essential to the thesis stated in the article. If the code is irreproducible, the reader cannot believe the authors. It devastates their credibility.
But what if a goal of the code was to illustrate general tendency in data and output is reproducible only to some degree? Then it may still fulfill its purpose in the article and the lack of full reproducibility does not intefere with experience for a reader.
Following this thought process inevitably leads to new questions, f.e. is it possible for executable code to serve its purpose in article while being completely unreproducible?</p>
<p>To explore this topic we decided to focus on objectives of code in scientific papers.
We have decided that the most accurate and reliable way of finding the purposes of code chunks in scientific articles is by examples. That is why we have analyzed over <span class="math inline">\(30\)</span> papers from <code>The R Journal</code> [<a href="https://journal.r-project.org" class="uri">https://journal.r-project.org</a>].<br />
We have gathered the code chunks into groups and considered a three degree of purpose: the whole article, the group of chunks, and the single chunk of code. We have prepared a list of possible purposes for every level and assign them to our examples. The whole list of purposes is explained in the next chapter.<br />
Then we have produced our measure of reproducibility, which is also detailed later.<br />
</p>
</div>
<div id="objectives" class="section level4" number="1.6.4.2">
<h4><span class="header-section-number">1.6.4.2</span> Objectives</h4>
<p>Since this article is centred around objectives, our understanding of them is of utmost importance. That is why we divided them into three categories, further divided into classes. We described them in detail in the sections below. To limit our individual biases in assessing what the intended objective is, we referred to relevant paragraphs in the original research paper. It has to be noted, that an object (let it be an article, a group of chunks or a chunk) can have more than one objective.</p>
<div id="main-objectives" class="section level5" number="1.6.4.2.1">
<h5><span class="header-section-number">1.6.4.2.1</span> Main Objectives</h5>
<p>Both code chunks by themselves and performed computations corresponding with them can provide wide variety of information. However we can identify and describe reasons why the programming language is present in general in a given paper. All code chunks serve together as a vital element supporting narration of the article and its objective usually can be identified with main objective of narration in article as a whole.</p>
<p>We systematized main objectives and grouped them into the following general objectives:</p>
<ul>
<li>package overview - presenting general structure of specific package, providing example of aplications implemented functions and discussing its performance</li>
<li>object construction - presenting process of constructing and developing virtual object</li>
<li>introduction to subject - using performing code as a complement to educational resource concerning given topic</li>
<li>method implementation - presenting in-depth process of developing solution and explaining it</li>
<li>addressing an issue - presenting solution to specific scientific/computational problem</li>
<li>error identification - recognising and presenting error in existing package, possibly submit alternative solution covering mentioned error</li>
</ul>
</div>
<div id="intermediate-objectives" class="section level5" number="1.6.4.2.2">
<h5><span class="header-section-number">1.6.4.2.2</span> Intermediate Objectives</h5>
<p>Since code chunks in research papers seldom appear on their own, but rather are part of a larger group of chunks serving a certain purpose. For instance, let there be three chunks, named A, B, and C. Let A load data from an external source, B modifies the data and extract a subset of it, and let C generate a plot, based on the data obtained from the two previous chunks. While each chunk has its own distinct objective, together they have at least common one - in this example this is generating a plot. Plot generated by A, B and C can be used to compare between performance of various functions. These chunk group’s objectives we define as intermediate objectives.</p>
<p>We systematized intermediate objectives and grouped them into the following general objectives:</p>
<ul>
<li>package usage - examples on how does an R package operate, how one can use functions provided by the package, in what manner output is generated etc.</li>
<li>practical use - underscoring of the practical usage of code used in code chunks in that group.</li>
<li>method comparison - comparison between functions and/or methods. For example, a microbenchmark between base R functions and functions from a presented package.</li>
<li>generating output - generating an output, for example plots, .csv files, images etc.</li>
<li>presenting specification - presentation on what package specification looks like.</li>
<li>data preparation - preparation of data that may be used later in the paper. This includes both loading the data and modifying it.</li>
<li>occurrence demonstration - demonstration of an occurrence described earlier in the article.</li>
<li>introduction to analysis - introduction to analysing a certain topic and data related to it.</li>
<li>possible error - description of a possible error one can encounter and how one can solve it.</li>
</ul>
</div>
<div id="chunk-objectives" class="section level5" number="1.6.4.2.3">
<h5><span class="header-section-number">1.6.4.2.3</span> Chunk Objectives</h5>
<p>Each chunk has a role - it serves one or more purposes, which we define as chunk objectives.</p>
<p>We systematized chunk objectives and grouped them into the following general objectives:</p>
<ul>
<li>aesthethic example - an example showcasing how output generated by the code chunk looks like.</li>
<li>functional example - an example of how functions showcased in the chunk work.</li>
<li>instruction - an instruction on how one achieves desired effect using R code.</li>
<li>instruction (package) - same as above, but using functions from the package introduced in the article containing the chunk.</li>
<li>data preparation - preparation of data for the following chunks.</li>
<li>data exploration - merging, subsetting, summarisation of data and other types of data manipulation used in order to explore data.</li>
<li>foreign error - turning attention to an error in work done by other author(s).</li>
<li>solving a problem - description of how one solves a given problem using R code.</li>
<li>data modification - modifying data in order to achieve desired effect.</li>
<li>presentation of results - presenting result of computation within the article. This can be done by specific summarising functions (e.g. summarise) or simply printing base R vectors.</li>
<li>plot - plotting graphs in the article.</li>
<li>generating files - generation of files, this includes graphical, text and other files.</li>
<li>results processing - processing of results in order to improve their aesthethic value or to make them more readable.</li>
<li>erroneous action - presenting code that does not run properly as an example of an action should be avoided.</li>
<li>uncallable code - code that, in principle, is impossible to run. This includes pseudocode.</li>
<li>comparison to foreign work - comparation of authors’ work (functions, methods etc.) with work of others, that achieves the same effect. This includes benchmark performance comparisons.</li>
<li>empirical proof - validation of what is mathematically described in earlier sections.</li>
</ul>
</div>
</div>
<div id="reproducibility-1" class="section level4" number="1.6.4.3">
<h4><span class="header-section-number">1.6.4.3</span> Reproducibility</h4>
<p>The sole purpose of this paper is to explore interactions between purposes of code chunks usage and reproducibility aberrations. That requires a system of classification of reproducibility.
We provide simple categorization of forms of reproducibility into the 6 types. This classification system shall serve as a tool for initial phase of our analysis, thus it is not directly involving purpose of discussed code at this stage.</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>perfect reproducibility - code perform flawlessly and after initial configuration precise output is recreated</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>margin of error - after initial configuration code provides output matching expectations within acceptable margin of error (f.e. difference in rounded decimals, default parameters of generated graphics)</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>editorial correction - code requires minor corrections to be executable and viable due to editorial error or changes in naming conventions</li>
</ol></li>
<li><ol start="4" style="list-style-type: decimal">
<li>environment setup - code to execute properly requires major and time-consuming setup and environment changes or may be not able to provide expected results at all</li>
</ol></li>
<li><ol start="5" style="list-style-type: decimal">
<li>unreproducible - code undoubtedly cannot be reproduced (f.e. due to unavailable data, unavailable package, unsupported fatal error)</li>
</ol></li>
<li>-1. missing point of reference - article does not provide (or vaguely provides) expected performence and determining reproducilibity is impossible</li>
</ul>
</div>
<div id="tables-description" class="section level4" number="1.6.4.4">
<h4><span class="header-section-number">1.6.4.4</span> Tables description</h4>
<p>For analysis purposes, we have put our work into tables. The one can see the small part of them here:<br />
</p>
<div id="table-of-articles" class="section level5" number="1.6.4.4.1">
<h5><span class="header-section-number">1.6.4.4.1</span> 1.Table of articles</h5>
<p>Every row represents one article. Every article has a column of an individual number, a sum of lengths of chunks of code, and information about purposes of articles.<br />
</p>
</div>
<div id="table-of-groups" class="section level5" number="1.6.4.4.2">
<h5><span class="header-section-number">1.6.4.4.2</span> 2.Table of groups</h5>
<p>Every row represents one group of chunks. Every group has a column of an individual number, a sum of lengths of chunks of code, and information about purposes of the group.<br />
</p>
</div>
<div id="table-of-chunks" class="section level5" number="1.6.4.4.3">
<h5><span class="header-section-number">1.6.4.4.3</span> 3.Table of chunks</h5>
<p>Every row represents one chunk. Every chunk has a column of an individual number, its length of code, its reproducibility, and information about purposes of code.<br />
</p>
</div>
<div id="length-assessment" class="section level5" number="1.6.4.4.4">
<h5><span class="header-section-number">1.6.4.4.4</span> Length assessment</h5>
<p>To objectively determine a length of code we have decided to count it with such rules:<br />
* skip all empty and commented lines<br />
* skip assignments, unless it contains the execution of a function<br />
* skip executions of functions <code>library</code> and <code>data</code><br />
* skip lines with only technical meaning, i.e. <code>}</code><br />
</p>
</div>
</div>
</div>
<div id="results-5" class="section level3" number="1.6.5">
<h3><span class="header-section-number">1.6.5</span> Results</h3>
</div>
<div id="summary-and-conclusions-4" class="section level3" number="1.6.6">
<h3><span class="header-section-number">1.6.6</span> Summary and conclusions</h3>

</div>
</div>
<div id="how-active-development-affects-reproducibility" class="section level2" number="1.7">
<h2><span class="header-section-number">1.7</span> How active development affects reproducibility</h2>
<p><em>Authors: Ngoc Anh Nguyen, Piotr Piątyszek, Marcin Łukaszyk (Warsaw University of Technology)</em></p>
<div id="abstract-6" class="section level3" number="1.7.1">
<h3><span class="header-section-number">1.7.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-4" class="section level3" number="1.7.2">
<h3><span class="header-section-number">1.7.2</span> Introduction and Motivation</h3>
<p>The key quality in measuring the outcome of researches and experiments is whether results in a paper can be attained by a different research team, using the same methods. Results presented in scientific articles may sometimes seem revolutionary, but there is very little use if it was just a single case impossible to reproduce. The closeness of agreement among repeated measurements of a variable made under the same operating conditions by different people, or over a period of time is what researches must bear in mind. <span class="citation">Peng (<a href="#ref-Peng1226" role="doc-biblioref">2011</a>)</span> leading author of the commentary and an advocate for making research reproducible by others, insists reproducibility should be a minimal standard.</p>
<p>There have been several reproducibility definitions proposed during the last decades. <span class="citation">Gentleman and Temple Lang (<a href="#ref-gentleman2007statistical" role="doc-biblioref">2007</a>)</span> suggest that by reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. The second definition is according to <span class="citation">Vandewalle, Kovacevic, and Vetterli (<a href="#ref-vandewalle2009reproducible" role="doc-biblioref">2009</a>)</span>, research work is called reproducible if all information relevant to the work, including, but not limited to, text, data, and code, is made available, such that an independent researcher can reproduce the results. As said by <span class="citation">LeVeque (<a href="#ref-leveque2009python" role="doc-biblioref">2009</a>)</span> the idea of ‘reproducible research’ in scientific computing is to archive and make publicly available all the codes used to create a paper’s figures or tables, preferably in such a manner that readers can download the codes and run them to reproduce the results. All definitions converge into one consistent postulate - the data and code should be made available for others to view and use. The availability of all information related to research paper gives other investigators the opportunity to verify previously published findings, conduct alternative analyses of the same data, eliminate uninformed criticisms and most importantly - expedite the exchange of information among scientists.</p>
<p>Reproducibility has great importance not only in the academic world but also it also plays a significant role in the business. The concept of technological dept is often used to describe the implied cost of additional rework caused by choosing an easy solution now instead of using a better approach that would take longer in software development.</p>
<p>There are papers about using version control systems to provide reproducible results <span class="citation">(Stanisic, Legrand, and Danjean <a href="#ref-stanisic2015an" role="doc-biblioref">2015</a>)</span>. The authors presented how we can manage to maintain our goal of reproducibility using Git and Org-Mode. Other researchers have created a software package that is designed to create reproducible data analysis <span class="citation">(Fomel et al. <a href="#ref-fomel2013madagascar" role="doc-biblioref">2013</a>)</span>. They have created a package that contains computational modules, data processing scripts, and research papers. The package is build using the Unix principle to write programs that are simple and do well one thing. The program breaks big data analysis chains into small steps to ensure that everything is going in the right way. Some papers suggest using Docker to make sure our research can be reproduced <span class="citation">(Hung et al. <a href="#ref-hung2016guidock" role="doc-biblioref">2016</a>)</span>.</p>
<p>The main goal of our work is to measure the impact of the active development of packages on the reproducibility of scientific papers. Multiple authors <span class="citation">(Rosenberg et al. <a href="#ref-rosenberg2020the" role="doc-biblioref">2020</a>; Kitzes, Turek, and Deniz <a href="#ref-kitzes2017practice" role="doc-biblioref">2017</a>)</span> suggest using the version control system as a key feature in creating reproducible research. The second paper also provides evidence, that this is widely known. Git and GitHub were used in over 80% of cases. However, there are two kinds of using a version control system. An author can push software into the repository, to make it easily accessible and does not update it anymore. The second option is to keep the repository up-to-date and resolve users’ issues. We have not found any research on how these two approaches impact reproducibility.</p>
</div>
<div id="methodology-6" class="section level3" number="1.7.3">
<h3><span class="header-section-number">1.7.3</span> Methodology</h3>
<p><strong>Articles</strong><br />
In our analysis, of reproducibility, we focused on articles introducing packages, that are actively developed on GitHub. Then we measure the reproducibility of an article using two versions of the package: current and the first after publication date to get the answer on the question, what if a package was never updated. In some cases, when it seems appropriate we used the last before publication. We selected 18 articles that were posted on R journal, that are on cran, are developed on GitHub, have code included to reproducibility, and doesn’t have too much impact on R environment.</p>
<p><strong>Measures of reproducibility</strong><br />
We measured how many examples aren’t reproducible using these two versions. We categorized articles into 3 types of reproducibility:<br />
<strong>1.</strong> The article is reproducible, minor differences can happen (e.g. different formating).<br />
<strong>2.</strong> There are differences in function names, other packages that the article uses don’t work but at least half of it works.<br />
<strong>3.</strong> Everything that doesn’t match 1 or 2. It means that the article is not reproducible.</p>
<p>We have counted the three most common issues in each article:<br />
<strong>1. Names</strong> - function or variable name has to be changed<br />
<strong>2. API</strong> - way of using a function or their arguments has changed<br />
<strong>3. Result</strong> - output differs<br />
Using these we can compare specific issues in the current and old versions of the package.</p>
<p><strong>Auxiliary variables</strong><br />
To measure how a package is developed, we used several auxiliary variables from GitHub and CRAN:</p>
<ul>
<li>number of stars</li>
<li>number of subscribers</li>
<li>number of contributors</li>
<li>number of issues</li>
<li>number of open issues</li>
<li>added and deleted lines since the publication date</li>
<li>commits number since the publication date</li>
<li>using Continuous Integration</li>
<li>versions on CRAN since the publication date</li>
</ul>
</div>
<div id="results-6" class="section level3" number="1.7.4">
<h3><span class="header-section-number">1.7.4</span> Results</h3>
<strong>Tested packages</strong><br />

<div style="border: 0px;overflow-x: scroll; width:100%; ">
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:unnamed-chunk-7">TABLE 1.2: </span>Tested packages with measured reproducibility
</caption>
<thead>
<tr>
<th style="text-align:left;">
package
</th>
<th style="text-align:left;">
old.version
</th>
<th style="text-align:right;">
old.reproducibility
</th>
<th style="text-align:right;">
new.reproducibility
</th>
<th style="text-align:right;">
old.names.issues
</th>
<th style="text-align:right;">
new.names.issues
</th>
<th style="text-align:right;">
old.api.issues
</th>
<th style="text-align:right;">
new.api.issues
</th>
<th style="text-align:right;">
old.result.issues
</th>
<th style="text-align:right;">
new.result.issues
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
VSURF
</td>
<td style="text-align:left;">
1.0.2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
MVN
</td>
<td style="text-align:left;">
3.8
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
mldr
</td>
<td style="text-align:left;">
0.2.51
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
fanplot
</td>
<td style="text-align:left;">
3.4.1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
Peptides
</td>
<td style="text-align:left;">
1.0.4
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
cmvnorm
</td>
<td style="text-align:left;">
1.0-3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
factorplot
</td>
<td style="text-align:left;">
1.1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
FactoMineR
</td>
<td style="text-align:left;">
1.3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
gridGraphics
</td>
<td style="text-align:left;">
0.1-5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
phaseR
</td>
<td style="text-align:left;">
1.3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
betategarch
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
tempdisagg
</td>
<td style="text-align:left;">
0.22
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
mvtnorm
</td>
<td style="text-align:left;">
0.9-7
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
brainR
</td>
<td style="text-align:left;">
1.2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
qmethod
</td>
<td style="text-align:left;">
1.3.0
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
stringdist
</td>
<td style="text-align:left;">
0.7.2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
rotations
</td>
<td style="text-align:left;">
1.3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:left;">
ggmap
</td>
<td style="text-align:left;">
2.3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
2
</td>
</tr>
</tbody>
</table>
</div>
<strong>Reproducibility scale</strong><br />
As shown in table below, most packages have same reproducibility scale in each version. Some are less reproducible in current version than in the old.
<table class="table table-striped" style="font-size: 11px; width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
old.reproducibility
</th>
<th style="text-align:right;">
new.reproducibility
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
8
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
</tr>
</tbody>
</table>
<p><strong>Issues count</strong></p>
<p><img src="book_files/figure-html/unnamed-chunk-9-1.svg" width="672" /></p>
<p>We compared if new versions of packages have more or less issues of each type than the old ones. Only for few articles these counts differ, but this data suggests negative impact of active development on reproducibility.</p>
<p><strong>Correlations with auxiliary variables</strong><br />
<img src="book_files/figure-html/unnamed-chunk-10-1.svg" width="672" /></p>
<p>This heatmap shows the correlation between reproducibility scale and issue count increase (new-old) with auxiliary variables. The reproducibility scale does not seem to be correlated with any of them. But there is a strong correlation between name issues count and number of lines added and removed since the publication date. Variables associated with popularity could impact on API changes. There are correlations with results, but results should not be analyzed alone, because when API issue occurs, then we cannot check results.</p>
</div>
<div id="summary-and-conclusions-5" class="section level3" number="1.7.5">
<h3><span class="header-section-number">1.7.5</span> Summary and conclusions</h3>

</div>
</div>
<div id="reproducibility-differences-of-articles-published-in-various-journals-and-using-r-or-python-language" class="section level2" number="1.8">
<h2><span class="header-section-number">1.8</span> Reproducibility differences of articles published in various journals and using R or Python language</h2>
<p><em>Authors: Bartłomiej Eljasiak, Konrad Komisarczyk, Mariusz Słapek (Warsaw University of Technology)</em></p>
<div id="abstract-7" class="section level3" number="1.8.1">
<h3><span class="header-section-number">1.8.1</span> Abstract</h3>
</div>
<div id="introduction-and-motivation-5" class="section level3" number="1.8.2">
<h3><span class="header-section-number">1.8.2</span> Introduction and Motivation</h3>
<p>Due to the growing number of research publications and open-source solutions, the importance of repeatability and reproducibility is increasing. Although reproducibility is a cornerstone of science, a large amount of published research results cannot be reproduced <span class="citation">(Gundersen and Kjensmo <a href="#ref-AAAI1817248" role="doc-biblioref">2018</a>)</span>. Repeatability and reproducibility are closely related to science.</p>
<p>“Reproducibility of a method/test can be defined as the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object, or test material) but under different conditions (different observers, laboratories etc.). (…) On the other hand, repeatability denotes the closeness of the agreement between independent results obtained with the same method on the identical subject(s) (or object or test material), under the same conditions.”<span class="citation">(Slezak and Waczulikova <a href="#ref-SlezakWaczulikova2011" role="doc-biblioref">2011</a>)</span></p>
<p>Reproducibility is crucial since it is what an researcher can guarantee about a research. This not only ensures that the results are correct, but rather ensures transparency and gives scientists confidence in understanding exactly what was done <span class="citation">(Eisner <a href="#ref-Eisner2018" role="doc-biblioref">2018</a>)</span>. It allows science to progress by building on previous work. What is more, it is necessary to prevent scientific misconduct. The increasing number of cases is causing a crisis of confidence in science <span class="citation">(Drummond <a href="#ref-Drummond2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>In psychology the problem has already been addressed. From 2011 to 2015 over two hundred scientists cooperated to reproduce results of one hundred psychological studies <span class="citation">(Anderson et al. <a href="#ref-Ezcuj" role="doc-biblioref">2019</a>)</span>. In computer science (and data science) scientists notice the need for creating tools and guidelines, which help to guarantee reproducibility of solutions <span class="citation">(Biecek and Kosinski <a href="#ref-Archivist" role="doc-biblioref">2017</a>, @Stodden1240)</span>. There exist already developed solutions which are tested to be applied <span class="citation">(Elmenreich et al. <a href="#ref-Elmenreich2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>Reproducibility can focus on different aspects of the publication, including code, results of analysis and data collection methods. This work will focus mainly on the code - results produced by evaluation of different functions and chunks of code from analysed publications.</p>
<p>In this paper we want to compare journals on the reproducibility of their articles. Moreover, we will present the reproducibility differences between R and Python - two of the most popular programming languages in data science publications.There is discussion between proponents of these two languages, which one is more convenient to use in data science. Different journals also compete between each other. There are already many metrics devised to assess which journal is better regarding this metric <span class="citation">(Elsevier, <a href="#ref-JournalMetrics" role="doc-biblioref">n.d.</a>)</span>.
There are no publications related to the reproducibility topic which compare different journals and languages. Although there are some exploring reproducibility within one specific journal <span class="citation">(Stodden, Seiler, and Ma <a href="#ref-Stodden2584" role="doc-biblioref">2018</a>)</span>. What is more, journals notice the importance of this subject <span class="citation">(McNutt <a href="#ref-McNutt679" role="doc-biblioref">2014</a>)</span>. Also according to scientists journals should take some responsibility for this subject <span class="citation">(Eisner <a href="#ref-Eisner2018" role="doc-biblioref">2018</a>)</span>.</p>
</div>
<div id="methodology-7" class="section level3" number="1.8.3">
<h3><span class="header-section-number">1.8.3</span> Methodology</h3>
<p>We decided to focus on three journals:</p>
<ul>
<li>The Journal of Statistical Software<br />
</li>
<li>The Journal of Machine Learning Research<br />
</li>
<li>The Journal of Open Source Software</li>
</ul>
<p>The Journal of Statistical Software and The Journal of Machine Learning Research are well known among scientists in the field of data science. The Journal of Open Source Software is relatively new, was established in 2016.</p>
<p>We choose articles randomly from the time frame 2015-present. From every journal we choose 10 articles, of which 5 are articles introducing an R package and 5 are introducing a Python library. We choose only articles having tests on their github repositories. For our metrics we test the following:</p>
<ul>
<li>Tests on github provided by authors - for R packages <code>test_that</code> tests, for Python libraries <code>pytest</code> and <code>unittest</code> tests.<br />
</li>
<li>Examples from the article - we test whether chunks of code included in the article produce the same results. Number of examples in an article varies a lot, in particular all the articles from Journal of Open Source Software do not have any examples.<br />
</li>
<li>Examples provided by the authors on github repository in the catalog <code>examples</code>.</li>
</ul>
<div id="how-to-compare-articles" class="section level4" number="1.8.3.1">
<h4><span class="header-section-number">1.8.3.1</span> How to compare articles?</h4>
<div id="insight-to-the-problem" class="section level5" number="1.8.3.1.1">
<h5><span class="header-section-number">1.8.3.1.1</span> Insight to the problem</h5>
<p>Before anything can be said about the differences in journals and languages, first there has to be a measure in which they can be compared. Journals in general prefer articles of the same structure. What it means is that articles from different journals can vary substantially. This includes not only topics but number of pages, style of writing and most importantly for the topic of this article the way they present code. Thus it comes as no surprise that there are many means how the code can be reproduced. Every so often when an article is presenting a package there can be no examples and only unit tests. Naturally, the opposite can occur. Obvious conclusion is that the proposed measure must not be in favor of any way of presenting code in the given article. The problem of defining the right measure of article reproducibility deserves a separate article itself and It should be stated that metrics used by us are for sure not without a flaw. We do not assume that they are unbiased but that they are true enough that we can draw conclusions from them.</p>
</div>
<div id="proposed-metrics" class="section level5" number="1.8.3.1.2">
<h5><span class="header-section-number">1.8.3.1.2</span> Proposed metrics</h5>
<p>First of all, we did all tests provided by the author in the article or located on an online repository. But if there was an example but there was no direct connection to if from the article it was not included in our reproduction process, because in our opinion it’s not a part of the article and therefore journal. Because of what has been said in the previous paragraph we decided to look at articles from two perspectives. One is more bias, second is true to the sheer number of reproducible examples and positive tests.</p>
</div>
<div id="students-mark" class="section level5" number="1.8.3.1.3">
<h5><span class="header-section-number">1.8.3.1.3</span> Student’s Mark</h5>
<p>Analysis of the problem has led to the decision that numerical assessment of article reproducibility has too many flaws and does not represent well the problems that occur while recreating the results of an article. What we propose is a 4-degree mark ranging from 2 to 5. The highest mark, 5 is given if the author provided all results to the code shown in the article and repository and if the results can be fully reproduced. The article is scored 4 when there are some minor problems in the reproducibility of the code. For example, an article may lack the outputs to some part of the code shown or there are some errors in tests. The major rule is that code should still do what it was meant to do. If some errors happen, but they are not affecting the results, they are negligible. The article is scored 3 in a few cases. If an article lacks all or the vast majority of code outputs, but when reproduced it still produces reasonable results. When in some tests or examples we can observe non-negligible differences, but this cannot happen to a key element of the article. For example, the method proposed in the article describing training machine learning model works and the model is trained well, but there are errors in the part of the code where the model is used by some different library. If we would have to score the article based only on this example we would give it a 3. The article is scored 2 if there are visible differences in reproducing results of key elements of the article. Or If the code from the article didn’t work even though we had all dependencies.</p>
</div>
<div id="reproducibility-value" class="section level5" number="1.8.3.1.4">
<h5><span class="header-section-number">1.8.3.1.4</span> Reproducibility value</h5>
<p>Second metric we used to analyse articles is simple and puts the same weight to the reproducibility problems of the tests and examples.</p>
<p><span class="math display">\[ R_{val} = 1 - \frac{negative \ tests + negative \ examples}{all \ tests + all \ examples} \]</span></p>
<p>So a score of 0 represents an article that failed in all tests and had only not working examples.</p>
</div>
</div>
</div>
<div id="results-7" class="section level3" number="1.8.4">
<h3><span class="header-section-number">1.8.4</span> Results</h3>
<p>Results of reproducing all chosen articles are presented in the following table:</p>
<table style="width:100%;">
<colgroup>
<col width="72%" />
<col width="6%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th>Title</th>
<th>Journal</th>
<th>Language</th>
<th>Student’s Mark</th>
<th>Reproducibility Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Autorank: A Python package for automated ranking of classifiers</td>
<td>JOSS</td>
<td>Python</td>
<td>4</td>
<td>0.95</td>
</tr>
<tr class="even">
<td>PyEscape: A narrow escape problem simulator packagefor Python</td>
<td>JOSS</td>
<td>Python</td>
<td>2</td>
<td>0.8</td>
</tr>
<tr class="odd">
<td>Seglearn: A Python Package for Learning Sequences and Time Series</td>
<td>JMLR</td>
<td>Python</td>
<td>4</td>
<td>0.88</td>
</tr>
<tr class="even">
<td>OpenEnsembles: A Python Resource for Ensemble Clustering</td>
<td>JMLR</td>
<td>Python</td>
<td>2</td>
<td>0.78</td>
</tr>
<tr class="odd">
<td>py-pde: A Python package for solving partial differential equations</td>
<td>JOSS</td>
<td>Python</td>
<td>2</td>
<td>0.86</td>
</tr>
<tr class="even">
<td>PypeR, A Python Package for Using R in Python</td>
<td>JSTAT</td>
<td>Python</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td>iml: An R package for Interpretable Machine Learning</td>
<td>JOSS</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="even">
<td>pyParticleEst: A Python Framework for Particle-Based Estimation Methods</td>
<td>JSTAT</td>
<td>Python</td>
<td>3</td>
<td>0.67</td>
</tr>
<tr class="odd">
<td>mimosa: A Modern Graphical User Interface for 2-level Mixed Models</td>
<td>JOSS</td>
<td>R</td>
<td>3</td>
<td>0.67</td>
</tr>
<tr class="even">
<td>TensorLy: Tensor Learning in Python</td>
<td>JMLR</td>
<td>Python</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td>The flare Package for High Dimensional Linear Regression and Precision Matrix Estimation in R</td>
<td>JMLR</td>
<td>R</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="even">
<td>The huge Package for High-dimensional Undirected Graph Estimation in R</td>
<td>JMLR</td>
<td>R</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td>mlr: Machine Learning in R</td>
<td>JMLR</td>
<td>R</td>
<td>4</td>
<td>0.98</td>
</tr>
<tr class="even">
<td>origami: A Generalized Framework for Cross-Validation in R</td>
<td>JOSS</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>learningCurve: An implementation of Crawford’s and Wright’s learning curve production functions</td>
<td>JOSS</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="even">
<td>tacmagic: Positron emission tomography analysis in R</td>
<td>JOSS</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>frailtyEM: An R Package for Estimating Semiparametric Shared Frailty Models</td>
<td>JSTAT</td>
<td>R</td>
<td>4</td>
<td>0.79</td>
</tr>
<tr class="even">
<td>Beyond Tandem Analysis: Joint Dimension Reduction and Clustering in R</td>
<td>JSTAT</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="odd">
<td>rmcfs: An R Package for Monte Carlo Feature Selection and Interdependency Discovery</td>
<td>JSTAT</td>
<td>R</td>
<td>2</td>
<td>0.57</td>
</tr>
<tr class="even">
<td>Pyglmnet: Python implementation of elastic-net regularized generalized linear models</td>
<td>JOSS</td>
<td>Python</td>
<td>5</td>
<td>0.98</td>
</tr>
<tr class="odd">
<td>corr2D: Implementation of Two-Dimensional Correlation Analysis in R</td>
<td>JSTAT</td>
<td>R</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="even">
<td>RLPy: A Value-Function-Based Reinforcement Learning Framework for Education and Research</td>
<td>JMLR</td>
<td>Python</td>
<td>2</td>
<td>0.59</td>
</tr>
<tr class="odd">
<td>Rclean: A Tool for Writing Cleaner, More Transparent Code</td>
<td>JOSS</td>
<td>R</td>
<td>5</td>
<td>1</td>
</tr>
<tr class="even">
<td>Graph Transliterator: A graph-based transliteration tool</td>
<td>JOSS</td>
<td>Python</td>
<td>4</td>
<td>0.93</td>
</tr>
<tr class="odd">
<td>CoClust: A Python Package for Co-Clustering</td>
<td>JSTAT</td>
<td>Python</td>
<td>2</td>
<td>0.3</td>
</tr>
</tbody>
</table>
<p>To better present obtained results plots below show distribution of marks within each journal and language:</p>
<div class="figure">
<img src="book_files/figure-html/plot_mark_journals_stacked-1.svg" alt="Distribution of 'Student's Mark' score of reproduced articles within each journal." width="672" />
<p class="caption">
(#fig:plot_mark_journals_stacked)Distribution of ‘Student’s Mark’ score of reproduced articles within each journal.
</p>
</div>
<div class="figure">
<img src="book_files/figure-html/plot_mark_languages_stacked-1.svg" alt="Distribution of 'Student's Mark' score of reproduced articles within each language." width="672" />
<p class="caption">
(#fig:plot_mark_languages_stacked)Distribution of ‘Student’s Mark’ score of reproduced articles within each language.
</p>
</div>
<p>Following plots show means of ‘Student’s Mark’ scores of articles within each journal and each language:</p>
<div class="figure">
<img src="book_files/figure-html/plot_mark_journal-1.svg" alt="Comparison of mean 'Student's Mark' score of reproduced articles between journals." width="672" />
<p class="caption">
(#fig:plot_mark_journal)Comparison of mean ‘Student’s Mark’ score of reproduced articles between journals.
</p>
</div>
<div class="figure">
<img src="book_files/figure-html/plot_mark_language-1.svg" alt="Comparison of mean 'Student's Mark' score of reproduced articles between languages." width="672" />
<p class="caption">
(#fig:plot_mark_language)Comparison of mean ‘Student’s Mark’ score of reproduced articles between languages.
</p>
</div>
<p>Based on the plots we can see that R articles had a better mean score and Journal of Open Source Software had also the best mean score among the journals.</p>
<p>Similar plots below show means of ‘Reprodcibility Value’ scores:</p>
<div class="figure">
<img src="book_files/figure-html/plot_value_journal-1.svg" alt="Comparison of mean 'Reproducibility Value' score of reproduced articles between journals." width="672" />
<p class="caption">
(#fig:plot_value_journal)Comparison of mean ‘Reproducibility Value’ score of reproduced articles between journals.
</p>
</div>
<div class="figure">
<img src="book_files/figure-html/plot_value_language-1.svg" alt="Comparison of mean 'Reproducibility Value' score of reproduced articles between languages." width="672" />
<p class="caption">
(#fig:plot_value_language)Comparison of mean ‘Reproducibility Value’ score of reproduced articles between languages.
</p>
</div>
<p>Same as with ‘Student’s Mark’ we can see that R articles had a better mean score and Journal of Open Source Software had the best mean score among the journals.</p>
</div>
<div id="summary-and-conclusions-6" class="section level3" number="1.8.5">
<h3><span class="header-section-number">1.8.5</span> Summary and conclusions</h3>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Anda407">
<p>Anda, Bente, Dag Sjøberg, and Audris Mockus. 2009. “Variability and Reproducibility in Software Engineering: A Study of Four Companies That Developed the Same System.” <em>Software Engineering, IEEE Transactions on</em> 35 (July): 407–29. <a href="https://doi.org/10.1109/TSE.2008.89">https://doi.org/10.1109/TSE.2008.89</a>.</p>
</div>
<div id="ref-Ezcuj">
<p>Anderson, Christopher, Joanna Anderson, Marcel van Assen, Peter Attridge, Angela Attwood, Jordan Axt, Molly Babel, et al. 2019. “Reproducibility Project: Psychology.” <a href="https://doi.org/10.17605/OSF.IO/EZCUJ">https://doi.org/10.17605/OSF.IO/EZCUJ</a>.</p>
</div>
<div id="ref-admit">
<p>Ardia, David, Lennart F. Hoogerheide, and Herman K. van Dijk. 2009. “AdMit.” <em>The R Journal</em> 1 (1): 25–30. <a href="https://doi.org/10.32614/RJ-2009-003">https://doi.org/10.32614/RJ-2009-003</a>.</p>
</div>
<div id="ref-Archivist">
<p>Biecek, Przemyslaw, and Marcin Kosinski. 2017. “archivist: An R Package for Managing, Recording and Restoring Data Analysis Results.” <em>Journal of Statistical Software</em> 82 (11): 1–28. <a href="https://doi.org/10.18637/jss.v082.i11">https://doi.org/10.18637/jss.v082.i11</a>.</p>
</div>
<div id="ref-RJ-2018-033">
<p>Broatch, Jennifer, Jennifer Green, and Andrew Karl. 2018. “RealVAMS: An R Package for Fitting a Multivariate Value- added Model (VAM).” <em>The R Journal</em> 10 (1): 22–30. <a href="https://doi.org/10.32614/RJ-2018-033">https://doi.org/10.32614/RJ-2018-033</a>.</p>
</div>
<div id="ref-RJ-2010-003">
<p>Brown, Patrick, and Lutong Zhou. 2010. “MCMC for Generalized Linear Mixed Models with glmmBUGS.” <em>The R Journal</em> 2 (1): 13–17. <a href="https://doi.org/10.32614/RJ-2010-003">https://doi.org/10.32614/RJ-2010-003</a>.</p>
</div>
<div id="ref-asymptest">
<p>Coeurjolly, J.-F., R. Drouilhet, P. Lafaye de Micheaux, and J.-F. Robineau. 2009. “asympTest: A Simple R Package for Classical Parametric Statistical Tests and Confidence Intervals in Large Samples.” <em>The R Journal</em> 1 (2): 26–30. <a href="https://doi.org/10.32614/RJ-2009-015">https://doi.org/10.32614/RJ-2009-015</a>.</p>
</div>
<div id="ref-ACMBadging2018">
<p>Computing Machinery, Association for. 2018. “Artifact Review and Badging.” <a href="https://www.acm.org/publications/policies/artifact-review-badging">https://www.acm.org/publications/policies/artifact-review-badging</a>.</p>
</div>
<div id="ref-ade4">
<p>Dray, Stéphane, and Anne-Béatrice Dufour. 2007. “The Ade4 Package: Implementing the Duality Diagram for Ecologists.” <em>Journal of Statistical Software, Articles</em> 22 (4): 1–20. <a href="https://doi.org/10.18637/jss.v022.i04">https://doi.org/10.18637/jss.v022.i04</a>.</p>
</div>
<div id="ref-Drummond2012">
<p>Drummond, Chris. 2012. “Reproducible Research: A Dissenting Opinion.” In.</p>
</div>
<div id="ref-Eisner2018">
<p>Eisner, D. A. 2018. “Reproducibility of Science: Fraud, Impact Factors and Carelessness.” <em>Journal of Molecular and Cellular Cardiology</em> 114 (January): 364–68. <a href="https://doi.org/10.1016/j.yjmcc.2017.10.009">https://doi.org/10.1016/j.yjmcc.2017.10.009</a>.</p>
</div>
<div id="ref-Elmenreich2018">
<p>Elmenreich, Wilfried, Philipp Moll, Sebastian Theuermann, and Mathias Lux. 2018. “Making Computer Science Results Reproducible - a Case Study Using Gradle and Docker,” August. <a href="https://doi.org/10.7287/peerj.preprints.27082v1">https://doi.org/10.7287/peerj.preprints.27082v1</a>.</p>
</div>
<div id="ref-JournalMetrics">
<p>Elsevier. n.d. “Journal Metrics - Impact, Speed and Reach.”</p>
</div>
<div id="ref-Mendez2019defRepr">
<p>Fern’andez, Daniel M’endez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” <em>CoRR</em> abs/1904.06499. <a href="http://arxiv.org/abs/1904.06499">http://arxiv.org/abs/1904.06499</a>.</p>
</div>
<div id="ref-Fernndez2019OpenSI">
<p>Fernández, Daniel Méndez, Daniel Graziotin, Stefan Wagner, and Heidi Seibold. 2019. “Open Science in Software Engineering.” <em>ArXiv</em> abs/1904.06499.</p>
</div>
<div id="ref-fomel2013madagascar">
<p>Fomel, Sergey, Paul Sava, Ioan Vlad, Yang Liu, and Vladimir Bashkardin. 2013. “Madagascar: Open-Source Software Project for Multidimensional Data Analysis and Reproducible Computational Experiments.” <em>Journal of Open Research Software</em> 1 (November): e8. <a href="https://doi.org/10.5334/jors.ag">https://doi.org/10.5334/jors.ag</a>.</p>
</div>
<div id="ref-gentleman2007statistical">
<p>Gentleman, Robert, and Duncan Temple Lang. 2007. “Statistical Analyses and Reproducible Research.” <em>Journal of Computational and Graphical Statistics</em> 16 (1): 1–23.</p>
</div>
<div id="ref-Goodman341ps12">
<p>Goodman, Steven N., Daniele Fanelli, and John P. A. Ioannidis. 2016a. “What Does Research Reproducibility Mean?” <em>Science Translational Medicine</em> 8 (341). <a href="https://doi.org/10.1126/scitranslmed.aaf5027">https://doi.org/10.1126/scitranslmed.aaf5027</a>.</p>
</div>
<div id="ref-Goodman2016">
<p>Goodman, Steven N., Daniele Fanelli, and John P. A. Ioannidis. 2016a. “What Does Research Reproducibility Mean?” <em>Science Translational Medicine</em> 8 (341). <a href="https://doi.org/10.1126/scitranslmed.aaf5027">https://doi.org/10.1126/scitranslmed.aaf5027</a>.</p> 2016b. “What Does Research Reproducibility Mean?” <em>Science Translational Medicine</em> 8 (341): 341ps12–341ps12. <a href="https://doi.org/10.1126/scitranslmed.aaf5027">https://doi.org/10.1126/scitranslmed.aaf5027</a>.</p>
</div>
<div id="ref-PMML">
<p>Guazzelli, Alex, Michael Zeller, Wen-Ching Lin, and Graham Williams. 2009. “PMML: An Open Standard for Sharing Models.” <em>The R Journal</em> 1 (1): 60–65. <a href="https://doi.org/10.32614/RJ-2009-010">https://doi.org/10.32614/RJ-2009-010</a>.</p>
</div>
<div id="ref-AAAI1817248">
<p>Gundersen, Odd Erik, and Sigbjørn Kjensmo. 2018. “State of the Art: Reproducibility in Artificial Intelligence.” <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248">https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17248</a>.</p>
</div>
<div id="ref-neuralnet">
<p>Günther, Frauke, and Stefan Fritsch. 2010. “neuralnet: Training of Neural Networks.” <em>The R Journal</em> 2 (1): 30–38. <a href="https://doi.org/10.32614/RJ-2010-006">https://doi.org/10.32614/RJ-2010-006</a>.</p>
</div>
<div id="ref-untb">
<p>Hankin, Robin. 2007. “Introducing Untb, an R Package for Simulating Ecological Drift Under the Unified Neutral Theory of Biodiversity.” <em>Journal of Statistical Software, Articles</em> 22 (12): 1–15. <a href="https://doi.org/10.18637/jss.v022.i12">https://doi.org/10.18637/jss.v022.i12</a>.</p>
</div>
<div id="ref-hung2016guidock">
<p>Hung, Ling-Hong, Daniel Kristiyanto, Sung Lee, and Ka Yee Yeung. 2016. “GUIdock: Using Docker Containers with a Common Graphics User Interface to Address the Reproducibility of Research.” <em>PloS One</em> 11 (April): e0152686. <a href="https://doi.org/10.1371/journal.pone.0152686">https://doi.org/10.1371/journal.pone.0152686</a>.</p>
</div>
<div id="ref-EMD">
<p>Kim, Donghoh, and Hee-Seok Oh. 2009. “EMD: A Package for Empirical Mode Decomposition and Hilbert Spectrum.” <em>The R Journal</em> 1 (1): 40–46. <a href="https://doi.org/10.32614/RJ-2009-002">https://doi.org/10.32614/RJ-2009-002</a>.</p>
</div>
<div id="ref-kitzes2017practice">
<p>Kitzes, Justin, Daniel Turek, and Fatma Deniz. 2017. <em>The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences</em>. Univ of California Press.</p>
</div>
<div id="ref-leveque2009python">
<p>LeVeque, Randall. 2009. “Python Tools for Reproducible Research on Hyperbolic Problems.” <em>Computing in Science &amp; Engineering</em> 11 (January): 19–27. <a href="https://doi.org/10.1109/MCSE.2009.13">https://doi.org/10.1109/MCSE.2009.13</a>.</p>
</div>
<div id="ref-marwickrrtools">
<p>Marwick, B. n.d. “Rrtools: Creates a Reproducible Research Compendium (2018).”</p>
</div>
<div id="ref-Marwick2016">
<p>Marwick, Ben. 2016. “Computational Reproducibility in Archaeological Research: Basic Principles and a Case Study of Their Implementation.” <em>Journal of Archaeological Method and Theory</em> 24 (2): 424–50. <a href="https://doi.org/10.1007/s10816-015-9272-9">https://doi.org/10.1007/s10816-015-9272-9</a>.</p>
</div>
<div id="ref-Marwick2017">
<p>Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2017. “Packaging Data Analytical Work Reproducibly Using R (and Friends).” <em>The American Statistician</em> 72 (1): 80–88. <a href="https://doi.org/10.1080/00031305.2017.1375986">https://doi.org/10.1080/00031305.2017.1375986</a>.</p>
</div>
<div id="ref-McNutt679">
<p>McNutt, Marcia. 2014. “Journals Unite for Reproducibility.” <em>Science</em> 346 (6210): 679–79. <a href="https://doi.org/10.1126/science.aaa1724">https://doi.org/10.1126/science.aaa1724</a>.</p>
</div>
<div id="ref-pls">
<p>Mevik, Björn-Helge, and Ron Wehrens. 2007. “The Pls Package: Principal Component and Partial Least Squares Regression in R.” <em>Journal of Statistical Software, Articles</em> 18 (2): 1–23. <a href="https://doi.org/10.18637/jss.v018.i02">https://doi.org/10.18637/jss.v018.i02</a>.</p>
</div>
<div id="ref-mvtnorm">
<p>Mi, Xuefei, Tetsuhisa Miwa, and Torsten Hothorn. 2009. “New Numerical Algorithm for Multivariate Normal Probabilities in Package mvtnorm.” <em>The R Journal</em> 1 (1): 37–39. <a href="https://doi.org/10.32614/RJ-2009-001">https://doi.org/10.32614/RJ-2009-001</a>.</p>
</div>
<div id="ref-Patil066803">
<p>Patil, Prasad, Roger D. Peng, and Jeffrey T. Leek. 2016. “A Statistical Definition for Reproducibility and Replicability.” <em>Science</em>. <a href="https://doi.org/10.1101/066803">https://doi.org/10.1101/066803</a>.</p>
</div>
<div id="ref-Peng1226">
<p>Peng, Roger D. 2011. “Reproducible Research in Computational Science.” <em>Science</em> 334 (6060): 1226–7. <a href="https://doi.org/10.1126/science.1213847">https://doi.org/10.1126/science.1213847</a>.</p>
</div>
<div id="ref-Piccolo2016">
<p>Piccolo, Stephen R., and Michael B. Frampton. 2016. “Tools and Techniques for Computational Reproducibility.” <em>GigaScience</em> 5 (1). <a href="https://doi.org/10.1186/s13742-016-0135-4">https://doi.org/10.1186/s13742-016-0135-4</a>.</p>
</div>
<div id="ref-RaffTheGradient2020">
<p>Raff, Edward. 2020. “Quantifying Independently Reproducible Machine Learning.” <a href="https://thegradient.pub/independently-reproducible-machine-learning/">https://thegradient.pub/independently-reproducible-machine-learning/</a>.</p>
</div>
<div id="ref-repro-guide">
<p>“Reproducibility in Science: A Guide to enhancing reproducibility in scientific results and writing.” 2014. <a href="http://ropensci.github.io/reproducibility-guide/">http://ropensci.github.io/reproducibility-guide/</a>.</p>
</div>
<div id="ref-rosenberg2020the">
<p>Rosenberg, David E., Yves Filion, Rebecca Teasley, Samuel Sandoval-Solis, Jory S. Hecht, Jakobus E. van Zyl, George F. McMahon, Jeffery S. Horsburgh, Joseph R. Kasprzyk, and David G. Tarboton. 2020. “The Next Frontier: Making Research More Reproducible.” <em>Journal of Water Resources Planning and Management</em> 146 (6): 01820002. <a href="https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215">https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215</a>.</p>
</div>
<div id="ref-SlezakWaczulikova2011">
<p>Slezak, Peter, and Iveta Waczulikova. 2011. “Reproducibility and Repeatability.” <em>Physiological Research / Academia Scientiarum Bohemoslovaca</em> 60 (April): 203–4; author reply 204.</p>
</div>
<div id="ref-solve">
<p>Soetaert, Karline, Thomas Petzoldt, and R. Woodrow Setzer. 2010. “Solving Differential Equations in R.” <em>The R Journal</em> 2 (2): 5–15. <a href="https://doi.org/10.32614/RJ-2010-013">https://doi.org/10.32614/RJ-2010-013</a>.</p>
</div>
<div id="ref-stanisic2015an">
<p>Stanisic, Luka, Arnaud Legrand, and Vincent Danjean. 2015. “An Effective Git and Org-Mode Based Workflow for Reproducible Research.” <em>SIGOPS Oper. Syst. Rev.</em> 49 (1): 61–70. <a href="https://doi.org/10.1145/2723872.2723881">https://doi.org/10.1145/2723872.2723881</a>.</p>
</div>
<div id="ref-Stodden2013SettingTD">
<p>Stodden, Victoria, David H. Bailey, Jonathan M. Borwein, Randall J. LeVeque, William J. Rider, and William Stein. 2013. “Setting the Default to Reproducible Reproducibility in Computational and Experimental Mathematics.” In.</p>
</div>
<div id="ref-Stodden2584">
<p>Stodden, Victoria, Jennifer Seiler, and Zhaokun Ma. 2018. “An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility.” <em>Proceedings of the National Academy of Sciences</em> 115 (11): 2584–9. <a href="https://doi.org/10.1073/pnas.1708290115">https://doi.org/10.1073/pnas.1708290115</a>.</p>
</div>
<div id="ref-party">
<p>Strobl, Carolin, Torsten Hothorn, and Achim Zeileis. 2009. “Party on!” <em>The R Journal</em> 1 (2): 14–17. <a href="https://doi.org/10.32614/RJ-2009-013">https://doi.org/10.32614/RJ-2009-013</a>.</p>
</div>
<div id="ref-Kluyver2016">
<p>Thomas, Kluyver, Ragan-Kelley Benjamin, P&amp;eacute;rez Fernando, Granger Brian, Bussonnier Matthias, Frederic Jonathan, Kelley Kyle, et al. 2016. “Jupyter Notebooks &amp;Ndash; a Publishing Format for Reproducible Computational Workflows.” <em>Stand Alone</em> 0 (Positioning and Power in Academic Publishing: Players, Agents and Agendas): 87–90. <a href="https://doi.org/10.3233/978-1-61499-649-1-87">https://doi.org/10.3233/978-1-61499-649-1-87</a>.</p>
</div>
<div id="ref-vandewalle2009reproducible">
<p>Vandewalle, Patrick, Jelena Kovacevic, and Martin Vetterli. 2009. “Reproducible Research in Signal Processing.” <em>IEEE Signal Processing Magazine</em> 26 (3): 37–47.</p>
</div>
<div id="ref-tmvtnorm">
<p>Wilhelm, Stefan, and B. G. Manjunath. 2010. “tmvtnorm: A Package for the Truncated Multivariate Normal Distribution.” <em>The R Journal</em> 2 (1): 25–29. <a href="https://doi.org/10.32614/RJ-2010-005">https://doi.org/10.32614/RJ-2010-005</a>.</p>
</div>
<div id="ref-bio">
<p>Yuan, Lester. 2007. “Maximum Likelihood Method for Predicting Environmental Conditions from Assemblage Composition: The R Package Bio.infer.” <em>Journal of Statistical Software, Articles</em> 22 (3): 1–20. <a href="https://doi.org/10.18637/jss.v022.i03">https://doi.org/10.18637/jss.v022.i03</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="imputation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mini-pw/2020L-WB-Book/edit/master/1-0-reproducibility.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
